{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/d/ret1/Taylor/jupyter_notebooks/Research') #TJ change working directory to be the parent directory\n",
    "from Py_files.Functions import *\n",
    "_, filter_files = generate_list_of_files(filter_directory, image_directory)\n",
    "import re\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "\n",
    "M51_pa_alpha_file = [x for x in image_files if extract_filter_name(x) == 'F187N'][0]\n",
    "M51_cont_files = [x for x in image_files if extract_filter_name(x) in ['F150W', \"F300M\"]]\n",
    "\n",
    "def get_EW_using_filters(feature_filter_file, continuum_filter_files, location, radius):\n",
    "    #TJ load files\n",
    "\n",
    "    #TJ get all 3 image fluxes\n",
    "    Fnu_feature = get_image_flux(feature_filter_file, location, radius, replace_negatives=False)\n",
    "    Fnu_cont = [get_image_flux(f, location, radius, replace_negatives=False) for f in continuum_filter_files]\n",
    "    feature_filter = extract_filter_name(feature_filter_file)\n",
    "    continuum_filters = [extract_filter_name(x) for x in continuum_filter_files]\n",
    "    if Fnu_feature.unit != Fnu_cont[0].unit:\n",
    "        print('units are not the same in the feature image and continuum image!')\n",
    "    elif Fnu_feature.unit == u.W/(u.m**2*u.Hz):\n",
    "            \n",
    "        #TJ look up pivot wavelengths\n",
    "        pivot_feat = jwst_pivots[feature_filter]\n",
    "        pivot_cont = [jwst_pivots[extract_filter_name(f)] for f in continuum_filter_files]\n",
    "        \n",
    "        #TJ convert continuum levels into F_lambda using pivot wavelengths, still need to multiply by dlamda\n",
    "        fλ_cont = [(Fnu * c / pivot**2).to(u.W / u.m**2 / u.m)\n",
    "                   for Fnu, pivot in zip(Fnu_cont, pivot_cont)]\n",
    "        \n",
    "        #TJ get mean wavelengths\n",
    "        cont_wls = [jwst_means[f] for f in continuum_filters]\n",
    "        line_wl = jwst_means[feature_filter]\n",
    "    \n",
    "        #TJ interpolate continuum values to the feature wavelength\n",
    "        feature_continuum = np.interp(\n",
    "            line_wl.value,\n",
    "            [w.value for w in cont_wls],\n",
    "            [f.value for f in fλ_cont]\n",
    "        ) * u.W / u.m**2 / u.m\n",
    "        \n",
    "        #TJ print continuum if needed\n",
    "        #print(\"F_lamda of photo continuum : \", feature_continuum)\n",
    "        #TJ get filter transmission curve info\n",
    "        wl, T = get_filter_data(feature_filter)\n",
    "    \n",
    "        #TJ multiply feature F_lambda by dlambda to complete unit conversion\n",
    "        norm = np.trapz(T, wl) / np.max(T)\n",
    "        cont_in_filter = feature_continuum * norm\n",
    "        \n",
    "        #TJ convert feature filter's F_nu into F_lamda\n",
    "        fλ_feature = ((Fnu_feature * c / pivot_feat**2).to(u.W / u.m**2 / u.m))*norm \n",
    "    \n",
    "        #TJ feature area is only the area above the continuum\n",
    "        feature_only = fλ_feature - cont_in_filter\n",
    "    \n",
    "        #TJEquivalent width is this area divided by the continuum level\n",
    "        EW = (feature_only / feature_continuum).to(u.m)\n",
    "    \n",
    "        return EW\n",
    "        \n",
    "    elif Fnu_feature.unit == u.W/u.m**2:\n",
    "        print('test')\n",
    "\n",
    "def get_Pa_a_continuum(continuum_filter_files, location, radius):\n",
    "\n",
    "    #TJ get the two image fluxes\n",
    "    app_sum = [get_image_flux(f, location, radius, replace_negatives=False) for f in continuum_filter_files]\n",
    "    continuum_filters = [extract_filter_name(x) for x in continuum_filter_files]\n",
    "    if app_sum[0].unit != app_sum[1].unit:\n",
    "        print('units are not the same in the two images!')\n",
    "        \n",
    "    elif app_sum[0].unit == u.W/(u.m**2*u.Hz):\n",
    "        #TJ look up pivot wavelengths\n",
    "        pivot_feat = jwst_pivots['F187N']\n",
    "        pivot_cont = [jwst_pivots[extract_filter_name(f)] for f in continuum_filter_files]\n",
    "        \n",
    "        #TJ convert continuum levels into F_lambda using pivot wavelengths, still need to multiply by dlamda\n",
    "        fλ_cont = [(Fnu * c / pivot**2).to(u.W / u.m**2 / u.m)\n",
    "                   for Fnu, pivot in zip(app_sum, pivot_cont)]\n",
    "\n",
    "    elif app_sum[0].unit == u.W/u.m**2:\n",
    "        fλ_cont = []\n",
    "    \n",
    "        for Fband, filt in zip(app_sum, continuum_filters):\n",
    "\n",
    "            wl, T = get_filter_data(filt)\n",
    "\n",
    "            width = np.trapz(T, wl)  # effective width\n",
    "\n",
    "            fλ = (Fband / width).to(u.W/u.m**2/u.m)\n",
    "\n",
    "            fλ_cont.append(fλ)\n",
    "    #TJ interpolate continuum values to the feature wavelength\n",
    "    feature_continuum = np.interp(\n",
    "        pivot_feat.value,\n",
    "        [w.value for w in pivot_cont],\n",
    "        [f.value for f in fλ_cont]\n",
    "    ) * u.W / u.m**2 / u.m\n",
    "    \n",
    "    #TJ print continuum if needed\n",
    "    #print(\"F_lamda of photo continuum : \", feature_continuum)\n",
    "    #TJ get filter transmission curve info\n",
    "    wl, T = get_filter_data(\"F187N\")\n",
    "\n",
    "    #TJ multiply feature F_lambda by dlambda to complete unit conversion\n",
    "    norm = np.trapz(T, wl) / np.max(T)\n",
    "    cont_in_filter = feature_continuum * norm\n",
    "    return cont_in_filter\n",
    "\n",
    "def get_filter_Weff(filter_name):\n",
    "    wl, T = get_filter_data(filter_name)\n",
    "    return np.trapz(T,wl) / np.max(T)\n",
    "        \n",
    "        \n",
    "def get_galaxy_ID_from_file(file):\n",
    "    try:\n",
    "        gal = file.split('/')[-1].split('_')[0]\n",
    "        return gal\n",
    "    except:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "import glob\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Gather file info per galaxy\n",
    "# -------------------------------------------------\n",
    "kiana_files = glob.glob('Data_files/Kiana_files/*nircam_all_clusters_results.csv')\n",
    "radius = 0.3*u.arcsec\n",
    "data = {}\n",
    "\n",
    "for file in kiana_files:\n",
    "    name = get_galaxy_ID_from_file(file)\n",
    "    data.setdefault(name, {})\n",
    "    data[name]['files'] = {}\n",
    "\n",
    "# Danny files (images etc.)\n",
    "danny_files = glob.glob('/d/crow2/phangs/cycle*/**/*', recursive=True)\n",
    "\n",
    "for file in danny_files:\n",
    "    name = get_galaxy_ID_from_file(file)\n",
    "\n",
    "    if name not in data:\n",
    "        continue\n",
    "\n",
    "    if (\"f187n_continuum.fits\" in file) or ('paa_native_anchor_cont.fits' in file):\n",
    "        data[name]['files']['F187N_continuum'] = file\n",
    "\n",
    "    elif ('f187n_line.fits' in file) or ('paa_native_anchor_line.fits' in file):\n",
    "        data[name]['files']['F187N_line'] = file\n",
    "\n",
    "    elif extract_filter_name(file) == \"F300M\":\n",
    "        data[name]['files'][\"F300M\"] = file\n",
    "\n",
    "    elif extract_filter_name(file) == \"F150W\":\n",
    "        data[name]['files'][\"F150W\"] = file\n",
    "    elif \"f187n_i2d_anchor\" in file:\n",
    "        data[name]['files'][\"F187N\"] = file\n",
    "\n",
    "\n",
    "galaxy_tables = {}\n",
    "\n",
    "for file in kiana_files:\n",
    "\n",
    "    name = get_galaxy_ID_from_file(file)\n",
    "\n",
    "    # Require needed Pa-alpha files\n",
    "    if 'F187N' not in data[name]['files']:\n",
    "        continue\n",
    "\n",
    "    t = Table.read(file, format=\"csv\")\n",
    "\n",
    "    locations = []\n",
    "    ages = []\n",
    "    stellar_masses = []\n",
    "    gas_masses = []\n",
    "    Avs = []\n",
    "    EWs = []\n",
    "    mass = []\n",
    "    for row in t:\n",
    "        \n",
    "        loc = SkyCoord(ra=row['ra']*u.deg,\n",
    "                       dec=row['dec']*u.deg)\n",
    "\n",
    "        EW = get_EW_using_filters(data[name]['files']['F187N'], [data[name]['files']['F150W'], data[name]['files']['F300M']], loc, radius)\n",
    "        \n",
    "        # --- Append ---\n",
    "        locations.append(loc)\n",
    "        ages.append(row['best.sfh.age'])\n",
    "        stellar_masses.append(row['best.stellar.m_star'])\n",
    "        gas_masses.append(row['best.stellar.m_gas'])\n",
    "        Avs.append(row['best.attenuation.A550'])\n",
    "        EWs.append(EW.to(u.AA))\n",
    "        mass.append(row['best.stellar.m_star'] + row['best.stellar.m_gas'])\n",
    "    # --- Create Table ---\n",
    "    tab = Table()\n",
    "    tab['location'] = locations\n",
    "    tab['age_Myr'] = ages\n",
    "    tab['stellar_mass_Msun'] = stellar_masses\n",
    "    tab['gas_mass_Msun'] = gas_masses\n",
    "    tab['Av'] = Avs\n",
    "    tab['PaA_EW'] = EWs\n",
    "    tab['mass'] = mass\n",
    "    galaxy_tables[name] = tab\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Optional: list galaxies with full data\n",
    "# -------------------------------------------------\n",
    "full_data = list(galaxy_tables.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_cuts = [1000, 5000, 15000, 50000, 150000, 500000, 1500000, 5000000]\n",
    "young = galaxy_tables['ngc1433'][galaxy_tables['ngc1433']['age_Myr'] < 20]\n",
    "galaxy_tables['ngc1433'].sort('mass')\n",
    "m1 = galaxy_tables['ngc1433'][galaxy_tables['ngc1433']['mass'] < mass_cuts[0]]\n",
    "m5 = galaxy_tables['ngc1433'][(galaxy_tables['ngc1433']['mass'] < mass_cuts[1]) & (galaxy_tables['ngc1433']['mass'] > mass_cuts[0])]\n",
    "m10 = galaxy_tables['ngc1433'][(galaxy_tables['ngc1433']['mass'] < mass_cuts[2]) & (galaxy_tables['ngc1433']['mass'] > mass_cuts[1])]\n",
    "m50 = galaxy_tables['ngc1433'][(galaxy_tables['ngc1433']['mass'] < mass_cuts[3]) & (galaxy_tables['ngc1433']['mass'] > mass_cuts[2])]\n",
    "m100 = galaxy_tables['ngc1433'][(galaxy_tables['ngc1433']['mass'] < mass_cuts[4]) & (galaxy_tables['ngc1433']['mass'] > mass_cuts[3])]\n",
    "m500 = galaxy_tables['ngc1433'][(galaxy_tables['ngc1433']['mass'] < mass_cuts[5]) & (galaxy_tables['ngc1433']['mass'] > mass_cuts[4])]\n",
    "m1000 = galaxy_tables['ngc1433'][(galaxy_tables['ngc1433']['mass'] < mass_cuts[6]) & (galaxy_tables['ngc1433']['mass'] > mass_cuts[5])]\n",
    "m5000 = galaxy_tables['ngc1433'][(galaxy_tables['ngc1433']['mass'] < mass_cuts[7]) & (galaxy_tables['ngc1433']['mass'] > mass_cuts[6])]\n",
    "mass_sets = [m1, m5, m10, m50, m100, m500, m1000, m5000]\n",
    "for set in mass_sets:\n",
    "    print(len(set))\n",
    "    print(np.mean(set['mass']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/d/ret1/Taylor/jupyter_notebooks/Research/slug2')  # Path to slug2 directory\n",
    "\n",
    "\n",
    "import os\n",
    "home_directory = \"/d/ret1/Taylor/jupyter_notebooks/Research\" \n",
    "os.chdir(home_directory) #TJ change working directory to be the parent directory\n",
    "from Py_files.Basic_analysis import *\n",
    "\n",
    "import glob\n",
    "import re\n",
    "slug_path = \"/d/ret1/Taylor/jupyter_notebooks/Research/slug2/bin/slug\"\n",
    "os.chdir(\"/d/ret1/Taylor/jupyter_notebooks/Research/slug2\")\n",
    "import slugpy\n",
    "wd = '/d/ret1/Taylor/jupyter_notebooks/Research'\n",
    "\n",
    "\n",
    "def try_float(x):\n",
    "    '''\n",
    "    Try to convert an item or an array of items to float.\n",
    "    If conversion fails, returns original value(s).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : scalar or array-like\n",
    "        Item(s) to convert.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float, array of floats, or original input if conversion fails.\n",
    "    '''\n",
    "    # If x is array-like, attempt vectorized conversion\n",
    "    if isinstance(x, (list, np.ndarray)):\n",
    "        try:\n",
    "            return np.array(x, dtype=float)\n",
    "        except ValueError:\n",
    "            return x\n",
    "    else:\n",
    "        # Scalar input\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            return x\n",
    "            \n",
    "def do_alex_model(model_name, mass, N=1000):\n",
    "    '''Replication of Alex's model'''\n",
    "    input_file = f\"{model_name}.slugin\"\n",
    "    \n",
    "    with open(input_file, 'w') as f:\n",
    "        f.write(f'model_name {model_name}\\n')\n",
    "        f.write(f'out_dir {wd}/SLUG_stuff\\n') #TJ this is where the output files will be written to\n",
    "        f.write(f'verbosity 1\\n') #TJ level of printed outputs while running (0=only warnings/errors) (1=some outputs) (2=lots of outputs)\n",
    "        ##################################################################\n",
    "        # Parameters controlling simulation execution and physical model #\n",
    "        ##################################################################\n",
    "        f.write(f'sim_type cluster\\n') #TJ must be either galaxy or cluster (defaults to galaxy)\n",
    "        f.write(f'n_trials {N}\\n') #TJ total number of model clusters to run\n",
    "        #f.write(f'checkpoint_interval = 100\\n') #TJ create checkpoint after this many trials (default to no checkpointing)\n",
    "        f.write(f'time_step 1.0e6\\n') #TJ simulation runs for 1million years before computing new values\n",
    "        #f.write(f'start_time 1.0e6\\n') #TJ default start time is the same as timestep\n",
    "        f.write(f'end_time 1.0e7\\n') #TJ how long does each simulation run for in years\n",
    "        #f.write(f'sfr 0.001\\n') #TJ star formation rate, ignored for sim types = cluster\n",
    "        #f.write(f'sfh sfh.txt\\n') #TJ star formation history, ignored for sim types = cluster\n",
    "        f.write(f'cluster_mass {mass}\\n') #TJ cluster mass in solar masses, ignored for sim type = galaxy\n",
    "        #f.write(f'redshift 0\\n') #TJ defaults to 0\n",
    "        ##################################################################\n",
    "        # Parameters controlling simulation outputs #\n",
    "        ##################################################################\n",
    "        f.write(f'out_cluster 1\\n') #TJ output cluster properties? default = 1\n",
    "        f.write(f'out_cluster_phot 1\\n') #TJ output cluster photometry? (must specify filters also)\n",
    "        f.write(f'out_cluster_spec 0\\n') #TJ output cluster spectroscopy? *adds significant computation time*\n",
    "        f.write(f'out_cluster_yield 1\\n') #TJ output cluster nucleosynthesis yields?\n",
    "        #f.write(f'out_integrated 1\\n') #TJ output integrated properties of galaxy? ignored for sim types = cluster\n",
    "        #f.write(f'out_integrated_phot 1\\n') #TJ output integrated photometry of galaxy? ignored for sim types = cluster\n",
    "        #f.write(f'out_integrated_spec 1\\n') #TJ output integrated spectroscopy of galaxy? ignored for sim types = cluster\n",
    "        #f.write(f'out_integrated_yield 1\\n') #TJ output integrated chemical yields of galaxy? ignored for sim types = cluster\n",
    "        f.write(f'output_mode ascii\\n') #TJ can be either binary, ascii, or fits\n",
    "        #####################################################################\n",
    "        # Parameters controlling the physical models used for stars         #\n",
    "        #####################################################################\n",
    "        #f.write(f'imf lib/imf/chabrier.imf\\n') #TJ what imf to use? defaults to chabrier 2001\n",
    "        #f.write(f'cmf lib/cmf/slug_default.cmf\\n') #TJ cluster mass function for galaxies, ignored for sim types = cluster\n",
    "        f.write(f'clf lib/clf/nodisrupt.clf\\n') #TJ cluster lifetime Default: lib/clf/slug_default.clf (dN/dt ~ t^-1.9)\n",
    "        f.write(f'tracks mist_2016_vvcrit_40\\n') #TJ choose the stellar track. Defaults to geneva_2013_vvcrit_00\n",
    "        f.write(f'atmospheres lib/atmospheres\\n') #TJ directory of the stellar atmospheres\n",
    "        f.write(f'specsyn_mode sb99\\n') #TJ Spectral synthesis mode, describing which models to use for stellar atmospheres allowed values below\n",
    "        # -- planck (treat stars as blackbodies)\n",
    "        # -- kurucz (use Kurucz atmospheres, as compiled by Lejeune+ 1997)\n",
    "        # -- kurucz+hillier (use Hillier models for WR stars, kurucz for all others)\n",
    "        # -- kurucz+pauldrach (use Pauldrach models for OB stars, kurucz for others)\n",
    "        # -- sb99 (emulate starburst99 -- Pauldrach for OB stars, Hillier for WR stars, kurucz for others) This is the default value\n",
    "        f.write(f'clust_frac 1.0\\n') #TJ fraction of stars born in clusters (always 1.0 for sim types = cluster)\n",
    "        f.write(f'min_stoch_mass 0.08\\n') #TJ minimum stochastically sampled mass. Everything below is considered to be continuously sampled\n",
    "        #f.write(f'metallicity       1.0\\n') #TJ metalicity function. If tracks is specified, this should be omitted\n",
    "        #####################################################################\n",
    "        # Parameters controlling extinction                                 #\n",
    "        #####################################################################\n",
    "        f.write(f'A_V lib/avdist/slug_default.av\\n') #TJ set extinction function\n",
    "        f.write(f'extinction_curve lib/extinct/MW_EXT_SLUG.dat\\n') #TJ shape of extinction curve\n",
    "        f.write(f'nebular_extinction_factor lib/avdist/neb_factor_default.av\\n') #TJ use a different extinction law for nebulae\n",
    "        #####################################################################\n",
    "        # Parameters controlling nebular emission                           #\n",
    "        #####################################################################\n",
    "        f.write(f'compute_nebular 1\\n') #TJ compute nebular emission specifically?\n",
    "        #f.write(f'atomic_data lib/atomic\\n') #TJ atomic information, defaults to lib/atomic\n",
    "        #f.write(f'nebular_no_metals 0\\n') #TJ 1 would be to turn off nebular metal emission (includes He), 0 means leave metals on\n",
    "        #f.write(f'nebular_den 1.0e2\\n') #TJ hydrogen density (default is 100)\n",
    "        #f.write(f'nebular_temp -1.0\\n') #TJ nebular temperature, default is -1, if negative, temp will be calculated from cloudy\n",
    "        f.write(f'nebular_logU -2.5\\n') #TJ logU representing ionization parameter\n",
    "        f.write(f'nebular_phi 0.73\\n') #TJ fraction of ionizing photons that are absorbed by Hydrogen atoms\n",
    "        #############################################\n",
    "        # Parameters describing photometric filters #\n",
    "        #############################################\n",
    "        f.write(f'phot_bands JWST_F150W, JWST_F187N, JWST_F300M\\n') #TJ list of filters for photometric results\n",
    "        f.write(f'filters lib/filters\\n') #TJ directory for filter information to be read from\n",
    "        #f.write(f'phot_mode Lnu\\n') #TJ what units should the photometry results print in?\n",
    "        ############################################\n",
    "        # Parameters controlling yield calculation #\n",
    "        ############################################\n",
    "        f.write(f'yield_dir lib/yields\\n') #TJ directory for yield files\n",
    "        \n",
    "        # are available:\n",
    "        # \n",
    "        \n",
    "        # \n",
    "        f.write(f'yield_mode sukhbold16+karakas16+doherty14\\n') #TJ Model to use for yield calculation. Currently the following models accepted:\n",
    "        # -- sukhbold16 = Solar metallicity type II SN yields from Sukhbold et al. (2016, ApJ, 821, 38); no other yields\n",
    "        # -- # karakas16+doherty14 = metallicity-dependent AGB star yields from Karakas & Lugaro (2016, ApJ, 825, 26), and super- \n",
    "        #                                                                                 AGB star yields from Doherty+ (2014, MNRAS, 437, 195)\n",
    "        # -- sukhbold16+karakas16+doherty14 = sukhbold16 used for SNII, karakas16+doherty14 for AGB\n",
    "        f.write(f'\\n')\n",
    "\n",
    "    return input_file\n",
    "\n",
    "\n",
    "def read_all_files(model_name):\n",
    "    '''\n",
    "    Reads all SLUG output .txt files for a given model_name into a dictionary of numpy arrays.\n",
    "    Keys are column names, values are data arrays.\n",
    "    '''\n",
    "    output = {}\n",
    "    files = glob.glob(f'{wd}/SLUG_stuff/{model_name}_cluster*.txt')\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Get column names\n",
    "        col_names = lines[0].strip().split()\n",
    "        n_cols_names = len(col_names)\n",
    "        \n",
    "        # Parse data lines\n",
    "        data_entries = []\n",
    "        for line in lines[2:]:\n",
    "            stripped = line.strip()\n",
    "            if line.startswith('---------'):\n",
    "                continue\n",
    "            entries = stripped.split()\n",
    "            \n",
    "            # Pad missing entries with 'nan'\n",
    "            if len(entries) < n_cols_names:\n",
    "                entries += ['nan'] * (n_cols_names - len(entries))\n",
    "            data_entries.append(entries)\n",
    "        \n",
    "        # Skip empty files gracefully\n",
    "        if not data_entries:\n",
    "            continue\n",
    "        \n",
    "        # Convert to numpy array of strings first\n",
    "        data_array = np.array(data_entries, dtype='U20')\n",
    "        \n",
    "        # Convert each column individually to float if possible, else keep as string\n",
    "        for i, name in enumerate(col_names):\n",
    "            col = data_array[:, i]\n",
    "            try:\n",
    "                col_converted = col.astype(float)\n",
    "            except ValueError:\n",
    "                col_converted = col  # keep as string if conversion fails\n",
    "            \n",
    "            if name in output:\n",
    "                print(f\"Warning: Duplicate column name '{name}' found. Overwriting previous value.\")\n",
    "            output[name] = col_converted\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_paalpha_ew(data):  \n",
    "    \"\"\"  \n",
    "    Compute Paα flux and equivalent width from SLUG output.  \n",
    "\n",
    "    Args:  \n",
    "        data (dict): Dictionary with filter fluxes (e.g., 'JWST_F150W').  \n",
    "\n",
    "    Returns:  \n",
    "        dict: Paα flux (Jy), continuum (Jy), and EW (Å).  \n",
    "    \"\"\"  \n",
    "    # Extract fluxes (adjust keys if needed)  \n",
    "    f150 = data['JWST_F150W']  # Continuum filter 1  \n",
    "    f300 = data['JWST_F300M']  # Continuum filter 2  \n",
    "    f187n = data['JWST_F187N'] # Paα filter  \n",
    "    f187c = data['JWST_F187N_n'] #continuum around Paa\n",
    "    # Estimate continuum at Paα (1.875 µm) by linear interpolation  \n",
    "    # Wavelengths (µm) for each filter (central λ from JWST)  \n",
    "    lambda150 = jwst_pivots['F150W'].to(u.um).value\n",
    "    lambda187 = jwst_pivots['F187N'].to(u.um).value\n",
    "    lambda300 = jwst_pivots['F300M'].to(u.um).value\n",
    "\n",
    "    # Linear fit to continuum (F150W and F200W)  \n",
    "    slope = (f300 - f150) / (lambda300 - lambda150)  \n",
    "    continuum = f150 + slope * (lambda187 - lambda150)  \n",
    "    \n",
    "    # Subtract continuum to isolate Paα flux  \n",
    "    paalpha_flux = f187n - continuum  \n",
    "    paalpha_flux_c = f187n - f187c\n",
    "    # Compute equivalent width (EW) in Ångströms  \n",
    "    # EW = Δλ (F_line / F_continuum), where Δλ = F187N filter width (~0.02 µm = 200 Å)  \n",
    "    f187n_width = 0.02 * 1e4  # Convert µm to Å (1 µm = 1e4 Å)  \n",
    "    ew = f187n_width * (paalpha_flux / continuum)  \n",
    "    ew_c = f187n_width * (paalpha_flux_c / f187c)  \n",
    "\n",
    "    return {'paalpha_flux': paalpha_flux, 'continuum': continuum, 'EW': ew,\n",
    "            'paalpha_flux_c': paalpha_flux, 'continuum_c': continuum, 'EW_c': ew_c}\n",
    "\n",
    "N_array = [1000, 1000, 1000, 1000, 800, 700, 500, 250]\n",
    "for i, set in enumerate(mass_sets):\n",
    "    m = np.round(np.mean(set['mass']))\n",
    "    model_name = f'testing_young_cluster_mass_{m}'\n",
    "    slug_input_file = do_alex_model(model_name, m, N=N_array[i])\n",
    "    os.system(f'{slug_path} {slug_input_file}')\n",
    "    temp = read_all_files(model_name)\n",
    "    rows = [dict(zip(temp.keys(), values)) for values in zip(*temp.values())]\n",
    "    ews = []\n",
    "    ews_c = []\n",
    "    for row in rows:\n",
    "        ews.append(compute_paalpha_ew(row)['EW'])\n",
    "        ews_c.append(compute_paalpha_ew(row)['EW_c'])\n",
    "    data = np.array([temp['Time'], ews, ews_c])\n",
    "    np.save(f'/d/ret1/Taylor/jupyter_notebooks/Research/SLUG_stuff/datasets/young_{m}_mass_EW.npy', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine grid size (e.g. 2 rows x 3 columns for 6 plots)\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(len(mass_sets) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 8), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, set in enumerate(mass_sets[:-1]):\n",
    "    m = np.round(np.mean(set['mass']))\n",
    "    data = np.load(f'/d/ret1/Taylor/jupyter_notebooks/Research/SLUG_stuff/datasets/young_{m}_mass_EW.npy')\n",
    "    \n",
    "    ax = axes[i]\n",
    "    chunk_data = []\n",
    "    c_chunk_data = []\n",
    "    for chunk in range(1000):\n",
    "        start = 10*chunk\n",
    "        ax.plot(data[0][start:start+10], data[1][start:start+10], linewidth = 0.1, color = 'black')\n",
    "        chunk_data.append(data[1][start:start+10])\n",
    "        c_chunk_data.append(data[2][start:start+10])\n",
    "        \n",
    "    chunk_data = np.array(chunk_data)  # shape: (100, 10)\n",
    "    c_chunk_data = np.array(chunk_data)  # shape: (100, 10)\n",
    "    \n",
    "    # Calculate median across chunks for each timestep\n",
    "    median_y = np.nanmedian(chunk_data, axis=0)\n",
    "    median_y_c = np.nanmedian(c_chunk_data, axis=0)\n",
    "    \n",
    "    # Overplot median track\n",
    "    ax.plot(data[0][start:start+10], median_y_c, color='blue', linewidth=2, label='Median_c')\n",
    "    ax.scatter(set['age_Myr']*1e6, set['PaA_EW'], color = 'red')\n",
    "    ax.set_title(f'Mass = {m}')\n",
    "    ax.set_xlabel('Age (years)')\n",
    "    ax.set_ylabel('EW in Angstroms')\n",
    "    ax.legend(fontsize=6)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim([1e6,5e7])\n",
    "    ax.set_ylim([1e0,5e4])\n",
    "    ax.set_yscale('log')\n",
    "# Hide any unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data['ngc1433']\n",
    "loc = test['region_1']['location']\n",
    "radius = 0.3*u.arcsec\n",
    "files = [test['files'][x] for x in test['files']]\n",
    "show_images(files, loc, radius)\n",
    "for file in files:\n",
    "    print(get_image_flux(file, loc, radius))\n",
    "get_image_flux(image_files[0], locations[0], radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(173):\n",
    "    print('Region ',i)\n",
    "    loc = test[f'region_{i}']['location']\n",
    "    print('continuum flux : ', get_image_flux(test['files']['F187N_continuum'], loc, radius))\n",
    "    print('Predicted continuum : ', get_Pa_a_continuum([test['files']['F150W'], test['files']['F300M']], loc, radius))\n",
    "    print('line flux : ', get_image_flux(test['files']['F187N_line'], loc, radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images([data['ic5332']['F300M']], loc, 1*u.arcsec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kiana_files[0])\n",
    "print(kiana_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table.read(kiana_files[1], format=\"csv\")\n",
    "t['best.sfh.age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "danny_files = glob.glob('/d/crow2/phangs/cycle*/**/*', recursive=True)\n",
    "danny_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = glob.glob('/d/crow2/phangs/cycle*/**/*')\n",
    "for f in file:\n",
    "    if 'ngc628' in f:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "kiana_files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table.read(kiana_files[0])\n",
    "t[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Table.read(kiana_files[1])\n",
    "t[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F300M.dat'\n",
    "data = np.genfromtxt(file)\n",
    "for line in data:\n",
    "    print(line[0], line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
