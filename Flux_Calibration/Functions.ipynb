{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286f71f1-ffcc-460d-9532-9219e479665c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some functions use pre-defined data files, a warning will print when this is the case.\n",
      "Current default filter data directory /d/crow1/tools/cigale/database_builder/filters/jwst/\n",
      "Current default image directory Data_files/Image_files/v0p3\n",
      "Wavelength-sorted lists of files saved to variables 'filter_files' and 'image_files'\n",
      "Regenerate sorted lists using 'image_files, filter_files = generate_list_of_files(filter_directory, image_directory)'\n",
      "JWST filter mean wavelengths stored as dictionary, called using jwst_means[\"F115W\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from photutils.aperture import CircularAperture, aperture_photometry\n",
    "from spectral_cube import SpectralCube\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import voigt_profile\n",
    "\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "import astropy.units as u\n",
    "from astropy.wcs import WCS\n",
    "from astropy.constants import c\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import simple_norm, imshow_norm\n",
    "from astropy.visualization import AsinhStretch\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize\n",
    "from astropy.visualization import SqrtStretch \n",
    "\n",
    "#TJ define functions needed to generate files\n",
    "os.chdir('/d/ret1/Taylor/jupyter_notebooks/Research') #TJ change working directory to be the parent directory\n",
    "\n",
    "print('Some functions use pre-defined data files, a warning will print when this is the case.')\n",
    "filter_directory = '/d/crow1/tools/cigale/database_builder/filters/jwst/'\n",
    "image_directory = 'Data_files/Image_files/v0p3'\n",
    "print(f'Current default filter data directory {filter_directory}')\n",
    "print(f'Current default image directory {image_directory}')\n",
    "print(\"Wavelength-sorted lists of files saved to variables 'filter_files' and 'image_files'\")\n",
    "print(\"Regenerate sorted lists using 'image_files, filter_files = generate_list_of_files(filter_directory, image_directory)'\")\n",
    "\n",
    "with open(\"Data_files/misc_data/jwst_filter_means.pkl\", \"rb\") as file:\n",
    "    jwst_means = pickle.load(file)\n",
    "print('JWST filter mean wavelengths stored as dictionary, called using jwst_means[\"F115W\"]')\n",
    "\n",
    "def extract_filter_name(filename):\n",
    "    '''extract entire filter name, for example: F164N\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    file_name : type = str - name of filter's fits file of format similar to ngc5194_nircam_1v3_f164n_i2d.fits\n",
    "        *note*: function keys on the \"_\" and .fits to get filter name, requires lower case filter names, see generalized \"sort_filters\" function\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    filter name as string\n",
    "    '''   \n",
    "    # For .fits files: ngc5194_nircam_1v3_f164n_i2d.fits → \"f164n\"\n",
    "    if filename.endswith('.fits'):\n",
    "        parts = os.path.basename(filename).split('_')\n",
    "        for part in parts:\n",
    "            if part.startswith('f') and part[1:].replace('n', '').replace('w', '').replace('m', '').isdigit():\n",
    "                return part.lower()\n",
    "    # For .dat files: F070M.dat → \"f070m\"\n",
    "    elif filename.endswith('.dat'):\n",
    "        return os.path.splitext(os.path.basename(filename))[0].lower()\n",
    "    return None\n",
    "\n",
    "def get_filter_number(filter_name):\n",
    "    '''extracts numbers from filter name (drops F, N, W, etc from the ends)\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter_files : type = list - list of filter names\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    filter name as string\n",
    "    '''   \n",
    "    match = re.search(r'[A-Za-z](\\d+)[A-Za-z]', filter_name)  # Numbers between ANY letters\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "def generate_list_of_files(filter_directory, image_directory):\n",
    "    '''cross-matches files in filter_directory to images in image_directory, sorts by filter number.\n",
    "    Filters will be duplicated if multiple image files for the same filter are supplied.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter_directory - type = string : Must be followed by a folder /miri and /nircam to differentiate detectors\n",
    "    image_directory - type = string : All files with .fits extention will be grabbed.\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    list of arrays, first entry is the image file array, second is the filter file array, both sorted by filter numer (in name)\n",
    "    '''   \n",
    "    #filter_directory = '/d/crow1/tools/cigale/database_builder/filters/jwst/'\n",
    "    path = ['nircam', 'miri']\n",
    "    filter_files = np.concatenate([glob.glob(os.path.join(filter_directory + file_path, \"*.dat\")) for file_path in path])\n",
    "    #image_directory = 'Data_files/Image_files'\n",
    "    image_files = glob.glob(os.path.join(image_directory, \"*.fits\"))\n",
    "    # Initialize aligned lists\n",
    "    image_file_array = []\n",
    "    filter_file_array = []\n",
    "    \n",
    "    # Loop through .fits files and find matching .dat files\n",
    "    for fits_file in image_files:\n",
    "        fits_filter = extract_filter_name(fits_file)\n",
    "        if not fits_filter:\n",
    "            continue  # Skip if no filter name found\n",
    "        \n",
    "        # Search for matching .dat file\n",
    "        for dat_file in filter_files:\n",
    "            dat_filter = extract_filter_name(dat_file)\n",
    "            if dat_filter == fits_filter:\n",
    "                image_file_array.append(fits_file)\n",
    "                filter_file_array.append(dat_file)\n",
    "                break  # Stop searching after first match\n",
    "    filter_name_array = [f.split(\"/\")[-1] for f in filter_file_array] #TJ generate array of just the filter names\n",
    "    filter_numbers = np.array([get_filter_number(file) for file in filter_name_array]) #TJ generate array of just filter numbers\n",
    "    sort_indices = np.argsort(filter_numbers) #TJ sort by these numbers\n",
    "    sorted_filter_names = np.array(filter_file_array)[sort_indices]\n",
    "    sorted_image_files = np.array(image_file_array)[sort_indices]\n",
    "    return sorted_image_files, sorted_filter_names\n",
    "\n",
    "image_files, filter_files = generate_list_of_files(filter_directory, image_directory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def try_float(x):\n",
    "    '''Try to convert item to float, if that fails, leave it as the type that it is, likely a string\n",
    "    -------------\n",
    "    Parameters\n",
    "    -------------\n",
    "    x : type = variable - item to be converted to float if possible\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    item passed as argument, converted to float if it can be\n",
    "    '''\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return x\n",
    "\n",
    "\n",
    "def gaussian_func(x, amplitude, xmean, stddev):\n",
    "    '''classic gaussian profile\n",
    "\n",
    "    -------------\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    x :  type = float - value to be passed on the x-axis to get a y-axis value\n",
    "    amplitude :  type = float - maximum height of the gaussian\n",
    "    xmean : type = float - line center\n",
    "    stddev : type = float - standard deviation of the gaussian\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    A single y-value based on the given x-value and other parameters\n",
    "    '''\n",
    "    return (amplitude * np.exp(-0.5 * ((x - xmean) / stddev)**2))\n",
    "\n",
    "\n",
    "def voigt(x, amp, center, sigma, gamma):\n",
    "    '''classic voigt profile\n",
    "\n",
    "    -------------\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    x :  type = float - value to be passed on the x-axis to get a y-axis value\n",
    "    amp :  type = float - maximum height of the voigt\n",
    "    center : type = float - line center\n",
    "    sigma : type = float - standard deviation of the Gaussian contribution\n",
    "    gamma : type = float - Full Width Half Max of the Lorenzian contribution\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    A single y-value based on the given x-value and other parameters\n",
    "    '''\n",
    "    profile = voigt_profile(x - center, sigma, gamma)\n",
    "    return amp * profile / np.max(profile)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_continuum_around(wavelength_array, flux_array, feature_index, window_size=25, iqr_mult=1.5):\n",
    "    '''Calculates the surrounding continuum around a feature using robust statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wavelength_array : array-like\n",
    "        Array of wavelengths\n",
    "    flux_array : array-like\n",
    "        Array of flux values (must match wavelength_array length)\n",
    "    feature_index : int\n",
    "        Index of the feature center in the arrays\n",
    "    window_size : int, optional\n",
    "        Number of indices to search for continuum (default: 25)\n",
    "    iqr_mult : float, optional\n",
    "        Multiplier for IQR outlier rejection (default: 1.5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean_cont : float\n",
    "        Robust average continuum flux\n",
    "    stdev : float\n",
    "        Robust standard deviation of continuum\n",
    "    '''\n",
    "    \n",
    "    n = len(wavelength_array)\n",
    "    start = max(0, feature_index - window_size)\n",
    "    end = min(n, feature_index + window_size + 1)\n",
    "    \n",
    "    # Create continuum window (excluding feature core)\n",
    "    feature_window = slice(max(0, feature_index-2), min(n, feature_index+3))\n",
    "    cont_window = np.r_[slice(start, feature_window.start), \n",
    "                       slice(feature_window.stop, end)]\n",
    "    window_fluxes = flux_array[cont_window]\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if len(window_fluxes) < 3:  # Need at least 3 points for meaningful stats\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Calculate robust continuum bounds\n",
    "    q25, q75 = np.nanpercentile(window_fluxes, [25, 75])\n",
    "    iqr = q75 - q25\n",
    "    lower_bound = q25 - iqr_mult * iqr\n",
    "    upper_bound = q75 + iqr_mult * iqr\n",
    "    \n",
    "    # Filter valid continuum points\n",
    "    good_flux = window_fluxes[(window_fluxes >= lower_bound) & \n",
    "                            (window_fluxes <= upper_bound) & \n",
    "                            ~np.isnan(window_fluxes)]\n",
    "    \n",
    "    return np.nanmean(good_flux), np.nanstd(good_flux)\n",
    "\n",
    "def assign_feature_weights(wavelength_array, flux_array, continuum_array, sigma_cont, extra_cont_points=2, feature_weight=10, continuum_weight=1):\n",
    "    \"\"\"\n",
    "    Returns an array of weights for each data point in the wavelength_array.\n",
    "    \n",
    "    Points above (continuum + sigma_cont) are assigned feature_weight.\n",
    "    First and last extra_cont_points are also assigned feature_weight to anchor baseline.\n",
    "    \"\"\"\n",
    "    weights = np.full_like(flux_array, continuum_weight, dtype=float)\n",
    "    \n",
    "    # Identify feature indices\n",
    "    feature_indices = np.where(flux_array > (continuum_array + sigma_cont))[0]\n",
    "    \n",
    "    # Assign higher weights to feature\n",
    "    weights[feature_indices] = feature_weight\n",
    "    \n",
    "    # Anchor: assign extra points before and after feature\n",
    "    if len(feature_indices) > 0:\n",
    "        first = feature_indices[0]\n",
    "        last = feature_indices[-1]\n",
    "        \n",
    "        # Assign higher weight to N points before the feature starts\n",
    "        start_anchor = max(0, first - extra_cont_points)\n",
    "        weights[start_anchor:first] = feature_weight\n",
    "        \n",
    "        # Assign higher weight to N points after the feature ends\n",
    "        end_anchor = min(len(weights), last + extra_cont_points + 1)\n",
    "        weights[last+1:end_anchor] = feature_weight\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def fit_voigt_to(wavelength_of_feature, tolerance, wavelength_array, flux_array, type = True, units = 1e+6, show_plot = False, feature_idx_width = 6):\n",
    "    '''Fits voigt profile to feature nearest to given wavelength.\n",
    "\n",
    "\n",
    "    need to add backup trial\n",
    "\n",
    "    -------------\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    wavelength_of_feature : type = float - wavelength closest \n",
    "    tolerance : type = float - number of units (u argument) that the center of the feature can be and still achieve a tag of 2\n",
    "    wavelength_array :  type = float - array of wavelengths including features\n",
    "    flux_array : type = list - flux density array at each corresponding wavelength\n",
    "    type : type = boolean : True if emission feature, False if absorption\n",
    "    u (optional, defaults to 1e+6 (microns)) : type = float - unit to convert to meters\n",
    "    Show_plot (optional, defaults to false) : type = boolean - show plot of fit?\n",
    "    feature_idx_width (optional, defaults to 6) : type = int - number of indexes on each side of the feature's center to fit to\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    [xrange, fitted] : type = list - plotting datapoints in [x,y] format\n",
    "    total_feature_flux : type = float - integrated flux in units of {flux}\n",
    "    center_WL, \n",
    "    this_features_snr, \n",
    "    chi2, reduced_chi2, \n",
    "    [*params], \n",
    "    tag : type = boolean : 0 representing bad fit or feature not found, 1 representing decent fit, no warnings triggered\n",
    "    '''   \n",
    "    if len(wavelength_array) != len(flux_array):\n",
    "        print(f'wavelength and flux array must be same length, instead len(wavelength) = {len(wavelength)}, but len(flux array) = {len(flux_array)}')\n",
    "        return None\n",
    "    \n",
    "    voigt_func = [] #TJ initialize array of x,y data for each voigt function\n",
    "    \n",
    "    center_idx = np.argmin(np.abs(wavelength_array - wavelength_of_feature)) #TJ assign the center index as the closest wavelength to the expected wavelength\n",
    "    continuum, cont_std = get_continuum_around(wavelength_array, flux_array, center_idx) #TJ get continuum and continuum stddev\n",
    "    idx_range = range(int(center_idx-np.floor(feature_idx_width/2)),int(center_idx+np.ceil(feature_idx_width/2)))\n",
    "    plt_range = range(min(idx_range)-feature_idx_width, max(idx_range)+feature_idx_width)\n",
    "    weights = assign_feature_weights(wavelength_array[idx_range], flux_array[idx_range], continuum, cont_std)\n",
    "    x_data = wavelength_array[idx_range] #TJ generate the x data as the 20 nearest datapoints\n",
    "    y_data = flux_array[idx_range] - continuum #TJ correct y-data for the net above continuum\n",
    "    #flux_uncertainty = flux_unc[idx_range] #TJ assign uncertainty array\n",
    "    # Initial guesses\n",
    "    amp_guess = max(flux_array[center_idx-1:center_idx+1]-continuum) if type else min((flux_array[center_idx-1:center_idx+1]-continuum))\n",
    "    mean_guess = wavelength_array[center_idx]\n",
    "    half_max = amp_guess / 2\n",
    "    indices_above_half = np.where(y_data > half_max)[0]\n",
    "\n",
    "    if len(indices_above_half) >= 2:\n",
    "        fwhm = x_data[indices_above_half[-1]] - x_data[indices_above_half[0]]\n",
    "        sigma_guess = fwhm / 2.355\n",
    "    else:\n",
    "        sigma_guess = wavelength_array[center_idx+1] - wavelength_array[center_idx]  # fallback\n",
    "    gamma_guess = sigma_guess / 2\n",
    "    amp_bounds = [amp_guess*0.75, amp_guess*1.25] \n",
    "    \n",
    "    bounds = ([min(amp_bounds), wavelength_array[center_idx-tolerance], 0, 0], [max(amp_bounds), wavelength_array[center_idx+tolerance], np.inf, np.inf])\n",
    "    \n",
    "    params, cov = curve_fit(voigt, x_data, y_data, p0=[amp_guess, mean_guess, sigma_guess, gamma_guess], bounds=bounds, sigma=1/weights, absolute_sigma=False, maxfev=20000)\n",
    "\n",
    "    xrange = np.linspace(min(wavelength_array[plt_range]),max(wavelength_array[plt_range]), len(wavelength_array[plt_range])*100) #TJ define high resolution xrange for plotting\n",
    "    fitted = voigt(xrange, *params) #TJ create the fitted y-data\n",
    "    total_feature_flux = np.trapz(fitted, xrange) #TJ integrate over fitted voigt to get total flux\n",
    "    this_features_snr = params[0]/cont_std #TJ snr is just amp divided by the noise in continuum\n",
    "    center_WL = params[1] #TJ assign center of the feature for redshift/velocity calculations\n",
    "    this_feature_flux = flux_array[idx_range]\n",
    "    #this_features_unc = flux_unc[idx_range]\n",
    "    residuals = this_feature_flux - voigt(wavelength_array[idx_range], *params)\n",
    "    #chi2 = np.sum((residuals / this_features_unc)**2)\n",
    "    #dof = len(y_data) - len(params)\n",
    "    #reduced_chi2 = chi2 / dof\n",
    "\n",
    "    if this_features_snr > 4:\n",
    "        tag = 1\n",
    "    else:\n",
    "        tag = 0\n",
    "    voigt_func = [[xrange, fitted], total_feature_flux, center_WL, this_features_snr, [*params], tag]\n",
    "    if show_plot:\n",
    "        plt.plot(wavelength_array[plt_range], flux_array[plt_range]-continuum, label='Continuum-Subtracted', color='purple')\n",
    "        plt.axvline(x=voigt_func[2], label = f'center_WL={round(params[1]*u.m.to(u.angstrom))}A')\n",
    "        if tag == 1:\n",
    "            plt.plot(xrange, fitted, color='blue', label=f'fitted')\n",
    "        else:\n",
    "            plt.plot(xrange, fitted, color='red', label=f'poorly fitted')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return voigt_func\n",
    "\n",
    "def get_feature_statistics(rest_wl_array, transitions):\n",
    "    c = 2.99792458e+8\n",
    "    fluxes = []\n",
    "    center_wl = []\n",
    "    velocities = []\n",
    "    z_temp = []\n",
    "    for i, feature in enumerate(voigts):\n",
    "        fluxes.append(feature[1])\n",
    "        center_wl.append(feature[2])\n",
    "        rest = rest_wl_array[i]*(1e-6)\n",
    "        obs = feature[2]*(1e-6)\n",
    "        velocity = c*(obs-rest)/rest\n",
    "        velocities.append(velocity)\n",
    "        z_temp.append(((obs-rest)/rest))\n",
    "    z = np.nanmedian(z_temp)\n",
    "    return fluxes, center_wl, velocities, z\n",
    "\n",
    "def load_and_sort_convolved_Karin_spectrum(file_path):\n",
    "    '''import data and sort by wavelength from very particularly structured file. Karin's raw file has some wavelengths\n",
    "    that are out of order and screw up plotting.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    file_path : type = str - path to file with data\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    structured array ('wavelength', 'intensity', 'uncertainty') where intensity and uncertainty are in W/m2/Hz\n",
    "    '''    \n",
    "    with open(file_path, 'r') as file:\n",
    "        header = file.readline().strip().split()\n",
    "        #TJ check first line structure for compliance\n",
    "        if ((len(header) == 3) & (type(try_float(header[0])) == type(0.1)) & (type(try_float(header[1])) == type(0.1)) & (type(try_float(header[2])) == type(0.1))):\n",
    "            data_list = []\n",
    "            data_list.append((try_float(header[0])*1e-6, try_float(header[1])*1e-20, try_float(header[2])*1e-20))\n",
    "            aperture_area_sr = (np.pi * (((0.75*u.arcsec).to(u.rad))**2)).value\n",
    "            for line in file:\n",
    "                parts = line.strip().split(maxsplit=3)\n",
    "                \n",
    "                #TJ Convert numeric columns to floats\n",
    "                wavelength = float(parts[0])*1e-6 #TJ float required for sorting\n",
    "                intensity = try_float(parts[1])*1e-20 * aperture_area_sr\n",
    "                uncertainty = try_float(parts[2])*1e-20 * aperture_area_sr\n",
    "                \n",
    "                data_list.append((wavelength, intensity, uncertainty))\n",
    "        \n",
    "            #TJ Define dtype with notes as string\n",
    "            dtype = [\n",
    "                ('wavelength', float),\n",
    "                ('intensity', float),\n",
    "                ('uncertainty', float),\n",
    "            ]\n",
    "            \n",
    "            data = np.array(data_list, dtype=dtype)\n",
    "            sorted_data = np.sort(data, order=['wavelength'])  #TJ Sort by wavelength\n",
    "            \n",
    "            return sorted_data\n",
    "        else:\n",
    "            print('''File format is not as expected. Should be 3 columns no header, if not, see \"import_data_and_sort_by_wavelength\"\n",
    "            function from Flux_calibration notebook''')\n",
    "            return None\n",
    "\n",
    "\n",
    "def is_loc_in_IFU(loc, IFU_file):\n",
    "    \"\"\"\n",
    "    Check if a given coordinate lies within the spatial footprint of an IFU cube.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coord : list [ra, dec] or SkyCoord\n",
    "        Sky position to check. RA/Dec in degrees if list.\n",
    "    fits_file : str\n",
    "        Path to the IFU FITS file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the coordinate lies within the cube's spatial coverage.\n",
    "    \"\"\"\n",
    "    # Convert to SkyCoord if needed\n",
    "    if not isinstance(loc, SkyCoord):\n",
    "        loc = SkyCoord(loc[0], loc[1], unit='deg')\n",
    "\n",
    "    # Load WCS and shape from the SCI extension\n",
    "    with fits.open(IFU_file) as hdul:\n",
    "        sci_header = hdul['SCI'].header\n",
    "        sci_data = hdul['SCI'].data\n",
    "        wcs = WCS(sci_header)\n",
    "        shape = sci_data.shape  # (nz, ny, nx)\n",
    "\n",
    "    # Convert sky coordinate to pixel coordinates (ignore spectral axis)\n",
    "    x, y = skycoord_to_pixel(loc, wcs, origin=0)\n",
    "\n",
    "    # Get spatial shape\n",
    "    _, ny, nx = shape\n",
    "    return (0 <= x < nx) and (0 <= y < ny)\n",
    "\n",
    "def which_fits(filter_file, list_of_fits):\n",
    "    '''open the filter file, determine the range of wavelengths needed to compute synthetic flux through it, return which fits files\n",
    "    are needed for this particular filter. This is to save time not convolving cubes we dont need.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter_file : type = str - string to location of filter file that we are interested in.\n",
    "    list_of_fits : type = list - list of strings to the IFU fits files that you want to check\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    needed_fits : type = list - list of strings to the fits files that are actually needed\n",
    "    '''   \n",
    "    filter_data = []\n",
    "    with open(filter_file, 'r') as f:\n",
    "            header = f.readline().strip().split()\n",
    "            for line in f:\n",
    "                data_line = line.strip().split()\n",
    "                filter_data.append(data_line)\n",
    "            \n",
    "    header, filter_T = filter_data[:2], np.array(filter_data[2:])\n",
    "\n",
    "    wl = [try_float(filter_T[i,0])*1e-10 for i in range(len(filter_T))]\n",
    "    T = [try_float(filter_T[i,1]) for i in range(len(filter_T))]\n",
    "    \n",
    "    min_wl, max_wl = min(wl), max(wl)\n",
    "    needed_fits = []\n",
    "    entirely_in = []\n",
    "    for file in list_of_fits:\n",
    "        cube = SpectralCube.read(file, hdu='SCI')\n",
    "        wavelength = cube.spectral_axis\n",
    "        if (wavelength[0].value*1e-6 < max_wl) and (wavelength[-1].value*1e-6 > min_wl):\n",
    "            needed_fits.append(file)\n",
    "            if ((wavelength[0].value*1e-6 < min_wl) and (wavelength[-1].value*1e-6 > max_wl)):\n",
    "                entirely_in.append(True)\n",
    "            else:\n",
    "                entirely_in.append(False)\n",
    "    needed_fits = np.array(needed_fits)\n",
    "\n",
    "    if (sum(entirely_in) == 1):\n",
    "        return needed_fits[entirely_in]\n",
    "    elif ((len(needed_fits) > 1) & (sum(entirely_in) == 0)):\n",
    "        print(f'More than one IFU file is needed for filter {extract_filter_name(filter_file)}')\n",
    "        return needed_fits\n",
    "    elif ((len(needed_fits) > 1) & (sum(entirely_in) > 1)):\n",
    "        print(f'More than one IFU file could be used for filter {extract_filter_name(filter_file)}')\n",
    "        return needed_fits[0]\n",
    "\n",
    "def is_aperture_fully_covered(IFU_file, image_file, loc, radius):\n",
    "    '''\n",
    "    Check if a circular aperture is fully within imaged regions for both IFU and image files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    IFU_file : str\n",
    "        Path to the IFU FITS file (must have WCS and SCI extension).\n",
    "    image_file : str\n",
    "        Path to the image FITS file (must have WCS and valid data).\n",
    "    loc : tuple (ra, dec)\n",
    "        Sky coordinates of the aperture center in degrees.\n",
    "    radius : float\n",
    "        Aperture radius in arcseconds.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (bool, bool)\n",
    "        (IFU_fully_covered, image_fully_covered)\n",
    "        True if aperture is fully within imaged regions for each file.\n",
    "    '''\n",
    "    def _check_coverage(file, ext):\n",
    "        # Open file and get data + WCS\n",
    "        with fits.open(file) as hdul:\n",
    "            data = hdul[ext].data\n",
    "            header = hdul[ext].header\n",
    "            wcs = WCS(header)\n",
    "            \n",
    "            # Handle 3D IFU cubes (use first wavelength slice)\n",
    "            if data.ndim == 3:\n",
    "                data = data[0]\n",
    "            \n",
    "            # Create coverage mask (1=imaged, 0=NaN/unimaged)\n",
    "            coverage_mask = np.where(np.isnan(data) | (data == 0), 0, 1)\n",
    "            \n",
    "            # Convert sky coordinates to pixel coordinates\n",
    "            loc_sky = SkyCoord(ra=loc[0] * u.deg, dec=loc[1] * u.deg, frame='icrs')\n",
    "            x_center, y_center = wcs.celestial.all_world2pix(loc_sky.ra.deg, loc_sky.dec.deg, 0)\n",
    "            \n",
    "            # Calculate pixel scale (arcsec/pixel)\n",
    "            try:\n",
    "                pixel_scale = np.abs(header['CDELT1']) * 3600  # deg -> arcsec\n",
    "            except KeyError:\n",
    "                pixel_scale = np.sqrt(header['PIXAR_A2'])  # Fallback for JWST files\n",
    "            \n",
    "            radius_pix = radius.value / pixel_scale\n",
    "            # Measure coverage\n",
    "            aperture = CircularAperture((x_center, y_center), r=radius_pix)\n",
    "            phot_table = aperture_photometry(coverage_mask, aperture)\n",
    "            measured_area = phot_table['aperture_sum'][0]\n",
    "            expected_area = np.pi * (radius_pix ** 2)\n",
    "            \n",
    "            # Allow 1-pixel tolerance for edge effects\n",
    "            return np.isclose(measured_area, expected_area, atol=1.0)\n",
    "\n",
    "    # Check IFU file (SCI extension)\n",
    "    ifu_covered = _check_coverage(IFU_file, ext='SCI')\n",
    "    \n",
    "    # Check image file (primary HDU or SCI)\n",
    "    try:\n",
    "        image_covered = _check_coverage(image_file, ext=0)  # Try primary HDU\n",
    "    except (KeyError, AttributeError):\n",
    "        image_covered = _check_coverage(image_file, ext='SCI')  # Fallback to SCI\n",
    "    \n",
    "    return (ifu_covered, image_covered)\n",
    "\n",
    "def find_max_radius(IFU_file, image_file, loc, min_radius=0.1*u.arcsec, max_radius=10.0*u.arcsec, tolerance=0.01*u.arcsec):\n",
    "    \"\"\"\n",
    "    Find the maximum aperture radius (arcsec) fully covered in both IFU and image files.\n",
    "    Uses binary search between min_radius and max_radius.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    IFU_file : str\n",
    "        Path to IFU FITS file.\n",
    "    image_file : str\n",
    "        Path to image FITS file.\n",
    "    loc : tuple (ra, dec)\n",
    "        Sky coordinates in degrees.\n",
    "    min_radius : float\n",
    "        Minimum aperture radius to test (arcsec).\n",
    "    max_radius : float\n",
    "        Maximum aperture radius to test (arcsec).\n",
    "    tolerance : float\n",
    "        Precision threshold for convergence (arcsec).\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Maximum fully covered radius (arcsec), or 0 if no valid radius found.\n",
    "    \"\"\"\n",
    "    def _is_covered(radius):\n",
    "        ifu_ok, image_ok = is_aperture_fully_covered(IFU_file, image_file, loc, radius)\n",
    "        return ifu_ok and image_ok\n",
    "    \n",
    "    # Binary search\n",
    "    best_radius = 0.0\n",
    "    while max_radius - min_radius > tolerance:\n",
    "        mid_radius = (min_radius + max_radius) / 2\n",
    "        if _is_covered(mid_radius):\n",
    "            best_radius = mid_radius\n",
    "            min_radius = mid_radius  # Try larger radii\n",
    "        else:\n",
    "            max_radius = mid_radius  # Try smaller radii\n",
    "    IFU_pix_scale = (fits.open(IFU_file)['SCI'].header['CDELT2']*u.deg).to(u.arcsec)\n",
    "    image_pix_scale = (fits.open(image_files[0])['SCI'].header['CDELT2']*u.deg).to(u.arcsec)\n",
    "    return (best_radius if best_radius > 0 else 0.0), best_radius/IFU_pix_scale,  best_radius/image_pix_scale\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_IFU_spectrum(IFU_filepath, loc, radius, replace_negatives = False):\n",
    "    '''extract spectrum from IFU file with aperature of radius, centered at ra,dec = loc\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    IFU_filepath : type = str - string to location of IFU fits file\n",
    "    loc : type = list - ra, dec in degrees or SkyCoord object\n",
    "    radius : type = float - radius of aperture, must have units attached (like u.deg or u.arcsecond)\n",
    "    replace_negatives (optional, defaults to nothing) : type = float : replace negative fluxes with this float times the smallest positive flux value, specify             as None to leave as negative values\n",
    "    Returns\n",
    "    -------------\n",
    "    structured array with entries for \"wavelength\" and \"intensity\" with respective units of (m) and (W/m2/Hz)\n",
    "    '''   \n",
    "    #fake_missing_header_info(IFU_filepath) #TJ run this if needed\n",
    "    hdul = fits.open(IFU_filepath)\n",
    "    header = hdul['SCI'].header\n",
    "    wcs = WCS(header)\n",
    "    cube = SpectralCube.read(IFU_filepath, hdu='SCI')\n",
    "\n",
    "    # === CONVERT RA/DEC TO PIXEL COORDINATES ===\n",
    "    # Create SkyCoord object for spatial coordinates\n",
    "    if type(loc) == list:\n",
    "        spatial_coords = SkyCoord(ra=loc[0]*u.deg, dec=loc[1]*u.deg)\n",
    "    elif type(loc) == SkyCoord:\n",
    "        spatial_coords = loc\n",
    "    else:\n",
    "        print('loc is not a list of ra, dec and it is not a SkyCoord object.')\n",
    "        return None\n",
    "    \n",
    "    # Convert spatial coordinates to pixels\n",
    "    x, y = wcs.celestial.all_world2pix(spatial_coords.ra.deg, \n",
    "                                      spatial_coords.dec.deg, 0)\n",
    "    \n",
    "    # === BUILD APERTURE ===\n",
    "    if (header['CDELT2'] != header['CDELT1']) and (header['CDELT2'] != -header['CDELT1']):\n",
    "        print('pixels are not square! function revisit get_IFU_spectrum() function to fix')\n",
    "        print(header['CDELT2'], 'in y vs ', header['CDELT1'], ' in x')\n",
    "        return None\n",
    "    cdelt = np.abs(header['CDELT2']) * u.deg\n",
    "    pixel_scale = cdelt.to(u.arcsec)  # arcsec/pixel\n",
    "    pix_area = header['PIXAR_SR'] #TJ pixel area in steradians\n",
    "    radius = radius.to(u.arcsec)\n",
    "    radius_pix = (radius / pixel_scale).value\n",
    "    aperture = CircularAperture((x, y), r=radius_pix)\n",
    "    aperture_area_sr = np.pi * (radius.to(u.rad))**2\n",
    "\n",
    "    # === CRITICAL UNIT HANDLING ===\n",
    "    cube = cube.with_spectral_unit(u.m)  # Ensure wavelength in meters\n",
    "    \n",
    "    # Convert flux units properly\n",
    "    # Step 1: MJy/sr → W/m²/Hz/sr\n",
    "    cube = cube.to(u.W/(u.m**2 * u.Hz * u.sr))  \n",
    "    \n",
    "    # Step 2: Multiply by pixel area to get W/m²/Hz/pixel\n",
    "    pix_area_sr = header['PIXAR_SR'] * u.sr\n",
    "    cube = cube * pix_area_sr\n",
    "    \n",
    "    # Step 3: Perform aperture sum (now in W/m²/Hz)\n",
    "    flux_density_spectrum = []\n",
    "    nan_detected = 0\n",
    "    for i in range(len(cube.spectral_axis)):\n",
    "        image_slice = cube[i].value  # Now in W/m²/Hz\n",
    "        if replace_negatives is not False:\n",
    "            if replace_negatives == 0:\n",
    "                image_slice[image_slice < 0] = 0\n",
    "            else:\n",
    "                min_positive = np.nanmin(image_slice[image_slice > 0])\n",
    "                image_slice[image_slice < 0] = replace_negatives * min_positive\n",
    "        if ~np.isnan(image_slice).sum() == 0:\n",
    "            print('An entire wavelength slice in the cube is NaN')\n",
    "        phot = aperture_photometry(image_slice, aperture)\n",
    "        if (np.isnan(phot['aperture_sum'][0]).sum() !=0):                \n",
    "            print(f\"{np.isnan(phot['aperture_sum'][0]).sum()} nan values detected for wl[{i}]: {cube.spectral_axis[i]}, this makes {nan_detected}\", end=\"\\r\")\n",
    "            nan_detected += 1\n",
    "\n",
    "        phot = aperture_photometry(np.nan_to_num(image_slice), aperture)\n",
    "        flux_density_spectrum.append(phot['aperture_sum'][0])  #TJ No extra multiplication! already in correct units\n",
    "    if nan_detected != 0:\n",
    "        print(f'A total of {nan_detected} were detected within {radius} in {IFU_filepath.split(\"/\")[-1]} over {len(cube.spectral_axis)} WLs')\n",
    "    wavelengths = cube.spectral_axis.to(u.m).value\n",
    "    flux_density_spectrum = np.array(flux_density_spectrum)\n",
    "\n",
    "\n",
    "    if replace_negatives is not False:\n",
    "        min_positive = min(flux_density_spectrum[flux_density_spectrum > 0])\n",
    "        flux_density_spectrum[flux_density_spectrum < 0] = replace_negatives*min_positive  #TJ replace negative numbers with a very small positive value\n",
    "\n",
    "\n",
    "    dtype = [('wavelength', 'f8'), ('intensity', 'f8')]\n",
    "    spectrum = np.zeros(len(cube.spectral_axis), dtype=dtype)\n",
    "    spectrum['wavelength'] = cube.spectral_axis.to(u.m).value\n",
    "    spectrum['intensity'] = np.array(flux_density_spectrum)\n",
    "\n",
    "    return spectrum\n",
    "\n",
    "def find_point_spectrum(IFU_filepath, loc):\n",
    "    '''extract the pixel spectrum with bilinear interpolation for a location in ra,dec\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    IFU_filepath : type = str - string to location of IFU fits file\n",
    "    loc : type = list - ra, dec in degrees or SkyCoord object\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    SpectralCube slice corresponding to the interpolated spectrum\n",
    "    '''   \n",
    "    hdul = fits.open(IFU_filepath)\n",
    "    header = hdul['SCI'].header\n",
    "    wcs = WCS(header)\n",
    "    cube = SpectralCube.read(IFU_filepath, hdu='SCI')\n",
    "\n",
    "    # === CONVERT RA/DEC TO PIXEL COORDINATES ===\n",
    "    # Create SkyCoord object for spatial coordinates\n",
    "    if type(loc) == list:\n",
    "        spatial_coords = SkyCoord(ra=loc[0]*u.deg, dec=loc[1]*u.deg)\n",
    "    elif type(loc) == SkyCoord:\n",
    "        spatial_coords = loc\n",
    "    else:\n",
    "        print('loc is not a list of ra, dec and it is not a SkyCoord object.')\n",
    "        return None\n",
    "    \n",
    "    # Convert spatial coordinates to pixels\n",
    "    x, y = wcs.celestial.all_world2pix(spatial_coords.ra.deg, \n",
    "                                      spatial_coords.dec.deg, 0)\n",
    "    \n",
    "    # Get integer and fractional parts\n",
    "    x_int, y_int = int(np.floor(x)), int(np.floor(y))\n",
    "    x_frac, y_frac = x - x_int, y - y_int\n",
    "    \n",
    "    # Ensure we don't go out of bounds\n",
    "    x_max = cube.shape[2] - 1\n",
    "    y_max = cube.shape[1] - 1\n",
    "    \n",
    "    x0 = max(0, min(x_int, x_max))\n",
    "    x1 = max(0, min(x_int + 1, x_max))\n",
    "    y0 = max(0, min(y_int, y_max))\n",
    "    y1 = max(0, min(y_int + 1, y_max))\n",
    "    \n",
    "    # Get the four surrounding spectra\n",
    "    spec00 = cube[:, y0, x0]\n",
    "    spec01 = cube[:, y0, x1]\n",
    "    spec10 = cube[:, y1, x0]\n",
    "    spec11 = cube[:, y1, x1]\n",
    "    \n",
    "    # Perform bilinear interpolation\n",
    "    interpolated_spectrum = (spec00 * (1 - x_frac) * (1 - y_frac) +\n",
    "                            spec01 * x_frac * (1 - y_frac) +\n",
    "                            spec10 * (1 - x_frac) * y_frac +\n",
    "                            spec11 * x_frac * y_frac)\n",
    "    \n",
    "    return interpolated_spectrum\n",
    "\n",
    "def get_Fnu_transmission(Fnu_array, wl_array, transmission_array, trans_wl_array, warnings = True):\n",
    "    '''get expected flux through filter. Assumes Fnu array is in W/m2/Hz and wl array is in meters. Otherwise, units will be weird.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    Fnu_array : type = array - array of flux density values\n",
    "    wl_array : type = array - array of wavelength values for the corresponding Fnu_array values (should be in meters)\n",
    "    transmission_array : type = array - array of unitless transmission coefficient\n",
    "    trans_wl_array : type = array - array of wavelength values for the corresponding transmission values (should be in meters)\n",
    "\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    total_flux : type = float - Ideally in units of W/m2\n",
    "    '''   \n",
    "    if ((trans_wl_array[0] < wl_array[0]) or (trans_wl_array[-1] > wl_array[-1])): #TJ Check if wavelengths are compatible with filter\n",
    "        if warnings:\n",
    "            print(f'filter goes from {trans_wl_array[0]} to {trans_wl_array[-1]}, but provided Fnu array goes from {wl_array[0]} to {wl_array[-1]}')\n",
    "        idx_start = np.searchsorted(trans_wl_array, wl_array[0], side='left')\n",
    "        idx_end = np.searchsorted(trans_wl_array, wl_array[-1], side='right')\n",
    "        \n",
    "        # Expand by one index if possible\n",
    "        idx_start = max(0, idx_start - 1)  # Include one lower index\n",
    "        idx_end = min(len(trans_wl_array), idx_end + 1)  # Include one higher index\n",
    "        \n",
    "        # Slice transmission data\n",
    "        trans_wl_array = trans_wl_array[idx_start:idx_end]\n",
    "        transmission_array = transmission_array[idx_start:idx_end]\n",
    "    if len(trans_wl_array) == 0:\n",
    "        raise ValueError(\"No overlap between flux wavelengths and filter transmission curve\")\n",
    "    #TJ convert all arrays to numpy arrays for better indexing and convert to MKS units\n",
    "    \n",
    "    Fnu_array = np.array(Fnu_array)\n",
    "    wl_array = np.array(wl_array)\n",
    "    transmission_array = np.array(transmission_array)\n",
    "    trans_wl_array = np.array(trans_wl_array)\n",
    "\n",
    "    \n",
    "    #TJ Convert wavelength to frequency, reverse so freq increases left to right\n",
    "    spec_freq_array = c / wl_array[::-1]\n",
    "    Fnu_array = Fnu_array[::-1]\n",
    "    trans_freq_array = c / trans_wl_array[::-1]\n",
    "    transmission_array = transmission_array[::-1]\n",
    "\n",
    "    #TJ Interpolate Fnu onto the transmission frequency grid\n",
    "    #TJ this is because jwst transmission arrays are averages over BW widths which are much coarser than Fnu is.\n",
    "    interp_Fnu = np.interp(trans_freq_array, spec_freq_array, Fnu_array)\n",
    "    \n",
    "    \n",
    "    weight = transmission_array / trans_freq_array #TJ weight the numerator and denominator by T *d_nu over nu for integration\n",
    "    numerator = np.trapz(interp_Fnu * weight, trans_freq_array)#TJ perform integration\n",
    "    denominator = np.trapz(weight, trans_freq_array)\n",
    "    ab_mean_flux = numerator / denominator\n",
    "    # Numerator: Fν * Transmission / nu integrated over frequency\n",
    "    \n",
    "    return ab_mean_flux.value\n",
    "\n",
    "def get_image_flux(image_file, loc, radius, replace_negatives = False):\n",
    "    '''extract flux from image file with aperature of radius, centered at ra,dec = loc\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    image_file : type = str - string to location of image fits file\n",
    "    loc : type = list - ra, dec in degrees or SkyCoord object\n",
    "    radius : type = float - radius of aperture, must have units attached (like u.deg or u.arcsecond)\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    flux_density observed through filter\n",
    "    '''\n",
    "    #TJ assign location as SkyCoord object\n",
    "    if type(loc) == list:\n",
    "        spatial_coords = SkyCoord(ra=loc[0]*u.deg, dec=loc[1]*u.deg)\n",
    "    elif type(loc) == SkyCoord:\n",
    "        spatial_coords = loc\n",
    "    else:\n",
    "        print('loc is not a list of ra, dec and it is not a SkyCoord object.')\n",
    "        return None\n",
    "    hdul = fits.open(image_file) #TJ load file\n",
    "    \n",
    "    data = hdul['SCI'].data*1e-20  #TJ convert flux density to mks units\n",
    "    if replace_negatives is not False:\n",
    "        min_positive = min(data[data>0])\n",
    "        data[data<0] = replace_negatives*min_positive\n",
    "    header = hdul['SCI'].header #TJ load header\n",
    "    if header['BUNIT'] != 'MJy/sr': #TJ check if units are MJy/sr, output will be nonsensical if not\n",
    "        print('flux is NOT in MJy/sr. review get_image_flux function to fix')\n",
    "        return None\n",
    "    pix_area = header[\"PIXAR_SR\"] #TJ define the angular size of a pixel in staradians\n",
    "    wcs = WCS(header) #TJ read in the world coordinate system\n",
    "    radius_pixels = (radius).to_value(u.deg) / abs(header['CDELT2']) #TJ get the radius of the aperture in number of pixels\n",
    "    \n",
    "    #TJ Convert RA/Dec to pixel coordinates\n",
    "    x, y = wcs.all_world2pix(spatial_coords.ra.deg, spatial_coords.dec.deg, 0)\n",
    "    aperture = CircularAperture((x, y), r = radius_pixels)\n",
    "    \n",
    "    #TJ Perform aperture photometry\n",
    "    phot_result = aperture_photometry(data, aperture)\n",
    "    total_flux = phot_result['aperture_sum'][0]*pix_area  #TJ the result is in pixel units, multiply by steradians per pixel to get units right\n",
    "\n",
    "    return total_flux    \n",
    "\n",
    "\n",
    "def stitch_spectra(fits_files, loc, radius, anchor_idx=0, replace_negatives=False):\n",
    "    \"\"\"\n",
    "    Stitch a list of IFU spectra keeping the spectrum at `anchor_idx` fixed.\n",
    "    Each other spectrum is shifted (additive offset) to match the anchor/combined\n",
    "    spectrum in the overlapping wavelength region before being prepended/appended.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_files : list of str (can also be two dictionaries with keys for \"wavelength\" and \"intensity\"\n",
    "    loc, radius : passed to get_IFU_spectrum(...)\n",
    "    anchor_idx : int\n",
    "        Index in fits_files of the spectrum to keep fixed (anchor).\n",
    "    replace_negatives : bool\n",
    "        Passed through to get_IFU_spectrum.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    combined : dict with keys 'wavelength' (1D np.array) and 'intensity' (1D np.array)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        anchor = fits_files[anchor_idx]\n",
    "        test = anchor['wavelength'][0]\n",
    "    except:\n",
    "        anchor = get_IFU_spectrum(fits_files[anchor_idx], loc, radius, replace_negatives=replace_negatives) #TJ extract anchor spectrum\n",
    "\n",
    "    base_wl = np.asarray(anchor['wavelength']).astype(float) #TJ convert to np array for better indexing\n",
    "    base_flux = np.asarray(anchor['intensity']).astype(float)\n",
    "    base = {'wavelength': base_wl, 'intensity': base_flux} #TJ save as dictionary of np arrays\n",
    "\n",
    "    for i in range(anchor_idx - 1, -1, -1): #TJ start from anchor index and go backward to zero\n",
    "        print(f\"Stitching LEFT: file {i} → anchor\")\n",
    "        try:\n",
    "            cur = fits_files[i]\n",
    "            test = cur['wavelength']\n",
    "        except:\n",
    "            cur = get_IFU_spectrum(fits_files[i], loc, radius, replace_negatives=replace_negatives) #TJ extract new spectrum to be stitched\n",
    "        cur_wl = np.asarray(cur['wavelength']).astype(float)\n",
    "        cur_flux = np.asarray(cur['intensity']).astype(float)\n",
    "        cur = {'wavelength': cur_wl, 'intensity': cur_flux}\n",
    "\n",
    "        base = _stitch_base_with_new(base, cur, side='left') #TJ stitch the two sections together\n",
    "\n",
    "    for i in range(anchor_idx + 1, len(fits_files)): #TJ start from anchor index and go forwards to max index\n",
    "        print(f\"Stitching RIGHT: anchor → file {i}\")\n",
    "        try:\n",
    "            cur = fits_files[i]\n",
    "            test = cur['wavelength']\n",
    "        except:\n",
    "            cur = get_IFU_spectrum(fits_files[i], loc, radius, replace_negatives=replace_negatives) #TJ load the new spectrum to be stitched\n",
    "        cur_wl = np.asarray(cur['wavelength']).astype(float)\n",
    "        cur_flux = np.asarray(cur['intensity']).astype(float)\n",
    "        cur = {'wavelength': cur_wl, 'intensity': cur_flux}\n",
    "\n",
    "        base = _stitch_base_with_new(base, cur, side='right') #TJ stitch onto existing spectrum\n",
    "\n",
    "    print(f'Combined spectrum: {base[\"wavelength\"][0]} -- {base[\"wavelength\"][-1]}')\n",
    "    return base\n",
    "\n",
    "\n",
    "def _stitch_base_with_new(base, new, side='right'):\n",
    "    \"\"\"\n",
    "    Shift `new` to match `base` in overlap, then concatenate.\n",
    "    - side='right': new lies at (or mostly) longer wavelengths than base -> append\n",
    "    - side='left' : new lies at (or mostly) shorter wavelengths than base -> prepend\n",
    "\n",
    "    The function always shifts `new` (not `base`) so that `base` remains fixed.\n",
    "    \"\"\"\n",
    "    bw, bi = base['wavelength'], base['intensity'] #TJ extract wavelength and intensity arrays from dictionaries\n",
    "    nw, ni = new['wavelength'], new['intensity']\n",
    "\n",
    "    if bw.size == 0: #TJ check that arrays are not empty\n",
    "        return new.copy()\n",
    "    if nw.size == 0:\n",
    "        return base.copy()\n",
    "\n",
    "    overlap_min = max(bw[0], nw[0]) #TJ calculate edge values for overlapping region\n",
    "    overlap_max = min(bw[-1], nw[-1])\n",
    "\n",
    "    mask_b = (bw >= overlap_min) & (bw <= overlap_max) & np.isfinite(bi) #TJ create mask in base array to select overlapping region. Ignore nans in flux array\n",
    "    mask_n = (nw >= overlap_min) & (nw <= overlap_max) & np.isfinite(ni)\n",
    "    \n",
    "    offset = 0.0 #TJ initialize offset to be zero\n",
    "\n",
    "    if np.count_nonzero(mask_b) > 0 and np.count_nonzero(mask_n) >= 2: #TJ need overlapping region to be at least two points to be able to interpolate\n",
    "        try: #TJ try to interpolate new spectrum's intensity into base spectrums wavelengths\n",
    "            interp_ni_on_b = np.interp(bw[mask_b], nw[mask_n], ni[mask_n])\n",
    "            diffs = bi[mask_b] - interp_ni_on_b #TJ create array of differences between interpolated new and base intensities\n",
    "            offset = np.nanmedian(diffs) #TJ use the median value of that differences array as the offset\n",
    "        except Exception:\n",
    "            offset = 0.0 #TJ if that fails, go back to zero as the offset\n",
    "    else:\n",
    "        #TJ this should never trigger\n",
    "        print('This print statement should not trigger, check the overlap between regions')\n",
    "        nmatch = min(10, bw.size, nw.size) #TJ use the edge 10 values to estimate offset if there is no overlap\n",
    "        if nmatch >= 1:\n",
    "            if side == 'right':\n",
    "                #TJ match base's right edge to new's left edge\n",
    "                base_edge_med = np.nanmedian(bi[-nmatch:]) #TJ grab the median of the base array's rightmost 10 values\n",
    "                new_edge_med = np.nanmedian(ni[:nmatch]) #TJ grab the median of the new array's leftmost 10 values\n",
    "                offset = base_edge_med - new_edge_med #TJ offset is the difference in the medians (note the difference between median of differences)\n",
    "            else:  #TJ repeat with opposite sides for the leftward stitch\n",
    "                base_edge_med = np.nanmedian(bi[:nmatch])\n",
    "                new_edge_med = np.nanmedian(ni[-nmatch:])\n",
    "                offset = base_edge_med - new_edge_med\n",
    "        else:\n",
    "            offset = 0.0\n",
    "\n",
    "    ni_corr = ni + offset #TJ new intensity is corrected by the offset\n",
    "\n",
    "    #TJ concatenate while keeping base intact and only adding the non-overlapping portion of new\n",
    "    if side == 'right':\n",
    "        #TJ find indices to keep in the new array\n",
    "        keep_idx = nw > bw[-1]\n",
    "        if np.any(keep_idx):\n",
    "            new_wl = np.concatenate([bw, nw[keep_idx]])\n",
    "            new_flux = np.concatenate([bi, ni_corr[keep_idx]])\n",
    "        else:\n",
    "            #TJ if new array is entirely contained within the previous array, do nothing, just copy\n",
    "            new_wl, new_flux = bw.copy(), bi.copy()\n",
    "    else:  #TJ repeat with other side\n",
    "        # Prepend new wavelengths strictly before base start\n",
    "        keep_idx = nw < bw[0]\n",
    "        if np.any(keep_idx):\n",
    "            new_wl = np.concatenate([nw[keep_idx], bw])\n",
    "            new_flux = np.concatenate([ni_corr[keep_idx], bi])\n",
    "        else:\n",
    "            new_wl, new_flux = bw.copy(), bi.copy()\n",
    "\n",
    "    # Ensure final arrays are sorted ascending in wavelength (should already be, but safe)\n",
    "    if not np.all(np.diff(new_wl) > 0):\n",
    "        print('Something weird happened and now the wavelengths are not monotonically ascending!')\n",
    "\n",
    "    return {'wavelength': new_wl, 'intensity': new_flux}\n",
    "jwst_pivot_wavelengths = {\n",
    "    'F090W': 0.902e-6,\n",
    "    'F115W': 1.154e-6,\n",
    "    'F140M': 1.405e-6,\n",
    "    'F150W': 1.501e-6,\n",
    "    'F162M': 1.627e-6,\n",
    "    'F164N': 1.645e-6,\n",
    "    'F182M': 1.845e-6,\n",
    "    'F187N': 1.874e-6,\n",
    "    'F200W': 1.988e-6,\n",
    "    'F210M': 2.096e-6,\n",
    "    'F212N': 2.121e-6,\n",
    "    'F250M': 2.503e-6,\n",
    "    'F277W': 2.776e-6,\n",
    "    'F300M': 2.996e-6,\n",
    "    'F322W2': 3.247e-6,\n",
    "    'F323N': 3.237e-6, \n",
    "    'F335M': 3.362e-6,  \n",
    "    'F356W': 3.565e-6, \n",
    "    'F360M': 3.623e-6, \n",
    "    'F405N': 4.053e-6,\n",
    "    'F410M': 4.083e-6,\n",
    "    'F430M': 4.281e-6,\n",
    "    'F444W': 4.402e-6,\n",
    "    'F460M': 4.630e-6,\n",
    "    'F466N': 4.654e-6,\n",
    "    'F470N': 4.708e-6,\n",
    "    'F480M': 4.817e-6,\n",
    "    \n",
    "    # MIRI Filters (from first table)\n",
    "    'F560W': 5.635e-6,\n",
    "    'F770W': 7.639e-6,\n",
    "    'F1000W': 9.953e-6,\n",
    "    'F1130W': 11.309e-6,\n",
    "    'F1280W': 12.810e-6,\n",
    "    'F1500W': 15.064e-6,\n",
    "    'F1800W': 17.984e-6,\n",
    "    'F2100W': 20.795e-6,\n",
    "    'F2550W': 25.365e-6\n",
    "}\n",
    "\n",
    "def get_filter_wl_range(filter):\n",
    "    '''Use the filter files to determine what wavelength range we need for each filter\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter : type = str - string describing the filter name (case sensitive), for example \"F335M\"\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    Path to newly convolved file as a string\n",
    "    '''   \n",
    "    filter_files = ['/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F115W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F140M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F150W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F164N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F182M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F187N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F200W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F210M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F212N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F250M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F300M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F335M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F360M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F405N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F430M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F444W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F560W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F770W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1000W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1130W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1280W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1500W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1800W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F2100W.dat']\n",
    "    filter_file = [filer_filepath for filer_filepath in filter_files if extract_filter_name(filer_filepath).upper() == filter][0]\n",
    "    filter_data = []\n",
    "    with open(filter_file, 'r') as f:\n",
    "        header = f.readline().strip().split()\n",
    "        for line in f:\n",
    "            data_line = line.strip().split()\n",
    "            filter_data.append(data_line)\n",
    "\n",
    "    header, filter_T = filter_data[:2], np.array(filter_data[2:])\n",
    "    filter_wl = [try_float(filter_T[i,0])*1e-10 for i in range(len(filter_T))]\n",
    "    return filter_wl[0]*u.m, filter_wl[-1]*u.m\n",
    "\n",
    "####################################################################\n",
    "\n",
    "\n",
    "\n",
    "def show_images(image_files, loc, radius, ncols=3, cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Create a collage of cutout images with an aperture overlay.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_image_fits_files : list of str\n",
    "        List of FITS image file paths (must contain SCI extension).\n",
    "    loc : list, tuple, or SkyCoord\n",
    "        Location of aperture center, either [RA, Dec] in degrees or a SkyCoord object.\n",
    "    radius : Quantity\n",
    "        Aperture radius (must have angular units, e.g. arcsec).\n",
    "    ncols : int, optional\n",
    "        Number of columns in the collage (default = 3).\n",
    "    cmap : str, optional\n",
    "        Colormap for displaying images (default = 'viridis').\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure loc is SkyCoord\n",
    "    if not isinstance(loc, SkyCoord):\n",
    "        loc_sky = SkyCoord(ra=loc[0]*u.deg, dec=loc[1]*u.deg, frame='icrs')\n",
    "    else:\n",
    "        loc_sky = loc\n",
    "\n",
    "    n_images = len(image_files)\n",
    "    nrows = int(np.ceil(n_images / ncols))\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 5*nrows), \n",
    "                             subplot_kw={'projection': None})\n",
    "    axes = np.atleast_1d(axes).ravel()  # Flatten in case of 1 row/col\n",
    "    \n",
    "    for ax, image_file in zip(axes, image_files):\n",
    "        # Load FITS\n",
    "        hdu = fits.open(image_file)['SCI']\n",
    "        image = hdu.data\n",
    "        header = hdu.header\n",
    "        wcs = WCS(header, naxis=2)\n",
    "        pixel_scale = np.abs(wcs.wcs.cdelt[0]) * 3600  # arcsec/pixel\n",
    "\n",
    "        # Make cutout\n",
    "        cutout = Cutout2D(image, position=loc_sky, size=(radius*3, radius*3), wcs=wcs)\n",
    "\n",
    "        # Convert SkyCoord -> pixel coords\n",
    "        x_img, y_img = cutout.wcs.world_to_pixel(loc_sky)\n",
    "\n",
    "        # Plot\n",
    "        im = ax.imshow(cutout.data, origin='lower', cmap=cmap,\n",
    "                  norm=ImageNormalize(cutout.data, stretch=AsinhStretch(), \n",
    "                                      vmin=0, vmax=np.percentile(cutout.data, 99)))\n",
    "        ax.add_patch(Circle((x_img, y_img), \n",
    "                            (radius.to(u.arcsec).value)/pixel_scale, \n",
    "                            ec='red', fc='none', lw=2, alpha=0.7))\n",
    "        cbar = plt.colorbar(\n",
    "            im,\n",
    "            ax=ax,\n",
    "            fraction=0.046,\n",
    "            pad=0.04\n",
    "        )\n",
    "        cbar.set_label(\"Flux (native units)\", fontsize=10)\n",
    "        ax.set_title(image_file.split(\"/\")[-1], fontsize=12)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # Hide empty panels if n_images doesn’t fill full grid\n",
    "    for ax in axes[n_images:]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_image_and_synth(filter, ifu_fileset, loc, radius, image_files=image_files, color_min_max = [1, 99.5]):\n",
    "    \"\"\"\n",
    "    Show real image and synthetic IFU-derived image side by side.\n",
    "    Works with either one or two IFU cubes needed for the filter.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Location handling\n",
    "    # ------------------------------------------------------------\n",
    "    if not isinstance(loc, SkyCoord):\n",
    "        loc_sky = SkyCoord(ra=loc[0] * u.deg, dec=loc[1] * u.deg, frame=\"icrs\")\n",
    "    else:\n",
    "        loc_sky = loc\n",
    "\n",
    "    def nearest_spaxel_map(cube_src, cube_target):\n",
    "        ny, nx = cube_target.shape[1:]\n",
    "        y_t, x_t = np.mgrid[:ny, :nx]\n",
    "\n",
    "        world = cube_target.wcs.celestial.pixel_to_world(x_t, y_t)\n",
    "        x_s, y_s = cube_src.wcs.celestial.world_to_pixel(world)\n",
    "\n",
    "        x_s = np.clip(np.round(x_s).astype(int), 0, cube_src.shape[2] - 1)\n",
    "        y_s = np.clip(np.round(y_s).astype(int), 0, cube_src.shape[1] - 1)\n",
    "\n",
    "        return y_s, x_s\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Locate real image\n",
    "    # ------------------------------------------------------------\n",
    "    real_image_file = [x for x in image_files if extract_filter_name(x) == filter][0]\n",
    "    \n",
    "    short_wl, long_wl = [x.value for x in get_filter_wl_range(filter)]\n",
    "\n",
    "    needed_ifus = []\n",
    "    for file in ifu_fileset:\n",
    "        wl = SpectralCube.read(file, hdu=\"SCI\").spectral_axis.to(u.m).value\n",
    "        if (wl[0] < short_wl) and (wl[-1] > long_wl):\n",
    "            needed_ifus = [file]\n",
    "            break\n",
    "        if (long_wl > wl[0]) and (short_wl < wl[-1]):\n",
    "            needed_ifus.append(file)\n",
    "        if len(needed_ifus) > 1:\n",
    "            break\n",
    "\n",
    "    cube1 = SpectralCube.read(needed_ifus[0], hdu=\"SCI\")\n",
    "    cube2 = SpectralCube.read(needed_ifus[1], hdu=\"SCI\") if len(needed_ifus) > 1 else None\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Base cube quantities\n",
    "    # ------------------------------------------------------------\n",
    "    wl1 = cube1.spectral_axis.to(u.m).value\n",
    "    d1 = cube1.unmasked_data[:].value\n",
    "\n",
    "    ny, nx = cube1.shape[1:]\n",
    "    n_pix = ny * nx\n",
    "    d1 = d1.reshape(len(wl1), n_pix).T\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Stitch spectra if needed\n",
    "    # ------------------------------------------------------------\n",
    "    if cube2 is not None:\n",
    "        wl2 = cube2.spectral_axis.to(u.m).value\n",
    "        d2 = cube2.unmasked_data[:].value\n",
    "\n",
    "        y2, x2 = nearest_spaxel_map(cube2, cube1)\n",
    "        d2 = d2[:, y2, x2].reshape(len(wl2), n_pix).T\n",
    "\n",
    "        wl_all = np.concatenate([wl1, wl2])\n",
    "        sort_idx = np.argsort(wl_all)\n",
    "        wl_all = wl_all[sort_idx]\n",
    "\n",
    "        spec_all = np.concatenate([d1, d2], axis=1)[:, sort_idx]\n",
    "\n",
    "        wl_min = max(wl1.min(), wl2.min())\n",
    "        wl_max = min(wl1.max(), wl2.max())\n",
    "        overlap = (wl_all >= wl_min) & (wl_all <= wl_max)\n",
    "        both = np.isin(wl_all, wl1) & np.isin(wl_all, wl2) & overlap\n",
    "        spec_all[:, both] *= 0.5\n",
    "    else:\n",
    "        wl_all = wl1\n",
    "        spec_all = d1\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Synthetic photometry\n",
    "    # ------------------------------------------------------------\n",
    "    filter_wl, filter_trans = get_filter_data(filter)\n",
    "\n",
    "    image = np.empty(n_pix)\n",
    "    for i in range(n_pix):\n",
    "        image[i] = get_Fnu_transmission(\n",
    "            spec_all[i], wl_all, filter_trans, filter_wl, warnings=True\n",
    "        )\n",
    "\n",
    "    synth_image = image.reshape(ny, nx)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Attach WCS to synthetic image\n",
    "    # ------------------------------------------------------------\n",
    "    synth_hdu = fits.PrimaryHDU(\n",
    "        synth_image,\n",
    "        header=cube1.wcs.celestial.to_header()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Plot real vs synthetic\n",
    "    # ------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # -------------------------\n",
    "    # REAL IMAGE\n",
    "    # -------------------------\n",
    "    hdu = fits.open(real_image_file)[\"SCI\"]\n",
    "    real_pix_size = hdu.header['PIXAR_A2']**0.5\n",
    "    aperture_radius = radius.to(u.arcsec).value / real_pix_size\n",
    "    cutout_real = Cutout2D(\n",
    "        hdu.data,\n",
    "        position=loc_sky,\n",
    "        size=(radius * 3, radius * 3),\n",
    "        wcs=WCS(hdu.header)\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # SYNTHETIC IMAGE (WCS CUTOUT)\n",
    "    # -------------------------\n",
    "    cutout_synth = Cutout2D(\n",
    "        synth_hdu.data,\n",
    "        position=loc_sky,\n",
    "        size=(radius * 3, radius * 3),\n",
    "        wcs=WCS(synth_hdu.header)\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Shared normalization (1–99%)\n",
    "    # ------------------------------------------------------------\n",
    "    combined = np.concatenate([\n",
    "        cutout_real.data[np.isfinite(cutout_real.data)],\n",
    "        cutout_synth.data[np.isfinite(cutout_synth.data)]\n",
    "    ])\n",
    "\n",
    "    vmin = np.percentile(combined, color_min_max[0])\n",
    "    vmax = np.percentile(combined, color_min_max[1])\n",
    "\n",
    "    cmap = plt.get_cmap(\"viridis\").copy()\n",
    "    cmap.set_under(\"black\")\n",
    "    cmap.set_over(\"white\")\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax, clip=False)\n",
    "\n",
    "    pix_scale = np.abs(cutout_synth.wcs.wcs.cdelt[0]) * 3600\n",
    "    r_ap_pix = radius.to(u.arcsec).value / pix_scale\n",
    "\n",
    "    # -------------------------\n",
    "    # PLOT REAL\n",
    "    # -------------------------\n",
    "    im0 = axes[0].imshow(\n",
    "        cutout_real.data,\n",
    "        origin=\"lower\",\n",
    "        cmap=cmap,\n",
    "        norm=norm\n",
    "    )\n",
    "    axes[0].set_title(f\"{filter} – Real\")\n",
    "\n",
    "    cbar0 = plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    cbar0.set_label(\"Flux\", fontsize=14)\n",
    "    cbar0.ax.tick_params(labelsize=12)\n",
    "\n",
    "    x_r, y_r = cutout_real.wcs.world_to_pixel(loc_sky)\n",
    "    axes[0].add_patch(\n",
    "        Circle((x_r, y_r), aperture_radius, edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # PLOT SYNTHETIC\n",
    "    # -------------------------\n",
    "    im1 = axes[1].imshow(\n",
    "        cutout_synth.data,\n",
    "        origin=\"lower\",\n",
    "        cmap=cmap,\n",
    "        norm=norm\n",
    "    )\n",
    "    axes[1].set_title(f\"{filter} – Synthetic (IFU)\")\n",
    "\n",
    "    cbar1 = plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    cbar1.set_label(\"Flux\", fontsize=14)\n",
    "    cbar1.ax.tick_params(labelsize=12)\n",
    "\n",
    "    x_s, y_s = cutout_synth.wcs.world_to_pixel(loc_sky)\n",
    "    axes[1].add_patch(\n",
    "        Circle((x_s, y_s), r_ap_pix, edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Cleanup\n",
    "    # -------------------------\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def is_filter_relevent(filter, ifu_file):\n",
    "    '''If a filter's mean wavelength is inside the ifu, returns True\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter : type = string - name of filter (\"F115W\")\n",
    "    ifu_file : type = string - string to location of ifu file\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    True if filter's mean wavelength is inside the ifu_file, False if it is not.\n",
    "    '''\n",
    "    wls = SpectralCube.read(ifu_file, hdu = 'SCI').spectral_axis.to(u.m)\n",
    "    short, long = wls[0], wls[-1]\n",
    "    return (jwst_means[filter] > short) & (jwst_means[filter] < long)\n",
    "    \n",
    "def adjust_spectrum(original_ifu, filter_name, image_files, location, radius, adjustment_operation = 'add'):\n",
    "    '''Takes an ifu file and adjusts the flux through an aperture centered at a location with specified radius.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    original_ifu : type = string (or, see retry=True)- string to location of ifu file\n",
    "    filter_name : type = string - filter name like \"F115W\"\n",
    "    location : type = either SkyCoord or list of [ra, dec] values in degrees - location of center of aperture\n",
    "    radius : type = angular size - radius of aperture, must have units attached.\n",
    "    adjustment_operation (optional, defaults to 'add'): type = string - either 'add' or 'multiply' to specify what kind of correction to use\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    Structured Numpy array with 'intensity' and 'wavelength' keys\n",
    "    '''\n",
    "    if filter_name is None:\n",
    "        return get_IFU_spectrum(original_ifu, location, radius, replace_negatives = False), 0\n",
    "    else:\n",
    "        image_file = [x for x in image_files if extract_filter_name(x)==filter_name][0]\n",
    "        raw_data = get_IFU_spectrum(original_ifu, location, radius, replace_negatives = False)\n",
    "        filter_wl, filter_trans = get_filter_data(filter_name) #TJ this is the transmission vs wavelength function for this filter\n",
    "        image_flux = get_image_flux(image_file, location, radius, replace_negatives = False) #TJ this is the flux we SHOULD get\n",
    "        initial_synth_flux = get_Fnu_transmission(raw_data['intensity'], raw_data['wavelength'], filter_trans, filter_wl, warnings = True) #TJ this is the current synthetic flux we get\n",
    "        if adjustment_operation == 'add':\n",
    "            correction = image_flux - initial_synth_flux\n",
    "            raw_data['intensity'] = raw_data['intensity'] + correction\n",
    "            return raw_data, correction #TJ now corrected to match photometry\n",
    "        elif adjustment_operation == 'multiply':\n",
    "            correction = image_flux/initial_synth_flux\n",
    "            raw_data['intensity'] = raw_data['intensity']*correction\n",
    "            return raw_data, correction #TJ now corrected\n",
    "        else:\n",
    "            print('adjustment operation not recognized, only \"add\" or \"multiply\" are currently implemented')\n",
    "            return None\n",
    "        print('Something went wrong.')\n",
    "        return raw_data, correction #TJ Now corrected data\n",
    "\n",
    "\n",
    "def get_largest_filter_within(ifu_file):\n",
    "    '''Takes an ifu file and selects the filter with the largest bandpass that is entirely within it.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    ifu_file : type = string - string to location of ifu file\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    Filter name (ex. \"F115W\") corresponding to the largest filter entirely contained within the IFU file \n",
    "    '''\n",
    "    filters = [extract_filter_name(x) for x in filter_files if full_coverage(extract_filter_name(x),ifu_file)==\"good\"]\n",
    "    if len(filters)<1:\n",
    "        print(f'No filters entirely within {ifu_file}')\n",
    "        return None\n",
    "    else:\n",
    "        best_filter = filters[np.argmax([(get_filter_wl_range(fil)[1].value - get_filter_wl_range(fil)[0].value) for fil in filters])]\n",
    "        return best_filter\n",
    "\n",
    "anchor_filters = ['F150W', 'F200W', 'F444W', 'F1000W', 'F1500W']\n",
    "\n",
    "def needed_datasets(filter_name, datasets):\n",
    "    '''returns which ifu_files should be considered when calculating the synthetic flux. If an ifu even slightly overlaps into\n",
    "    the filter's range it is included.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter : type = string - name of filter (\"F115W\")\n",
    "    datasets : type = structured array - array with keys for 'wavelength' and 'intensity'\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    Filter name (ex. \"F115W\") corresponding to the largest filter entirely contained within the IFU file \n",
    "    '''\n",
    "    needed = []\n",
    "    filter_wl, _ = get_filter_data(filter_name)\n",
    "    for data in datasets:\n",
    "        if (filter_wl[0] < data['wavelength'][-1]) & (filter_wl[-1] > data['wavelength'][0]):\n",
    "            needed.append(data)\n",
    "    return needed\n",
    "\n",
    "def merge_datasets(ds1, ds2):\n",
    "    \"\"\"\n",
    "    Merge two structured arrays with 'wavelength' and 'intensity' keys.\n",
    "    Handles overlapping regions by averaging intensities, and automatically\n",
    "    determines which dataset has higher wavelength resolution.\n",
    "    \"\"\"\n",
    "    # Sort by wavelength, just to be safe\n",
    "    ds1 = np.sort(ds1, order='wavelength')\n",
    "    ds2 = np.sort(ds2, order='wavelength')\n",
    "\n",
    "    # Determine wavelength resolutions\n",
    "    d1_res = np.mean(np.diff(ds1['wavelength']))\n",
    "    d2_res = np.mean(np.diff(ds2['wavelength']))\n",
    "\n",
    "    # Assign high- and low-resolution datasets\n",
    "    if d1_res < d2_res:\n",
    "        highres, lowres = ds1, ds2\n",
    "    else:\n",
    "        highres, lowres = ds2, ds1\n",
    "\n",
    "    # Determine overlap region\n",
    "    overlap_start = max(highres['wavelength'][0], lowres['wavelength'][0])\n",
    "    overlap_end   = min(highres['wavelength'][-1], lowres['wavelength'][-1])\n",
    "\n",
    "    # Interpolate the lowres data onto highres wavelengths (only inside overlap)\n",
    "    overlap_mask = (highres['wavelength'] >= overlap_start) & (highres['wavelength'] <= overlap_end)\n",
    "    interp_flux = np.interp(highres['wavelength'][overlap_mask],\n",
    "                            lowres['wavelength'], lowres['intensity'])\n",
    "\n",
    "    # Combine in overlap by averaging\n",
    "    merged_overlap_wl = highres['wavelength'][overlap_mask]\n",
    "    merged_overlap_intensity = 0.5 * (highres['intensity'][overlap_mask] + interp_flux)\n",
    "\n",
    "    # Keep the unique non-overlapping parts from both sides\n",
    "    full_low_side  = ds1[ds1['wavelength'] < overlap_start]\n",
    "    full_high_side = ds2[ds2['wavelength'] > overlap_end]\n",
    "\n",
    "    # Concatenate all pieces and sort\n",
    "    merged = np.concatenate([\n",
    "        full_low_side,\n",
    "        np.rec.fromarrays([merged_overlap_wl, merged_overlap_intensity],\n",
    "                          names=('wavelength', 'intensity')),\n",
    "        full_high_side\n",
    "    ])\n",
    "    merged = np.sort(merged, order='wavelength')\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def get_all_fluxes(filter_files, spec_datasets, image_files, location, radius):\n",
    "    '''Creates synthetic fluxes for all filters in the files that have wavelengths that span the entire filter.\n",
    "    For filters that straddle multiple wavelengths, any wavelength inside a filter that has intensity values\n",
    "    from multiple datasets uses the average intensity from each dataset.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter_files : type = list of strings - name of filter [\"F115W\", \"F2100W\"]\n",
    "    datasets : type = list of structured arrays - arrays with keys for 'wavelength' and 'intensity'\n",
    "    image_files : type = list of strings - strings to image files\n",
    "    location : type = either SkyCoord or list of [ra, dec] values in degrees - location of center of aperture\n",
    "    radius : type = angular size - radius of aperture, must have units attached.\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    A dictionary with keys for 'filter_name', 'mean_wl', 'synth_flux', and 'photo_flux'\n",
    "    '''\n",
    "    results = {}\n",
    "\n",
    "    results['filter_name'] = []\n",
    "    results['mean_wl'] = []\n",
    "    results['synth_flux'] = []\n",
    "    results['photo_flux'] = []\n",
    "    results['wavelength'] = []\n",
    "    results['intensity'] = []\n",
    "    \n",
    "    for i, data in enumerate(spec_datasets[1:]):\n",
    "        if i == 0:\n",
    "            prior_data = spec_datasets[0]\n",
    "        prior_data = merge_datasets(prior_data, data)\n",
    "        \n",
    "    results['wavelength'].append(prior_data['wavelength'])\n",
    "    results['intensity'].append(prior_data['intensity'])\n",
    "    \n",
    "    \n",
    "    for filter_file in filter_files:\n",
    "        filter_name = extract_filter_name(filter_file)\n",
    "        image_file = [x for x in image_files if extract_filter_name(x)==filter_name][0]\n",
    "        photo_flux = get_image_flux(image_file, location, radius, replace_negatives = False)\n",
    "        results['photo_flux'].append(photo_flux)\n",
    "        filter_wl, filter_trans = get_filter_data(filter_name)\n",
    "        results['filter_name'].append(filter_name)\n",
    "        results['mean_wl'].append(jwst_means[filter_name].value)\n",
    "        needed_data = needed_datasets(filter_name, spec_datasets)\n",
    "        if len(needed_data) == 0:\n",
    "            print('no spectral data was found for ', filter_name)\n",
    "        if len(needed_data)<2:\n",
    "            synth_flux = get_Fnu_transmission(needed_data[0]['intensity'], needed_data[0]['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "            results['synth_flux'].append(synth_flux)\n",
    "            \n",
    "        else:\n",
    "            full_data = merge_datasets(needed_data[0], needed_data[1])\n",
    "            synth_flux = get_Fnu_transmission(full_data['intensity'], full_data['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "            results['synth_flux'].append(synth_flux)\n",
    "    results['wavelength'] = np.array(results['wavelength'][0])\n",
    "    results['intensity'] = np.array(results['intensity'][0])\n",
    "    results['filter_name'] = np.array(results['filter_name'])\n",
    "    results['mean_wl'] = np.array(results['mean_wl'])\n",
    "    results['synth_flux'] = np.array(results['synth_flux'])\n",
    "    results['photo_flux'] = np.array(results['photo_flux'])\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_overlap_region(ds1, ds2):\n",
    "    \"\"\"\n",
    "    Return only the overlapping wavelength region between two structured arrays\n",
    "    with 'wavelength' and 'intensity'. The returned region contains:\n",
    "        - wavelength grid from the higher-resolution dataset (within overlap)\n",
    "        - intensity = average(intensity_highres, interpolated_intensity_lowres)\n",
    "    \"\"\"\n",
    "\n",
    "    # Sort to ensure order\n",
    "    ds1 = np.sort(ds1, order='wavelength')\n",
    "    ds2 = np.sort(ds2, order='wavelength')\n",
    "\n",
    "    # Compute wavelength resolutions\n",
    "    d1_res = np.mean(np.diff(ds1['wavelength']))\n",
    "    d2_res = np.mean(np.diff(ds2['wavelength']))\n",
    "\n",
    "    # Identify high- and low-resolution datasets\n",
    "    if d1_res < d2_res:\n",
    "        highres, lowres = ds1, ds2\n",
    "    else:\n",
    "        highres, lowres = ds2, ds1\n",
    "\n",
    "    # Determine numerical overlap bounds\n",
    "    overlap_start = max(highres['wavelength'][0], lowres['wavelength'][0])\n",
    "    overlap_end   = min(highres['wavelength'][-1], lowres['wavelength'][-1])\n",
    "\n",
    "    # If no overlap, return empty structured array\n",
    "    if overlap_start >= overlap_end:\n",
    "        return np.recarray(0, dtype=[('wavelength', float), ('intensity', float)])\n",
    "\n",
    "    # Mask for high-res wavelengths inside the overlap\n",
    "    mask = (highres['wavelength'] >= overlap_start) & (highres['wavelength'] <= overlap_end)\n",
    "\n",
    "    high_wl = highres['wavelength'][mask]\n",
    "    high_flux = highres['intensity'][mask]\n",
    "\n",
    "    # Interpolate lowres intensities onto the highres wavelength grid\n",
    "    interp_flux = np.interp(high_wl,\n",
    "                            lowres['wavelength'],\n",
    "                            lowres['intensity'])\n",
    "\n",
    "    # Average intensities\n",
    "    avg_flux = 0.5 * (high_flux + interp_flux)\n",
    "\n",
    "    # Return structured array\n",
    "    overlap = np.rec.fromarrays(\n",
    "        [high_wl, avg_flux],\n",
    "        names=('wavelength', 'intensity')\n",
    "    )\n",
    "\n",
    "    return overlap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_data(ifu_files, image_files, filter_files, loc, radius, anchor_filters = anchor_filters):\n",
    "    '''\n",
    "\n",
    "\n",
    "    '''\n",
    "    temp_filepath = 'Data_files/misc_data/temp_data'\n",
    "    if os.path.exists(temp_filepath):\n",
    "        shutil.rmtree(temp_filepath)\n",
    "    os.makedirs(temp_filepath)\n",
    "    results = {}\n",
    "    if loc == [202.5062429, 47.2143358]:\n",
    "        loc_index = 0\n",
    "    elif loc == [202.4335225, 47.1729608]:\n",
    "        loc_index = 1\n",
    "    elif loc == [202.4340450, 47.1732517]:\n",
    "        loc_index = 2\n",
    "    elif loc == [202.4823742, 47.1958589]:\n",
    "        loc_index = 3\n",
    "    else:\n",
    "        loc_index = \"?\"\n",
    "    \n",
    "    results['add_datasets'] = []\n",
    "    results['mult_datasets'] = []\n",
    "\n",
    "    results['ifu_files'] = ifu_files\n",
    "    results['image_files'] = image_files\n",
    "    \n",
    "    results['location'] = loc\n",
    "    results['loc_idx'] = loc_index\n",
    "    results['radius'] = radius\n",
    "    \n",
    "    print('adjusting spectra using additive and multiplicative corrections')\n",
    "    results['add_correction_values'] = []\n",
    "    results['mult_correction_values'] = []\n",
    "    \n",
    "    for i, ifu_file in enumerate(ifu_files):\n",
    "        mult_data, mult_correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), image_files, loc, radius, adjustment_operation = 'multiply')\n",
    "        results['mult_correction_values'].append(mult_correction)\n",
    "        add_data, add_correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), image_files, loc, radius, adjustment_operation = 'add')\n",
    "        results['add_correction_values'].append(add_correction)\n",
    "        fname = os.path.join(temp_filepath, f\"add_grism_{i+1}_of_{len(ifu_files)}.npy\")\n",
    "        np.save(fname, add_data)\n",
    "        fname = os.path.join(temp_filepath, f\"mult_grism_{i+1}_of_{len(ifu_files)}.npy\")\n",
    "        np.save(fname, mult_data)\n",
    "\n",
    "        print(f'adjusted {i+1} of {len(ifu_files)}')\n",
    "\n",
    "    add_datasets = []\n",
    "    mult_datasets = []\n",
    "    add_files = glob.glob(f'Data_files/misc_data/temp_data/add_grism*')\n",
    "    mult_files = glob.glob(f'Data_files/misc_data/temp_data/mult_grism*')\n",
    "    for file in add_files:\n",
    "        data = np.load(file)\n",
    "        results['add_datasets'].append(data)\n",
    "        add_datasets.append(data)\n",
    "    for file in mult_files:\n",
    "        data = np.load(file)\n",
    "        results['mult_datasets'].append(data)\n",
    "        mult_datasets.append(data)\n",
    "\n",
    "    \n",
    "    print('calculating additive corrected synthetic photometry...')\n",
    "    add_results = get_all_fluxes(filter_files, add_datasets, image_files, loc, radius)\n",
    "    print('calculating multiplicative corrected synthetic photometry...')\n",
    "    mult_results = get_all_fluxes(filter_files, mult_datasets, image_files, loc, radius)\n",
    "    \n",
    "    \n",
    "    print('Compiling results and cleaning up...')\n",
    "    \n",
    "    results['filter_names'] = add_results['filter_name']\n",
    "    results['filter_wavelengths'] = add_results['mean_wl']\n",
    "    results['add_synthetic_fluxes'] = add_results['synth_flux']\n",
    "    results['mult_synthetic_fluxes'] = mult_results['synth_flux']\n",
    "    if np.mean(add_results['photo_flux']) != np.mean(mult_results['photo_flux']):\n",
    "        print('!!!!!!!!Photo fluxes were not the same in the two datasets! Something has gone wrong')\n",
    "    results['photo_fluxes'] = add_results['photo_flux']\n",
    "    if np.mean(add_results['wavelength']) != np.mean(mult_results['wavelength']):\n",
    "        print('!!!!!!!!!Wavelength arrays were not the same in the two datasets! Something has gone wrong')\n",
    "    results['wavelength'] = add_results['wavelength']\n",
    "    results['add_intensity'] = add_results['intensity']\n",
    "    results['mult_intensity'] = mult_results['intensity']\n",
    "    \n",
    "    shutil.rmtree(temp_filepath)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_results(results, correction = 'mult', show_images = [], color_min_max = [1, 99.5]):\n",
    "    '''\n",
    "\n",
    "\n",
    "    '''\n",
    "    #TJ ensure that location is a skycoord\n",
    "    if not isinstance(results['location'], SkyCoord):\n",
    "        loc_sky = SkyCoord(ra=results['location'][0] * u.deg, dec=results['location'][1] * u.deg, frame=\"icrs\")\n",
    "    else:\n",
    "        loc_sky = results['location']\n",
    "\n",
    "    #TJ define helper function to match up pixels between multiple IFU files\n",
    "    def nearest_spaxel_map(cube_src, cube_target):\n",
    "        ny, nx = cube_target.shape[1:]\n",
    "        y_t, x_t = np.mgrid[:ny, :nx]\n",
    "\n",
    "        world = cube_target.wcs.celestial.pixel_to_world(x_t, y_t)\n",
    "        x_s, y_s = cube_src.wcs.celestial.world_to_pixel(world)\n",
    "\n",
    "        x_s = np.clip(np.round(x_s).astype(int), 0, cube_src.shape[2] - 1)\n",
    "        y_s = np.clip(np.round(y_s).astype(int), 0, cube_src.shape[1] - 1)\n",
    "\n",
    "        return y_s, x_s\n",
    "    \n",
    "    #TJ write title phrase for plot display\n",
    "    if correction == 'mult':\n",
    "        method = 'multiplicative correction'\n",
    "    elif correction == 'add':\n",
    "        method = 'additive correction'\n",
    "    else:\n",
    "        method = 'unrecognized method'\n",
    "        \n",
    "    #TJ create figure axes, fontsizes, marker sizes, colors, initial axis limits, etc\n",
    "    fig = plt.figure(figsize = (45,30))\n",
    "    ax_spec = fig.add_axes((0.05, 0.4, 1, 0.6))\n",
    "    ax_scat = fig.add_axes((0.05, 0.05, 1, 0.35))\n",
    "    fontsize_sm = 35\n",
    "    fontsize_lg = 45\n",
    "    marker_size = 250\n",
    "    cube_colors = ['purple', 'blue', 'cyan', 'green', 'orange', 'red', 'pink']\n",
    "    spec_y_min = 1 #TJ Flux should always be around 10^-20 so setting limits of 0-1 should never be too strict\n",
    "    spec_y_max = 0\n",
    "    short_bounds = []\n",
    "    long_bounds = []\n",
    "\n",
    "    #TJ load the datasets one by one, plotting in different colors to show separate cubes. Plot averaged overlap regions in black\n",
    "    for i, dataset in  enumerate(results[correction+'_datasets']):\n",
    "            \n",
    "            short_bounds.append(dataset['wavelength'][0])\n",
    "            long_bounds.append(dataset['wavelength'][-1])\n",
    "            ax_spec.plot(dataset['wavelength'], dataset['intensity'], alpha = 0.5, color = cube_colors[i], linewidth = 5)\n",
    "            spec_y_min = min(spec_y_min, np.percentile(dataset['intensity'], 1)*0.5)\n",
    "            spec_y_max = max(spec_y_max, np.percentile(dataset['intensity'], 98)*1.5)\n",
    "            if i > 0:\n",
    "                overlap_data = get_overlap_region(results[correction+'_datasets'][i-1], dataset)\n",
    "                ax_spec.plot(overlap_data['wavelength'], overlap_data['intensity'], alpha = 0.5, color = 'black')\n",
    "    \n",
    "    \n",
    "    #TJ plot the entire spectrum on the scatter plot below in white to force scale to be the same as above.\n",
    "    ax_scat.plot(results['wavelength'], [1]*len(results[correction+'_intensity']), color = 'white', alpha = 0)\n",
    "    ax_spec.scatter(results['filter_wavelengths'], results[correction+'_synthetic_fluxes'], marker = '*', s=marker_size, color = 'black')\n",
    "    ax_spec.scatter([], [], marker = '*', s=marker_size, color = 'black', label = 'Synth')\n",
    "    ax_spec.scatter(results['filter_wavelengths'], results['photo_fluxes'], marker = \"o\", s=marker_size, color = 'black')\n",
    "    ax_spec.scatter([], [], marker = \"o\", s=marker_size, color = 'black', label = 'Photo')\n",
    "\n",
    "    #TJ plot the horizontal filter coverage in black\n",
    "    for i, filter in enumerate(results['filter_names']):\n",
    "        filter_short_wl, filter_long_wl = [x.value for x in get_filter_wl_range(filter)]\n",
    "        ax_spec.hlines(y=results['photo_fluxes'][i], xmin=filter_short_wl, xmax=filter_long_wl, color='black', alpha=0.7, linewidth=3)\n",
    "\n",
    "    #TJ plot the data on the bottom graph\n",
    "    ax_scat.scatter(results['filter_wavelengths'], results[correction+'_synthetic_fluxes']/results['photo_fluxes'], s=marker_size, color = 'black')\n",
    "\n",
    "    #TJ generate tick labels and sizes\n",
    "    ax_scat.tick_params(axis='x', which='minor', width=2, length=10, right=True, top=True, direction='in',\n",
    "                       labelsize=fontsize_sm)\n",
    "    ax_scat.tick_params(axis='x', which='major', width=3, length=15, right=True, top=True, direction='in',\n",
    "                       labelsize=fontsize_sm)\n",
    "    ax_scat.tick_params(axis='y', which='both', width=3, length=15, right=True, top=True, direction='in',\n",
    "                       labelsize=fontsize_sm)\n",
    "    ax_scat.set_xlabel('wavelength (m)', fontsize = 40)\n",
    "    ax_scat.set_ylabel('synthetic/photometric flux', fontsize = 40)\n",
    "    ax_spec.tick_params(axis='x', which='minor', width=2, length=10, right=True, top=True, direction='in',\n",
    "                       labelsize=fontsize_sm)\n",
    "    ax_spec.tick_params(axis='x', which='major', width=3, length=15, right=True, top=True, direction='in',\n",
    "                       labelsize=fontsize_sm)\n",
    "    ax_spec.tick_params(axis='y', which='both', width=3, length=15, right=True, top=True, direction='in',\n",
    "                       labelsize=fontsize_sm)\n",
    "    ax_spec.set_ylabel('Intensity (MJy/sr)', fontsize = 40)\n",
    "    \n",
    "    ax_spec.set_title(f\"{results['radius']}-radius aperture at location {results['loc_idx']}\\nUsing {method}\", fontsize = 50)\n",
    "\n",
    "    #TJ set scale to logorithmic on both horizontal axes and the vertical axis\n",
    "    ax_scat.set_xscale('log')\n",
    "    ax_spec.set_xscale('log')\n",
    "    ax_spec.set_yscale('log')\n",
    "    \n",
    "    #TJ initialize filter name label locations\n",
    "    label_positions = [] \n",
    "    \n",
    "    for x, y, name in zip(results['filter_wavelengths'], results[correction+'_synthetic_fluxes']/results['photo_fluxes'], results['filter_names']):\n",
    "        #TJ plot the horizontal filter coverage on the lower plot.\n",
    "        filter_short_wl, filter_long_wl = [x.value for x in get_filter_wl_range(name)]\n",
    "        ax_scat.hlines(y=y, xmin=filter_short_wl, xmax=filter_long_wl, color='black', alpha=0.7, linewidth=3)\n",
    "\n",
    "        #TJ default offset is 0.05 lower than the scatter point\n",
    "        y_offset = -0.05 \n",
    "\n",
    "        #TJ label the filters used to calibrate the cubes in red to distinguish them\n",
    "        if name in anchor_filters:\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'black'\n",
    "        #TJ transform the entire figure axes to a coordinate system to calculate distances between labels\n",
    "        x_disp, y_disp = ax_spec.transData.transform((x, y))\n",
    "        \n",
    "        #TJ check if labels are too close to another label\n",
    "        too_close = False\n",
    "        for (xx, yy) in label_positions:\n",
    "            if abs(x_disp - xx) < 20 and abs(y_disp + y_offset - yy) < 5:  \n",
    "                #TJ use 20px horizontal and 5px vertical proximity as threshhold for \"too close\"\n",
    "                too_close = True\n",
    "                break\n",
    "        \n",
    "        #TJ if overlapping, nudge upward instead of downward\n",
    "        if too_close:\n",
    "            y_offset = +0.2 \n",
    "        #TJ if the location of the scatter point is less than 0.8, always default to labeling it up instead of down\n",
    "        if y < 0.8:\n",
    "            y_offset = +0.4\n",
    "        #TJ save adjusted label position\n",
    "        label_positions.append((x_disp, y_disp + y_offset))\n",
    "        \n",
    "        #TJ this filter is extremely close to another filter and should always be above instead of below\n",
    "        if name == \"F182M\":\n",
    "            y_offset = +0.25\n",
    "        #TJ same with this one\n",
    "        if name == 'F212N':\n",
    "            y_offset = +0.25\n",
    "            \n",
    "        #TJ now plot text in chosen coordinates\n",
    "        ax_scat.text(\n",
    "            x, y + y_offset,\n",
    "            name,\n",
    "            ha=\"center\", va=\"top\",\n",
    "            fontsize=fontsize_sm, rotation = 90, color = color,\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7, pad=0.5)\n",
    "        )\n",
    "\n",
    "    #TJ find the image files that were asked to be displayed within the figure\n",
    "    show_image_files = [x for x in results['image_files'] if extract_filter_name(x) in show_images]\n",
    "    #TJ set the locations of the images to display\n",
    "    image_locations = [(0.05, 0.75, 0.2, 0.2), (0.3, 0.75, 0.2, 0.2), (0.6, 0.41, 0.2, 0.2), (0.8, 0.41, 0.2, 0.2), (0.85, 0.65, 0.18, 0.18)]\n",
    "\n",
    "    #TJ if show images is a list of filters, then put them into the figure\n",
    "    for i, (img, title) in enumerate(zip(show_image_files, show_images)):\n",
    "        #TJ add new axis in the next available location (real image takes one, synth image takes other)\n",
    "        ax_real = fig.add_axes(image_locations[2*i])\n",
    "        ax_synth = fig.add_axes(image_locations[(2*i)+1])\n",
    "        \n",
    "        #TJ get real image file for this filter\n",
    "        real_image_file = [x for x in image_files if extract_filter_name(x) == title][0]\n",
    "        #TJ get wavelength coverage, for calculating which ifu cubes are needed\n",
    "        short_wl, long_wl = [x.value for x in get_filter_wl_range(title)]\n",
    "\n",
    "        #TJ initialize ifu cubes that are needed for this wavelength coverage\n",
    "        needed_ifus = []\n",
    "        for file in results['ifu_files']:\n",
    "            wl = SpectralCube.read(file, hdu=\"SCI\").spectral_axis.to(u.m).value\n",
    "            #TJ if filter is even partially within a cube, add it to the list of needed cubes.\n",
    "            if (long_wl > wl[0]) and (short_wl < wl[-1]):\n",
    "                needed_ifus.append(file)\n",
    "            #TJ no filter should ever need more than 2 cubes, so stop checking.\n",
    "            if len(needed_ifus) > 1:\n",
    "                break\n",
    "        #TJ read in the cubes that are needed.\n",
    "        cube1 = SpectralCube.read(needed_ifus[0], hdu=\"SCI\")\n",
    "        cube2 = SpectralCube.read(needed_ifus[1], hdu=\"SCI\") if len(needed_ifus) > 1 else None\n",
    "        #TJ extract wavelength array and remove masked data\n",
    "        wl1 = cube1.spectral_axis.to(u.m).value\n",
    "        d1 = cube1.unmasked_data[:].value\n",
    "        \n",
    "        #TJ extract the shape of the cubes (minus the wavelength axis)\n",
    "        ny, nx = cube1.shape[1:]\n",
    "\n",
    "        #TJ get number of pixels in first cube\n",
    "        n_pix = ny * nx\n",
    "        d1 = d1.reshape(len(wl1), n_pix).T\n",
    "        \n",
    "        #TJ if we need two cubes, we need to stitch them together and get averages for the overlapping region\n",
    "        if cube2 is not None:\n",
    "\n",
    "            wl2 = cube2.spectral_axis.to(u.m).value\n",
    "            d2 = cube2.unmasked_data[:].value\n",
    "        \n",
    "            y2, x2 = nearest_spaxel_map(cube2, cube1)\n",
    "            d2 = d2[:, y2, x2].reshape(len(wl2), n_pix).T\n",
    "        \n",
    "            # --- decide reference (higher spectral resolution) ---\n",
    "            dlam1 = np.nanmedian(np.diff(wl1))\n",
    "            dlam2 = np.nanmedian(np.diff(wl2))\n",
    "        \n",
    "            if dlam1 <= dlam2:\n",
    "                wl_ref, spec_ref = wl1, d1\n",
    "                wl_other, spec_other = wl2, d2\n",
    "            else:\n",
    "                wl_ref, spec_ref = wl2, d2\n",
    "                wl_other, spec_other = wl1, d1\n",
    "        \n",
    "            # --- interpolate OTHER cube onto wl_ref ---\n",
    "            n_ref = len(wl_ref)\n",
    "            spec_other_interp = np.full((n_pix, n_ref), np.nan)\n",
    "        \n",
    "            for i in range(n_pix):\n",
    "                f = interp1d(\n",
    "                    wl_other,\n",
    "                    spec_other[i],\n",
    "                    bounds_error=False,\n",
    "                    fill_value=np.nan\n",
    "                )\n",
    "                spec_other_interp[i] = f(wl_ref)\n",
    "        \n",
    "            # --- determine overlap ---\n",
    "            wl_min = max(wl_ref.min(), wl_other.min())\n",
    "            wl_max = min(wl_ref.max(), wl_other.max())\n",
    "        \n",
    "            ref_overlap = (wl_ref >= wl_min) & (wl_ref <= wl_max)\n",
    "        \n",
    "            # --- average in overlap ---\n",
    "            spec_ref_avg = spec_ref.copy()\n",
    "            both = ref_overlap & np.isfinite(spec_other_interp)\n",
    "        \n",
    "            spec_ref_avg[both] = 0.5 * (\n",
    "                spec_ref[both] + spec_other_interp[both]\n",
    "            )\n",
    "        \n",
    "            # --- non-overlapping wavelengths from other cube ---\n",
    "            left_mask  = wl_other < wl_min\n",
    "            right_mask = wl_other > wl_max\n",
    "        \n",
    "            wl_left  = wl_other[left_mask]\n",
    "            wl_right = wl_other[right_mask]\n",
    "        \n",
    "            spec_left  = spec_other[:, left_mask]\n",
    "            spec_right = spec_other[:, right_mask]\n",
    "        \n",
    "            # --- build full wavelength grid ---\n",
    "            wl_all = np.concatenate([wl_left, wl_ref, wl_right])\n",
    "        \n",
    "            # --- build full spectrum ---\n",
    "            spec_all = np.full((n_pix, len(wl_all)), np.nan)\n",
    "        \n",
    "            i0 = len(wl_left)\n",
    "            i1 = i0 + len(wl_ref)\n",
    "        \n",
    "            spec_all[:, i0:i1] = spec_ref_avg\n",
    "        \n",
    "            if wl_left.size > 0:\n",
    "                spec_all[:, :i0] = spec_left\n",
    "        \n",
    "            if wl_right.size > 0:\n",
    "                spec_all[:, i1:] = spec_right\n",
    "        \n",
    "        else:\n",
    "            wl_all = wl1\n",
    "            spec_all = d1\n",
    "\n",
    "        #TJ extract filter information\n",
    "        filter_wl, filter_trans = get_filter_data(title)\n",
    "\n",
    "        #TJ initialize image\n",
    "        image = np.empty(n_pix)\n",
    "        #TJ loop through the pixels applying the filter to each one.\n",
    "        for i in range(n_pix):\n",
    "            image[i] = get_Fnu_transmission(spec_all[i], wl_all, filter_trans, filter_wl, warnings=True)\n",
    "\n",
    "        #TJ reshape the image into a 2d array\n",
    "        synth_image = image.reshape(ny, nx)\n",
    "    \n",
    "        #TJ assign a WCS coordinate system to the synthetic image\n",
    "        synth_hdu = fits.PrimaryHDU(synth_image, header=cube1.wcs.celestial.to_header())\n",
    "    \n",
    "        # -------------------------\n",
    "        #TJ FOR THE REAL IMAGE-----\n",
    "        # -------------------------\n",
    "        #TJ load real image hdu\n",
    "        hdu = fits.open(real_image_file)[\"SCI\"]\n",
    "        #TJ calculate pixel size. If they are not square, print warning.\n",
    "        if hdu.header['CDELT1'] != hdu.header['CDELT2']:\n",
    "            print('!!!!!!!Pixels are not square!!!!!!')\n",
    "            print(f\"pixels in one direction are {hdu.header['CDELT1']*3600} arcseconds\")\n",
    "            print(f\"pixels in other direction are {hdu.header['CDELT2']*3600} arcseconds\")\n",
    "            \n",
    "        real_pix_size = hdu.header['PIXAR_A2']**0.5\n",
    "        #TJ calculate how many pixels the aperture radius should be\n",
    "        aperture_radius = results['radius'].to(u.arcsec).value / real_pix_size\n",
    "        cutout_real = Cutout2D(\n",
    "            hdu.data,\n",
    "            position=loc_sky,\n",
    "            size=(results['radius'] * 3, results['radius'] * 3),\n",
    "            wcs=WCS(hdu.header)\n",
    "        )\n",
    "    \n",
    "        # ----------------------------\n",
    "        #TJ FOR THE SYNTHETIC IMAGE---\n",
    "        # ----------------------------\n",
    "        cutout_synth = Cutout2D(\n",
    "            synth_hdu.data,\n",
    "            position=loc_sky,\n",
    "            size=(results['radius'] * 3, results['radius'] * 3),\n",
    "            wcs=WCS(synth_hdu.header)\n",
    "        )\n",
    "        \n",
    "        #TJ get pixel scale for the synthetic image and calculate aperture radius in pixels\n",
    "        pix_scale = np.abs(cutout_synth.wcs.wcs.cdelt[0]) * 3600\n",
    "        r_ap_pix = results['radius'].to(u.arcsec).value / pix_scale\n",
    "        \n",
    "        #TJ generate min and max flux values for color bar (to avoid a single bright pixel making everything else basically zero color)\n",
    "        #TJ first, we need to isolate the aperture, we dont care if the bright pixel is outside the aperture\n",
    "        x0_r, y0_r = cutout_real.wcs.world_to_pixel(loc_sky)\n",
    "        x0_s, y0_s = cutout_synth.wcs.world_to_pixel(loc_sky)\n",
    "        \n",
    "        # Pixel grids\n",
    "        yy_r, xx_r = np.indices(cutout_real.data.shape)\n",
    "        yy_s, xx_s = np.indices(cutout_synth.data.shape)\n",
    "        \n",
    "        # Aperture masks\n",
    "        ap_mask_real = ((xx_r - x0_r)**2 + (yy_r - y0_r)**2) <= aperture_radius**2\n",
    "        ap_mask_synth = ((xx_s - x0_s)**2 + (yy_s - y0_s)**2) <= r_ap_pix**2\n",
    "        \n",
    "        # Collect aperture pixels only\n",
    "        ap_pixels = np.concatenate([\n",
    "            cutout_real.data[ap_mask_real & np.isfinite(cutout_real.data)],\n",
    "            cutout_synth.data[ap_mask_synth & np.isfinite(cutout_synth.data)]\n",
    "        ])\n",
    "        \n",
    "        # Robust statistics\n",
    "        median = np.median(ap_pixels)\n",
    "        sigma = mad_std(ap_pixels)  # robust σ (MAD-based)\n",
    "        \n",
    "        # Sigma limits (tweak if needed)\n",
    "        nsig_low = 3.0\n",
    "        nsig_high = 20.0\n",
    "        \n",
    "        vmin = median - nsig_low * sigma\n",
    "        vmax = median + nsig_high * sigma\n",
    "        \n",
    "        # Safety fallback\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "            vmin = np.nanmin(ap_pixels)\n",
    "            vmax = np.nanmax(ap_pixels)\n",
    "        \n",
    "        # Colormap behavior\n",
    "        cmap = plt.get_cmap(\"viridis\").copy()\n",
    "        cmap.set_under(\"black\")\n",
    "        cmap.set_over(\"white\")\n",
    "        \n",
    "        norm = colors.Normalize(vmin=vmin, vmax=vmax, clip=False)\n",
    "    \n",
    "        # -------------------------\n",
    "        #TJ PLOT THE REAL IMAGE\n",
    "        # -------------------------\n",
    "        im0 = ax_real.imshow(\n",
    "            cutout_real.data,\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap,\n",
    "            norm=norm\n",
    "        )\n",
    "        #TJ add title\n",
    "        ax_real.set_title(f\"{title} – Real\", fontsize = fontsize_lg)\n",
    "        #TJ add color bar\n",
    "        cbar0 = plt.colorbar(im0, ax=ax_real, fraction=0.046, pad=0.04)\n",
    "        #cbar0.set_label(\"Flux\", fontsize=fontsize_lg)\n",
    "        cbar0.ax.tick_params(labelsize=fontsize_sm)\n",
    "    \n",
    "        x_r, y_r = cutout_real.wcs.world_to_pixel(loc_sky)\n",
    "        ax_real.add_patch(\n",
    "            Circle((x_r, y_r), aperture_radius, edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
    "        )\n",
    "    \n",
    "        # -----------------------------\n",
    "        #TJ PLOT THE SYNTHETIC IMAGE---\n",
    "        # -----------------------------\n",
    "        im1 = ax_synth.imshow(\n",
    "            cutout_synth.data,\n",
    "            origin=\"lower\",\n",
    "            cmap=cmap,\n",
    "            norm=norm\n",
    "        )\n",
    "        #TJ add title\n",
    "        ax_synth.set_title(f\"{title} – Synthetic\", fontsize = fontsize_lg)\n",
    "        #TJ add color bar\n",
    "        cbar1 = plt.colorbar(im1, ax=ax_synth, fraction=0.046, pad=0.04)\n",
    "        #cbar1.set_label(\"Flux\", fontsize=fontsize_lg)\n",
    "        cbar1.ax.tick_params(labelsize=fontsize_sm)\n",
    "    \n",
    "        x_s, y_s = cutout_synth.wcs.world_to_pixel(loc_sky)\n",
    "        ax_synth.add_patch(\n",
    "            Circle((x_s, y_s), r_ap_pix, edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
    "        )\n",
    "    \n",
    "        #TJ add tick marks\n",
    "        ax_real.set_xticks([])\n",
    "        ax_real.set_yticks([])\n",
    "        ax_synth.set_xticks([])\n",
    "        ax_synth.set_yticks([])\n",
    "\n",
    "    ymin, ymax = ax_scat.get_ylim()\n",
    "    text_y_pos = ymin * 1.1\n",
    "    ax_scat.axhline(y = 1, color = 'gray', linestyle = '--', linewidth = 4, alpha = 0.5)\n",
    "    ax_scat.axvline(x=7.650000025896587e-06, color='gray', linestyle='--', linewidth=4, alpha=0.7)\n",
    "    ax_scat.text(7.25e-6, text_y_pos, \"← NIRCam\", \n",
    "                 ha='right', va='center', \n",
    "                 bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=2),\n",
    "                 fontsize=fontsize_lg)\n",
    "    \n",
    "    # Add MIRI label to the right\n",
    "    ax_scat.text(8e-6, text_y_pos, \"MIRI →\", \n",
    "             ha='left', va='center', \n",
    "             bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=2),\n",
    "             fontsize=fontsize_lg)\n",
    "    ax_spec.legend(loc = 'upper left', bbox_to_anchor=(1, 1), fontsize = fontsize_lg)\n",
    "    print('mean ratio : ', np.mean(results[correction+'_synthetic_fluxes']/results['photo_fluxes']))\n",
    "    print('ratio std : ', np.std(results[correction+'_synthetic_fluxes']/results['photo_fluxes']))\n",
    "    correction_factors = np.array(results[correction+'_correction_values'])\n",
    "    print('corrections per solid angle : ', correction_factors/(np.pi*(results['radius'].value)**2))\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a541fc5-777a-4835-8acd-a75ff25c1d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580287a0-04bd-408a-916e-03a53a2b1f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
