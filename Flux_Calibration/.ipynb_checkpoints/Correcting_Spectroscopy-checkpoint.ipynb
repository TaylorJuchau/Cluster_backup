{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "from photutils.aperture import CircularAperture, aperture_photometry\n",
    "from spectral_cube import SpectralCube\n",
    "\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.table import Table\n",
    "import astropy.units as u\n",
    "from astropy.wcs import WCS\n",
    "from astropy.constants import c\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import simple_norm, imshow_norm\n",
    "from astropy.visualization import AsinhStretch\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize\n",
    "from astropy.visualization import SqrtStretch\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from astropy.nddata import block_reduce\n",
    "from astropy.nddata import Cutout2D\n",
    "\n",
    "home_directory = \"/d/ret1/Taylor/jupyter_notebooks/Research\" \n",
    "os.chdir(home_directory) #TJ change working directory to be the parent directory\n",
    "\n",
    "from Py_files.Basic_analysis import * #TJ import basic functions from custom package\n",
    "from Py_files.Convolution_script import * #TJ import convolution functions from custom package\n",
    "from Py_files.All_flux_calibration_functions import *\n",
    "with open(\"Data_files/misc_data/jwst_pivots.pkl\", \"rb\") as file:\n",
    "    jwst_pivots = pickle.load(file)\n",
    "with open(\"Data_files/misc_data/jwst_filter_means.pkl\", \"rb\") as file:\n",
    "    jwst_means = pickle.load(file)\n",
    "\n",
    "image_files, filter_files = generate_list_of_files()\n",
    "v0p3_images, _ = generate_v0p3_files()\n",
    "full_raw_ifu_files_loc0 = ['Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g140m-f100lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g235m-f170lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch4-shortmediumlong_s3d.fits']\n",
    "full_raw_ifu_files_loc1 = ['Data_files/IFU_files/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g140m-f100lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g235m-f170lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch4-shortmediumlong_s3d.fits']\n",
    "full_raw_ifu_files_loc3 = ['Data_files/IFU_files/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g140m-f100lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g235m-f170lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch4-shortmediumlong_s3d.fits']\n",
    "#TJ this full_spectrum is built by stitching cube0 to cube1 anchored to cube1\n",
    "#TJ then stitching cube2 to cube3 anchored to cube3, then stitching all the others unaltered\n",
    "full_spec = 'Data_files/misc_data/Updated_flux_calibration/full_spectrum_loc0_rad1p25.npy'\n",
    "locations = [[202.5062429, 47.2143358], [202.4335225, 47.1729608], [202.4340450, 47.1732517], [202.4823742, 47.1958589]]\n",
    "radius = 1.25*u.arcsec\n",
    "\n",
    "#TJ load the additive offset data\n",
    "\n",
    "loc0_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_0/g*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "    loc0_datasets.append(data)\n",
    "    \n",
    "loc1_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_1/g*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "    loc1_datasets.append(data)\n",
    "    \n",
    "loc2_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_2/g*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "    loc2_datasets.append(data)\n",
    "\n",
    "loc3_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_3/g*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "    loc3_datasets.append(data)\n",
    "    \n",
    "#TJ load the multiplicative data\n",
    "loc0_m_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_0/m*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "    loc0_m_datasets.append(data)\n",
    "    \n",
    "loc1_m_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_1/m*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "    loc1_m_datasets.append(data)\n",
    "    \n",
    "loc2_m_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_2/m*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "loc2_m_datasets.append(data)\n",
    "\n",
    "loc3_m_datasets = []\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_3/m*')\n",
    "for file in files:\n",
    "    data = np.load(file)\n",
    "    loc3_m_datasets.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_filter_relevent(filter, ifu_file):\n",
    "    '''If a filter's mean wavelength is inside the ifu, returns True\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter : type = string - name of filter (\"F115W\")\n",
    "    ifu_file : type = string - string to location of ifu file\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    True if filter's mean wavelength is inside the ifu_file, False if it is not.\n",
    "    '''\n",
    "    wls = SpectralCube.read(ifu_file, hdu = 'SCI').spectral_axis.to(u.m)\n",
    "    short, long = wls[0], wls[-1]\n",
    "    return (jwst_means[filter] > short) & (jwst_means[filter] < long)\n",
    "    \n",
    "def adjust_spectrum(original_ifu, filter_name, image_files, location, radius, adjustment_operation = 'add'):\n",
    "    '''Takes an ifu file and adjusts the flux through an aperture centered at a location with specified radius.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    original_ifu : type = string (or, see retry=True)- string to location of ifu file\n",
    "    filter_name : type = string - filter name like \"F115W\"\n",
    "    location : type = either SkyCoord or list of [ra, dec] values in degrees - location of center of aperture\n",
    "    radius : type = angular size - radius of aperture, must have units attached.\n",
    "    adjustment_operation (optional, defaults to 'add'): type = string - either 'add' or 'multiply' to specify what kind of correction to use\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    Structured Numpy array with 'intensity' and 'wavelength' keys\n",
    "    '''\n",
    "    if filter_name is None:\n",
    "        return get_IFU_spectrum(original_ifu, location, radius, replace_negatives = False), 0\n",
    "    else:\n",
    "        image_file = [x for x in image_files if extract_filter_name(x)==filter_name][0]\n",
    "        raw_data = get_IFU_spectrum(original_ifu, location, radius, replace_negatives = False)\n",
    "        filter_wl, filter_trans = get_filter_data(filter_name) #TJ this is the transmission vs wavelength function for this filter\n",
    "        image_flux = get_image_flux(image_file, location, radius, replace_negatives = False) #TJ this is the flux we SHOULD get\n",
    "        initial_synth_flux = get_Fnu_transmission(raw_data['intensity'], raw_data['wavelength'], filter_trans, filter_wl, warnings = True) #TJ this is the current synthetic flux we get\n",
    "        if adjustment_operation == 'add':\n",
    "            correction = image_flux - initial_synth_flux\n",
    "            raw_data['intensity'] = raw_data['intensity'] + correction\n",
    "            return raw_data, correction #TJ now corrected to match photometry\n",
    "        elif adjustment_operation == 'multiply':\n",
    "            correction = image_flux/initial_synth_flux\n",
    "            raw_data['intensity'] = raw_data['intensity']*correction\n",
    "            return raw_data, correction #TJ now corrected\n",
    "        else:\n",
    "            print('adjustment operation not recognized, only \"add\" or \"multiply\" are currently implemented')\n",
    "            return None\n",
    "        print('Something went wrong.')\n",
    "        return raw_data, correction#TJ Now corrected data\n",
    "\n",
    "\n",
    "def get_largest_filter_within(ifu_file):\n",
    "    '''Takes an ifu file and selects the filter with the largest bandpass that is entirely within it.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    ifu_file : type = string - string to location of ifu file\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    Filter name (ex. \"F115W\") corresponding to the largest filter entirely contained within the IFU file \n",
    "    '''\n",
    "    filters = [extract_filter_name(x) for x in filter_files if full_coverage(extract_filter_name(x),ifu_file)==\"good\"]\n",
    "    if len(filters)<1:\n",
    "        print(f'No filters entirely within {ifu_file}')\n",
    "        return None\n",
    "    else:\n",
    "        best_filter = filters[np.argmax([(get_filter_wl_range(fil)[1].value - get_filter_wl_range(fil)[0].value) for fil in filters])]\n",
    "        return best_filter\n",
    "\n",
    "def needed_datasets(filter_name, datasets):\n",
    "    '''returns which ifu_files should be considered when calculating the synthetic flux. If an ifu even slightly overlaps into\n",
    "    the filter's range it is included.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter : type = string - name of filter (\"F115W\")\n",
    "    datasets : type = structured array - array with keys for 'wavelength' and 'intensity'\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    Filter name (ex. \"F115W\") corresponding to the largest filter entirely contained within the IFU file \n",
    "    '''\n",
    "    needed = []\n",
    "    filter_wl, _ = get_filter_data(filter_name)\n",
    "    for data in datasets:\n",
    "        if (filter_wl[0] < data['wavelength'][-1]) & (filter_wl[-1] > data['wavelength'][0]):\n",
    "            needed.append(data)\n",
    "    return needed\n",
    "\n",
    "def merge_datasets(ds1, ds2):\n",
    "    \"\"\"\n",
    "    Merge two structured arrays with 'wavelength' and 'intensity' keys.\n",
    "    Handles overlapping regions by averaging intensities, and automatically\n",
    "    determines which dataset has higher wavelength resolution.\n",
    "    \"\"\"\n",
    "    # Sort by wavelength, just to be safe\n",
    "    ds1 = np.sort(ds1, order='wavelength')\n",
    "    ds2 = np.sort(ds2, order='wavelength')\n",
    "\n",
    "    # Determine wavelength resolutions\n",
    "    d1_res = np.mean(np.diff(ds1['wavelength']))\n",
    "    d2_res = np.mean(np.diff(ds2['wavelength']))\n",
    "\n",
    "    # Assign high- and low-resolution datasets\n",
    "    if d1_res < d2_res:\n",
    "        highres, lowres = ds1, ds2\n",
    "    else:\n",
    "        highres, lowres = ds2, ds1\n",
    "\n",
    "    # Determine overlap region\n",
    "    overlap_start = max(highres['wavelength'][0], lowres['wavelength'][0])\n",
    "    overlap_end   = min(highres['wavelength'][-1], lowres['wavelength'][-1])\n",
    "\n",
    "    # Interpolate the lowres data onto highres wavelengths (only inside overlap)\n",
    "    overlap_mask = (highres['wavelength'] >= overlap_start) & (highres['wavelength'] <= overlap_end)\n",
    "    interp_flux = np.interp(highres['wavelength'][overlap_mask],\n",
    "                            lowres['wavelength'], lowres['intensity'])\n",
    "\n",
    "    # Combine in overlap by averaging\n",
    "    merged_overlap_wl = highres['wavelength'][overlap_mask]\n",
    "    merged_overlap_intensity = 0.5 * (highres['intensity'][overlap_mask] + interp_flux)\n",
    "\n",
    "    # Keep the unique non-overlapping parts from both sides\n",
    "    full_low_side  = ds1[ds1['wavelength'] < overlap_start]\n",
    "    full_high_side = ds2[ds2['wavelength'] > overlap_end]\n",
    "\n",
    "    # Concatenate all pieces and sort\n",
    "    merged = np.concatenate([\n",
    "        full_low_side,\n",
    "        np.rec.fromarrays([merged_overlap_wl, merged_overlap_intensity],\n",
    "                          names=('wavelength', 'intensity')),\n",
    "        full_high_side\n",
    "    ])\n",
    "    merged = np.sort(merged, order='wavelength')\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def get_all_fluxes(filter_files, spec_datasets, image_files, location, radius):\n",
    "    '''Creates synthetic fluxes for all filters in the files that have wavelengths that span the entire filter.\n",
    "    For filters that straddle multiple wavelengths, any wavelength inside a filter that has intensity values\n",
    "    from multiple datasets uses the average intensity from each dataset.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter_files : type = list of strings - name of filter [\"F115W\", \"F2100W\"]\n",
    "    datasets : type = list of structured arrays - arrays with keys for 'wavelength' and 'intensity'\n",
    "    image_files : type = list of strings - strings to image files\n",
    "    location : type = either SkyCoord or list of [ra, dec] values in degrees - location of center of aperture\n",
    "    radius : type = angular size - radius of aperture, must have units attached.\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    A dictionary with keys for 'filter_name', 'mean_wl', 'synth_flux', and 'photo_flux'\n",
    "    '''\n",
    "    results = {}\n",
    "\n",
    "    results['filter_name'] = []\n",
    "    results['mean_wl'] = []\n",
    "    results['synth_flux'] = []\n",
    "    results['photo_flux'] = []\n",
    "    results['wavelength'] = []\n",
    "    results['intensity'] = []\n",
    "    \n",
    "    for i, data in enumerate(spec_datasets[1:]):\n",
    "        if i == 0:\n",
    "            prior_data = spec_datasets[0]\n",
    "        prior_data = merge_datasets(prior_data, data)\n",
    "        \n",
    "    results['wavelength'].append(prior_data['wavelength'])\n",
    "    results['intensity'].append(prior_data['intensity'])\n",
    "    \n",
    "    \n",
    "    for filter_file in filter_files:\n",
    "        filter_name = extract_filter_name(filter_file)\n",
    "        image_file = [x for x in image_files if extract_filter_name(x)==filter_name][0]\n",
    "        photo_flux = get_image_flux(image_file, location, radius, replace_negatives = False)\n",
    "        results['photo_flux'].append(photo_flux)\n",
    "        filter_wl, filter_trans = get_filter_data(filter_name)\n",
    "        results['filter_name'].append(filter_name)\n",
    "        results['mean_wl'].append(jwst_means[filter_name].value)\n",
    "        needed_data = needed_datasets(filter_name, spec_datasets)\n",
    "        if len(needed_data) == 0:\n",
    "            print('no spectral data was found for ', filter_name)\n",
    "        if len(needed_data)<2:\n",
    "            synth_flux = get_Fnu_transmission(needed_data[0]['intensity'], needed_data[0]['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "            results['synth_flux'].append(synth_flux)\n",
    "            \n",
    "        else:\n",
    "            full_data = merge_datasets(needed_data[0], needed_data[1])\n",
    "            synth_flux = get_Fnu_transmission(full_data['intensity'], full_data['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "            results['synth_flux'].append(synth_flux)\n",
    "    results['wavelength'] = np.array(results['wavelength'][0])\n",
    "    results['intensity'] = np.array(results['intensity'][0])\n",
    "    results['filter_name'] = np.array(results['filter_name'])\n",
    "    results['mean_wl'] = np.array(results['mean_wl'])\n",
    "    results['synth_flux'] = np.array(results['synth_flux'])\n",
    "    results['photo_flux'] = np.array(results['photo_flux'])\n",
    "    return results\n",
    "\n",
    "def plot_spec_and_scatter(spec_x, spec_y, scat_x, scat_y):\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TJ creates calibrated data files, this takes several minutes and doesnt generally need to be redone\n",
    "\n",
    "loc0_add_correction_factors = []\n",
    "loc1_add_correction_factors = []\n",
    "loc2_add_correction_factors = []\n",
    "loc3_add_correction_factors = []\n",
    "\n",
    "loc0_mult_correction_factors = []\n",
    "loc1_mult_correction_factors = []\n",
    "loc2_mult_correction_factors = []\n",
    "loc3_mult_correction_factors = []\n",
    "\n",
    "#TJ calibrate data using additive offset\n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc0):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[0], radius, adjustment_operation = 'add')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_0/grism_{i+1}_of_7.npy', data)\n",
    "    loc0_add_correction_factors.append(correction)\n",
    "    print(i)\n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc1):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[1], radius, adjustment_operation = 'add')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_1/grism_{i+1}_of_7.npy', data)\n",
    "    loc1_add_correction_factors.append(correction)\n",
    "    \n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc1):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[2], radius, adjustment_operation = 'add')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_2/grism_{i+1}_of_7.npy', data)\n",
    "    loc2_add_correction_factors.append(correction)\n",
    "    \n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc3):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[3], radius, adjustment_operation = 'add')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_3/grism_{i+1}_of_7.npy', data)\n",
    "    loc3_add_correction_factors.append(correction)\n",
    "\n",
    "\n",
    "#TJ calibrate data using multiplicative offset\n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc0):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[0], radius, adjustment_operation = 'multiply')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_0/multi-grism_{i+1}_of_7.npy', data)\n",
    "    loc0_mult_correction_factors.append(correction)\n",
    "    \n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc1):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[1], radius, adjustment_operation = 'multiply')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_1/multi-grism_{i+1}_of_7.npy', data)\n",
    "    loc1_mult_correction_factors.append(correction)\n",
    "    \n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc1):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[2], radius, adjustment_operation = 'multiply')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_2/multi-grism_{i+1}_of_7.npy', data)\n",
    "    loc2_mult_correction_factors.append(correction)\n",
    "    \n",
    "for i, ifu_file in enumerate(full_raw_ifu_files_loc3):\n",
    "    data, correction = adjust_spectrum(ifu_file, get_largest_filter_within(ifu_file), v0p3_images, locations[3], radius, adjustment_operation = 'multiply')\n",
    "    np.save(f'Data_files/IFU_files/calibrated_IFUs/location_3/multi-grism_{i+1}_of_7.npy', data)\n",
    "    loc3_mult_correction_factors.append(correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "addresults0 = get_all_fluxes(filter_files, loc0_datasets, v0p3_images, locations[0], radius)\n",
    "addresults1 = get_all_fluxes(filter_files, loc1_datasets, v0p3_images, locations[1], radius)\n",
    "addresults2 = get_all_fluxes(filter_files, loc2_datasets, v0p3_images, locations[2], radius)\n",
    "addresults3 = get_all_fluxes(filter_files, loc3_datasets, v0p3_images, locations[3], radius)\n",
    "\n",
    "multresults0 = get_all_fluxes(filter_files, loc0_m_datasets, v0p3_images, locations[0], radius)\n",
    "multresults1 = get_all_fluxes(filter_files, loc1_m_datasets, v0p3_images, locations[1], radius)\n",
    "multresults2 = get_all_fluxes(filter_files, loc2_m_datasets, v0p3_images, locations[2], radius)\n",
    "multresults3 = get_all_fluxes(filter_files, loc3_m_datasets, v0p3_images, locations[3], radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc0_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (45,30))\n",
    "ax_spec = fig.add_axes((0.05, 0.4, 1, 0.6))\n",
    "ax_scat = fig.add_axes((0.05, 0.05, 1, 0.35))\n",
    "fontsize_sm = 35\n",
    "fontsize_lg = 45\n",
    "marker_size = 250\n",
    "cube_colors = ['purple', 'blue', 'cyan', 'green', 'orange', 'red', 'pink']\n",
    "spec_y_min = 1 #TJ Flux should always be around 10^-20 so setting limits of 0-1 should never be too strict\n",
    "spec_y_max = 0\n",
    "short_bounds = []\n",
    "long_bounds = []\n",
    "for i, dataset in  enumerate(loc0_datasets):\n",
    "        \n",
    "        short_bounds.append(dataset['wavelength'][0])\n",
    "        long_bounds.append(dataset['wavelength'][-1])\n",
    "        ax_spec.plot(dataset['wavelength'], dataset['intensity'], alpha = 0.5, color = cube_colors[i])\n",
    "        spec_y_min = min(spec_y_min, np.percentile(dataset['intensity'], 1)*0.5)\n",
    "        spec_y_max = max(spec_y_max, np.percentile(dataset['intensity'], 98)*1.5)\n",
    "\n",
    "\n",
    "#ax_spec.plot(addresults0['wavelength'], addresults0['intensity'], linewidth = 5)\n",
    "ax_scat.plot(addresults0['wavelength'], [1]*len(addresults0['intensity']), color = 'white', alpha = 0)\n",
    "ax_spec.scatter(addresults0['mean_wl'], addresults0['synth_flux'], marker = '*', s=marker_size, color = 'black')\n",
    "ax_spec.scatter([], [], marker = '', s=marker_size, color = 'black', label = 'Synth')\n",
    "ax_spec.scatter(addresults0['mean_wl'], addresults0['photo_flux'], marker = \"o\", s=marker_size, color = 'black')\n",
    "ax_spec.scatter([], [], marker = \"o\", s=marker_size, color = 'black', label = 'Photo')\n",
    "for i, filter in enumerate(filter_names):\n",
    "    filter_short_wl, filter_long_wl = [x.value for x in get_filter_wl_range(filter)]\n",
    "    ax_spec.hlines(y=addresults0['photo_flux'][i], xmin=filter_short_wl, xmax=filter_long_wl, color='black', alpha=0.7, linewidth=1)\n",
    "\n",
    "ax_scat.scatter(addresults0['mean_wl'], addresults0['synth_flux']/addresults0['photo_flux'], s=marker_size, color = 'black')\n",
    "\n",
    "\n",
    "ax_scat.tick_params(axis='x', which='minor', width=2, length=10, right=True, top=True, direction='in',\n",
    "                   labelsize=fontsize_sm)\n",
    "ax_scat.tick_params(axis='x', which='major', width=3, length=15, right=True, top=True, direction='in',\n",
    "                   labelsize=fontsize_sm)\n",
    "ax_scat.tick_params(axis='y', which='both', width=3, length=15, right=True, top=True, direction='in',\n",
    "                   labelsize=fontsize_sm)\n",
    "ax_scat.set_xlabel('wavelength (m)', fontsize = 40)\n",
    "ax_scat.set_ylabel('synthetic/photometric flux', fontsize = 40)\n",
    "ax_spec.tick_params(axis='x', which='minor', width=2, length=10, right=True, top=True, direction='in',\n",
    "                   labelsize=fontsize_sm)\n",
    "ax_spec.tick_params(axis='x', which='major', width=3, length=15, right=True, top=True, direction='in',\n",
    "                   labelsize=fontsize_sm)\n",
    "ax_spec.tick_params(axis='y', which='both', width=3, length=15, right=True, top=True, direction='in',\n",
    "                   labelsize=fontsize_sm)\n",
    "ax_spec.set_ylabel('Intensity (MJy/sr)', fontsize = 40)\n",
    "\n",
    "ax_spec.set_title(f'1.25 arcsecond-radius annular intensity vs radius', fontsize = 50)\n",
    "\n",
    "ax_scat.set_xscale('log')\n",
    "ax_spec.set_xscale('log')\n",
    "ax_spec.set_yscale('log')\n",
    "label_positions = []  # to store display-space positions\n",
    "\n",
    "for x, y, name in zip(addresults0['mean_wl'], addresults0['synth_flux']/addresults0['photo_flux'], addresults0['filter_name']):\n",
    "    # initial offset (just below the point)\n",
    "    filter_short_wl, filter_long_wl = [x.value for x in get_filter_wl_range(name)]\n",
    "    ax_scat.hlines(y=y, xmin=filter_short_wl, xmax=filter_long_wl, color='black', alpha=0.7, linewidth=1)\n",
    "    y_offset = -0.1 \n",
    "    \n",
    "    # convert data point to display coords\n",
    "    x_disp, y_disp = ax_spec.transData.transform((x, y))\n",
    "    \n",
    "    # check overlap in display coordinates\n",
    "    too_close = False\n",
    "    for (xx, yy) in label_positions:\n",
    "        if abs(x_disp - xx) < 20 and abs(y_disp + y_offset - yy) < 5:  \n",
    "            # 20px horizontal & 12px vertical proximity → overlap\n",
    "            too_close = True\n",
    "            break\n",
    "    \n",
    "    # if overlapping, nudge upward instead of downward\n",
    "    if too_close:\n",
    "        y_offset = +0.2  \n",
    "    if y < 0.8:\n",
    "        y_offset = +0.22\n",
    "    # save adjusted label display position\n",
    "    label_positions.append((x_disp, y_disp + y_offset))\n",
    "            \n",
    "    \n",
    "    # actually plot text in data coordinates\n",
    "    ax_scat.text(\n",
    "        x, y + y_offset,\n",
    "        name,\n",
    "        ha=\"center\", va=\"top\",\n",
    "        fontsize=fontsize_sm, rotation = 90,\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7, pad=0.5)\n",
    "    )\n",
    "\n",
    "ymin, ymax = ax_scat.get_ylim()\n",
    "text_y_pos = ymin * 1.1\n",
    "ax_scat.axhline(y = 1, color = 'gray', linestyle = '--', linewidth = 4, alpha = 0.5)\n",
    "ax_scat.axvline(x=7.650000025896587e-06, color='gray', linestyle='--', linewidth=4, alpha=0.7)\n",
    "ax_scat.text(7.25e-6, text_y_pos, \"← NIRCam\", \n",
    "             ha='right', va='center', \n",
    "             bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=2),\n",
    "             fontsize=fontsize_lg)\n",
    "\n",
    "# Add MIRI label to the right\n",
    "ax_scat.text(8e-6, text_y_pos, \"MIRI →\", \n",
    "         ha='left', va='center', \n",
    "         bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', pad=2),\n",
    "         fontsize=fontsize_lg)\n",
    "ax_spec.legend(loc = 'upper left', bbox_to_anchor=(1, 1))\n",
    "print(np.mean(addresults0['synth_flux']/addresults0['photo_flux']))\n",
    "print(np.std(addresults0['synth_flux']/addresults0['photo_flux']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_filter_wl_range('F150W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresults0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12, 10))  \n",
    "ax_spec = fig.add_axes((0.05, 0.4, 1, 0.6))\n",
    "ax_scat = fig.add_axes((0.05, 0.05, 1, 0.35))\n",
    "ax_spec.plot(multresults0['wavelength'], multresults0['intensity'])\n",
    "ax_spec.scatter(multresults0['mean_wl'], multresults0['synth_flux'], marker = '*')\n",
    "ax_spec.scatter(multresults0['mean_wl'], multresults0['photo_flux'], marker = \"o\")\n",
    "\n",
    "ax_scat.scatter(multresults0['mean_wl'], multresults0['synth_flux']/multresults0['photo_flux'])\n",
    "#ax_scat.scatter(addresults1['mean_wl'], addresults1['synth_flux']/addresults1['photo_flux'])\n",
    "#ax_scat.scatter(addresults2['mean_wl'], addresults2['synth_flux']/addresults2['photo_flux'])\n",
    "#ax_scat.scatter(addresults3['mean_wl'], addresults3['synth_flux']/addresults3['photo_flux'])\n",
    "ax_scat.set_xscale('log')\n",
    "ax_spec.set_xscale('log')\n",
    "ax_spec.set_yscale('log')\n",
    "print(np.mean(multresults0['synth_flux']/multresults0['photo_flux']))\n",
    "print(np.std(multresults0['synth_flux']/multresults0['photo_flux']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge_datasets(ds1, ds2):\n",
    "    \"\"\"\n",
    "    Merge two structured arrays with 'wavelength' and 'intensity' keys.\n",
    "    Handles overlapping regions by averaging intensities, and automatically\n",
    "    determines which dataset has higher wavelength resolution.\n",
    "    \"\"\"\n",
    "    # Sort by wavelength, just to be safe\n",
    "    ds1 = np.sort(ds1, order='wavelength')\n",
    "    ds2 = np.sort(ds2, order='wavelength')\n",
    "\n",
    "    # Determine wavelength resolutions\n",
    "    d1_res = np.mean(np.diff(ds1['wavelength']))\n",
    "    d2_res = np.mean(np.diff(ds2['wavelength']))\n",
    "\n",
    "    # Assign high- and low-resolution datasets\n",
    "    if d1_res < d2_res:\n",
    "        highres, lowres = ds1, ds2\n",
    "    else:\n",
    "        highres, lowres = ds2, ds1\n",
    "\n",
    "    # Determine overlap region\n",
    "    overlap_start = max(highres['wavelength'][0], lowres['wavelength'][0])\n",
    "    overlap_end   = min(highres['wavelength'][-1], lowres['wavelength'][-1])\n",
    "\n",
    "    # Interpolate the lowres data onto highres wavelengths (only inside overlap)\n",
    "    overlap_mask = (highres['wavelength'] >= overlap_start) & (highres['wavelength'] <= overlap_end)\n",
    "    interp_flux = np.interp(highres['wavelength'][overlap_mask],\n",
    "                            lowres['wavelength'], lowres['intensity'])\n",
    "\n",
    "    # Combine in overlap by averaging\n",
    "    merged_overlap_wl = highres['wavelength'][overlap_mask]\n",
    "    merged_overlap_intensity = 0.5 * (highres['intensity'][overlap_mask] + interp_flux)\n",
    "\n",
    "    # Keep the unique non-overlapping parts from both sides\n",
    "    full_low_side  = ds1[ds1['wavelength'] < overlap_start]\n",
    "    full_high_side = ds2[ds2['wavelength'] > overlap_end]\n",
    "\n",
    "    # Concatenate all pieces and sort\n",
    "    merged = np.concatenate([\n",
    "        full_low_side,\n",
    "        np.rec.fromarrays([merged_overlap_wl, merged_overlap_intensity],\n",
    "                          names=('wavelength', 'intensity')),\n",
    "        full_high_side\n",
    "    ])\n",
    "    merged = np.sort(merged, order='wavelength')\n",
    "\n",
    "    return merged\n",
    "\n",
    "data = merge_datasets(dataset1, dataset2)\n",
    "#plt.loglog(dataset1['wavelength'], dataset1['intensity'], alpha = 0.3, color = 'red')\n",
    "#plt.loglog(dataset2['wavelength'], dataset2['intensity'], alpha = 0.3, color = 'blue')\n",
    "plt.loglog(data['wavelength'], data['intensity'], color = 'orange', linewidth = 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax_spec = fig.add_axes((0.05, 0.4, 1, 0.6))\n",
    "ax_scat = fig.add_axes((0.05, 0.05, 1, 0.35))\n",
    "loc = locations[0]\n",
    "\n",
    "for ifu_file in full_raw_ifu_files_loc0[:2]:\n",
    "    filter_name = get_largest_filter_within(ifu_file)\n",
    "    image_file = [x for x in v0p3_images if extract_filter_name(x)==filter_name][0]\n",
    "    raw_data = get_IFU_spectrum(ifu_file, loc, radius, replace_negatives = False)\n",
    "    dataset = adjust_spectrum(ifu_file, filter_name, v0p3_images, loc, radius, adjustment_operation = 'add')\n",
    "    filter_wl, filter_trans = get_filter_data(filter_name)\n",
    "    synth = get_Fnu_transmission(dataset['intensity'], dataset['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "    photo = get_image_flux(image_file, loc, radius, replace_negatives = False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax_spec.plot(raw_data['wavelength'], raw_data['intensity'], color = 'red', alpha = 0.5)\n",
    "    ax_spec.plot(dataset['wavelength'], dataset['intensity'], color = 'green', alpha = 0.5)\n",
    "    ax_spec.set_xscale('log')\n",
    "    ax_spec.set_yscale('log')\n",
    "    for filter_file in filter_files:\n",
    "        \n",
    "        filter_name = extract_filter_name(filter_file)\n",
    "        if is_filter_relevent(filter_name, ifu_file):\n",
    "            filter_mean = jwst_means[filter_name].value\n",
    "            image_file = [x for x in v0p3_images if extract_filter_name(x)==filter_name][0]\n",
    "            photo_flux =  get_image_flux(image_file, loc, radius, replace_negatives = False)\n",
    "            filter_wl, filter_trans = get_filter_data(filter_name)\n",
    "            \n",
    "            synth_flux = get_Fnu_transmission(dataset['intensity'], dataset['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "            ax_spec.scatter(filter_mean, photo_flux, marker = \"*\", s =25, color = 'blue')\n",
    "            ax_spec.scatter(filter_mean, synth_flux, marker = \"*\", s = 10, color = 'black')\n",
    "            ax_scat.scatter(filter_mean, (synth_flux/photo_flux))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "            \n",
    "filters = [extract_filter_name(x) for x in filter_files if full_coverage(extract_filter_name(x),full_raw_ifu_files_loc0[0])==\"good\"]\n",
    "\n",
    "\n",
    "loc = locations[0]\n",
    "ifu_file = full_raw_ifu_files_loc0[0]\n",
    "fig = plt.figure(figsize = (45,30))\n",
    "ax_spec = fig.add_axes((0.05, 0.4, 1, 0.6))\n",
    "ax_scatter = fig.add_axes((0.05, 0.05, 1, 0.35))\n",
    "ax_spec.set_yscale('log')\n",
    "ax_spec.set_xscale('log')\n",
    "for filter_name in filters:\n",
    "    image_file = [x for x in image_files if extract_filter_name(x)==filter_name][0]\n",
    "    raw_data = get_IFU_spectrum(ifu_file, loc, radius, replace_negatives = False)\n",
    "    dataset = adjust_spectrum(ifu_file, filter_name, loc, radius, adjustment_operation = 'add', retry = False)\n",
    "    filter_wl, filter_trans = get_filter_data(filter_name)\n",
    "    synth = get_Fnu_transmission(dataset['intensity'], dataset['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "    photo = get_image_flux(image_file, loc, radius, replace_negatives = False)\n",
    "    ax_spec.plot(raw_data['wavelength'], raw_data['intensity'], alpha = 0.5, color = 'red')\n",
    "    ax_spec.plot(dataset['wavelength'], dataset['intensity'], alpha = 0.5, color = 'green')\n",
    "    for filter_file in filter_files[:4]:\n",
    "        filter_name = extract_filter_name(filter_file)\n",
    "        filter_mean = jwst_means[filter_name].value\n",
    "        image_file = [x for x in image_files if extract_filter_name(x)==filter_name][0]\n",
    "        photo_flux =  get_image_flux(image_file, loc, radius, replace_negatives = False)\n",
    "        filter_wl, filter_trans = get_filter_data(filter_name)\n",
    "        synth_flux = get_Fnu_transmission(dataset['intensity'], dataset['wavelength'], filter_trans, filter_wl, warnings = True)\n",
    "        plt.scatter(filter_mean, photo_flux, marker = \"*\", s =25, color = 'blue')\n",
    "        plt.scatter(filter_mean, synth_flux, marker = \"*\", s = 10, color = 'black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(full_raw_ifu_files_loc0):\n",
    "    print(i, get_largest_filter_within(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_2/*')\n",
    "colors = ['purple', 'blue', 'cyan', 'green', 'orange', 'red', 'pink']\n",
    "for i, file in enumerate(files):\n",
    "    data = np.load(file)\n",
    "    plt.loglog(data['wavelength'], data['intensity'], color = colors[i])\n",
    "plt.show()\n",
    "\n",
    "files = glob.glob('Data_files/IFU_files/calibrated_IFUs/location_3/*')\n",
    "colors = ['purple', 'blue', 'cyan', 'green', 'orange', 'red', 'pink']\n",
    "for i, file in enumerate(files):\n",
    "    data = np.load(file)\n",
    "    plt.loglog(data['wavelength'], data['intensity'], color = colors[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def check_internet():\n",
    "    # Try pinging Google's DNS with one packet\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ping\", \"-c\", \"1\", \"-W\", \"1\", \"8.8.8.8\"],  # \"-W 1\" timeout = 1s\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        return result.returncode == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"Monitoring internet connectivity every second... (stop cell to end)\")\n",
    "while True:\n",
    "    connected = check_internet()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    if connected:\n",
    "        print(f\"{timestamp}  CONNECTED\")\n",
    "    else:\n",
    "        print(f\"{timestamp}  *** DISCONNECTED ***\")\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
