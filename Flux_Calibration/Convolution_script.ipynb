{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"STPSF_PATH\"] = \"/d/ret1/Taylor/stpsf-data/\" \n",
    "import webbpsf\n",
    "os.environ[\"STPSF_PATH\"] = \"/d/ret1/Taylor/stpsf-data/\" #TJ for some reason this only works if you do this line twice... no idea why\n",
    "\n",
    "print(os.path.exists(os.environ[\"STPSF_PATH\"]))\n",
    "print(os.environ[\"STPSF_PATH\"]) #TJ check that this kernel has access to the filter files\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import simple_norm, imshow_norm\n",
    "from ipywidgets import interact, Dropdown\n",
    "from astropy.wcs import WCS\n",
    "from astropy.constants import c\n",
    "from photutils.aperture import CircularAperture, aperture_photometry\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "from tabulate import tabulate\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from astropy.convolution import convolve_fft, Gaussian2DKernel\n",
    "\n",
    "\n",
    "#parent_dir = Path().resolve().parent #TJ current notebook's parent directory\n",
    "\n",
    "os.chdir('/d/ret1/Taylor/jupyter_notebooks/Research') #TJ change working directory to be the parent directory\n",
    "from Py_files.Basic_analysis import * #import basic functions from custom package\n",
    "from Py_files.Image_vs_spectra import * \n",
    "#TJ import flux calibration functions (mainly compare_IFU_to_image(IFU_filepath, image_filepath, filter_filepath, loc, radius))\n",
    "\n",
    "def get_filter_wl_range(filter):\n",
    "    '''Use the filter files to determine what wavelength range we need for each filter\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    filter : type = str - string describing the filter name (case sensitive), for example \"F335M\"\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    Path to newly convolved file as a string\n",
    "    '''   \n",
    "    filter_files = ['/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F115W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F140M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F150W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F164N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F182M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F187N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F200W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F210M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F212N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F250M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F300M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F335M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F360M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F405N.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F430M.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/nircam/F444W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F560W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F770W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1000W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1130W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1280W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1500W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F1800W.dat',\n",
    "       '/d/crow1/tools/cigale/database_builder/filters/jwst/miri/F2100W.dat']\n",
    "    filter_file = [filer_filepath for filer_filepath in filter_files if extract_filter_name(filer_filepath).upper() == filter][0]\n",
    "    filter_data = []\n",
    "    with open(filter_file, 'r') as f:\n",
    "        header = f.readline().strip().split()\n",
    "        for line in f:\n",
    "            data_line = line.strip().split()\n",
    "            filter_data.append(data_line)\n",
    "\n",
    "    header, filter_T = filter_data[:2], np.array(filter_data[2:])\n",
    "    filter_wl = [try_float(filter_T[i,0])*1e-10 for i in range(len(filter_T))]\n",
    "    return filter_wl[0]*u.m, filter_wl[-1]*u.m\n",
    "\n",
    "def convolve(IFU_fits_file, instrument, filter, output_file = None):\n",
    "    '''Convolve an IFU cube to the PSF of the provided filter.\n",
    "    -------------\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    IFU_fits_file : type = str - string to location of IFU fits that you want to convolve.\n",
    "    instrument : type = str - Either \"MIRI\" or \"NIRCam\" depending on wavelength\n",
    "    filter : type = str - string describing the filter name (case sensitive), for example \"F335M\"\n",
    "    output_file (optional, defaults to use the IFU_file with _convolved_to_{filter}) : type = str - name of the convolved file\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    Path to newly convolved file as a string\n",
    "    '''   \n",
    "\n",
    "    IFU_hdul = fits.open(IFU_fits_file)\n",
    "    header = IFU_hdul[\"SCI\"].header\n",
    "    cube = SpectralCube.read(IFU_fits_file, hdu='SCI')\n",
    "    spectral_axis = cube.spectral_axis  #TJ in meters\n",
    "    # === Load webbpsf instrument ===\n",
    "    if instrument == 'NIRCam':\n",
    "        inst = webbpsf.NIRCam()\n",
    "    elif instrument == \"MIRI\":\n",
    "        inst = webbpsf.MIRI()\n",
    "    inst.filter = filter\n",
    "\n",
    "    # === Prepare output cube ===\n",
    "    convolved_data = np.zeros_like(cube.unmasked_data[:].value)\n",
    "    tqdm_kwargs = {\n",
    "        'dynamic_ncols': True,  # Auto-adjusts width\n",
    "        'mininterval': 0.5,     # Update every 0.5 seconds (optional)\n",
    "        'position': 0,          # Fix position (set to 0 for notebooks)\n",
    "        'leave': True           # Leaves progress bar after completion\n",
    "    }\n",
    "    # === Loop through wavelengths and convolve ===\n",
    "    for i, wavelength in enumerate(tqdm(spectral_axis, desc=f\"Convolving to {filter}\")):\n",
    "        psf = inst.calc_psf(monochromatic=wavelength.to(u.m).value)    \n",
    "        psf_data = psf[0].data\n",
    "        psf_data /= psf_data.sum()  # Normalize PSF to conserve flux\n",
    "    \n",
    "        image_slice = cube.filled_data[...].value[i]  # 2D image at this wavelength\n",
    "        convolved_slice = convolve_fft(image_slice, psf_data, normalize_kernel=True, boundary='fill', fill_value=0)\n",
    "        convolved_data[i] = convolved_slice\n",
    "    \n",
    "    # === Save the convolved cube ===\n",
    "    out_hdu = fits.PrimaryHDU(convolved_data, header=header)\n",
    "    if output_file:\n",
    "        out_hdu.writeto(f\"Data_files/IFU_files/{output_file}\", overwrite=True)\n",
    "        print(f\"✅ PSF convolution complete and saved as {output_file}\")\n",
    "        return f\"Data_files/IFU_files/{output_file}\"\n",
    "    else:\n",
    "        out_hdu.writeto(f\"Data_files/IFU_files/{IFU_fits_file}_convolved_to_{filter}.fits\", overwrite=True)\n",
    "        print(f\"✅ PSF convolution complete and saved as {IFU_fits_file}_convolved_to_{filter}\")\n",
    "        return f\"Data_files/IFU_files/{IFU_fits_file}_convolved_to_{filter}.fits\"\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === User Inputs ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #TJ add the path to psf data files if needed (may need to go back through adding the path in terminal)\n",
    "    #filters = ['F335M', 'F360M', 'F405N', 'F430M', 'F444W']\n",
    "    filters = ['F335M', 'F360M', 'F405N', 'F430M', 'F444W', 'F560W', 'F770W', 'F1000W', 'F1130W', 'F1280W', 'F1500W', 'F1800W', 'F2100W'] \n",
    "    # === Set up ===\n",
    "    \n",
    "    for filter in filters:\n",
    "        if filter in ['F335M', 'F360M', 'F405N', 'F430M', 'F444W']:\n",
    "            IFU_filepath = \"Data_files/IFU_files/M51_SW_f290lp_g395m-f290lp_s3d.fits\"\n",
    "            instrument = 'NIRCam'  # or 'MIRI' if using MIRI cube\n",
    "        if filter in ['F560W']:\n",
    "            IFU_filepath = 'Data_files/IFU_files/SW_IFU_ch1-shortmediumlong_s3d.fits'\n",
    "            instrument = \"MIRI\"\n",
    "        if filter in ['F770W', 'F1000W']:\n",
    "            IFU_filepath = 'Data_files/IFU_files/SW_IFU_ch2-shortmediumlong_s3d.fits'\n",
    "            instrument = \"MIRI\"\n",
    "        if filter in ['F1130W', 'F1280W', 'F1500W']:\n",
    "            IFU_filepath = 'Data_files/IFU_files/SW_IFU_ch3-shortmediumlong_s3d.fits'\n",
    "            instrument = \"MIRI\"\n",
    "        if filter in ['F1800W', 'F2100W']:\n",
    "            IFU_filepath = 'Data_files/IFU_files/SW_IFU_ch4-shortmediumlong_s3d.fits'\n",
    "            instrument = \"MIRI\"\n",
    "            \n",
    "        IFU_hdul = fits.open(IFU_filepath)\n",
    "        header = IFU_hdul[\"SCI\"].header\n",
    "        cube = SpectralCube.read(IFU_filepath, hdu='SCI')\n",
    "        wcs = WCS(header)\n",
    "        spectral_axis = cube.spectral_axis  #TJ in meters\n",
    "        # === Load webbpsf instrument ===\n",
    "        if instrument == 'NIRCam':\n",
    "            inst = webbpsf.NIRCam()\n",
    "        elif instrument == \"MIRI\":\n",
    "            inst = webbpsf.MIRI()\n",
    "        inst.filter = filter\n",
    "    \n",
    "        # === Prepare output cube ===\n",
    "        convolved_data = np.zeros_like(cube.unmasked_data[:].value)\n",
    "        tqdm_kwargs = {\n",
    "            'dynamic_ncols': True,  # Auto-adjusts width\n",
    "            'mininterval': 0.5,     # Update every 0.5 seconds (optional)\n",
    "            'position': 0,          # Fix position (set to 0 for notebooks)\n",
    "            'leave': True           # Leaves progress bar after completion\n",
    "        }\n",
    "        # === Loop through wavelengths and convolve ===\n",
    "        for i, wavelength in enumerate(tqdm(spectral_axis, desc=f\"Convolving to {filter}\")):\n",
    "            psf = inst.calc_psf(monochromatic=wavelength.to(u.m).value)    \n",
    "            psf_data = psf[0].data\n",
    "            psf_data /= psf_data.sum()  # Normalize PSF to conserve flux\n",
    "        \n",
    "            image_slice = cube.filled_data[...].value[i]  # 2D image at this wavelength\n",
    "            convolved_slice = convolve_fft(image_slice, psf_data, normalize_kernel=True, boundary='fill', fill_value=0)\n",
    "            convolved_data[i] = convolved_slice\n",
    "        \n",
    "        # === Save the convolved cube ===\n",
    "        out_hdu = fits.PrimaryHDU(convolved_data, header=header)\n",
    "        out_hdu.writeto(f\"Data_files/IFU_files/M51_IFU_convolved_to_{inst.filter}.fits\", overwrite=True)\n",
    "        \n",
    "        print(\"✅ PSF convolution complete and saved as 'M51_IFU_convolved.fits'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.signal import fftconvolve\n",
    "import astropy.units as u\n",
    "from astropy.nddata import Cutout2D\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "def generate_psf(\n",
    "    psf_type,\n",
    "    fwhm_pix=None,\n",
    "    size=51,\n",
    "    psf_array=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a 2D PSF kernel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    psf_type : str\n",
    "        Type of PSF. Options:\n",
    "        - 'gaussian'\n",
    "        - 'array' (user-supplied PSF)\n",
    "    fwhm_pix : float, optional\n",
    "        FWHM of Gaussian PSF in pixels (required if psf_type='gaussian')\n",
    "    size : int\n",
    "        Size of PSF array (must be odd)\n",
    "    psf_array : 2D ndarray, optional\n",
    "        User-provided PSF array (required if psf_type='array')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    psf : 2D ndarray\n",
    "        Normalized PSF kernel\n",
    "    \"\"\"\n",
    "\n",
    "    if size % 2 == 0:\n",
    "        raise ValueError(\"PSF size must be odd.\")\n",
    "\n",
    "    if psf_type == \"gaussian\":\n",
    "        if fwhm_pix is None:\n",
    "            raise ValueError(\"fwhm_pix must be provided for Gaussian PSF.\")\n",
    "\n",
    "        sigma = fwhm_pix / (2.0 * np.sqrt(2.0 * np.log(2.0)))\n",
    "\n",
    "        y, x = np.mgrid[:size, :size]\n",
    "        cy = cx = size // 2\n",
    "\n",
    "        psf = np.exp(-((x - cx)**2 + (y - cy)**2) / (2.0 * sigma**2))\n",
    "\n",
    "    elif psf_type == \"array\":\n",
    "        if psf_array is None:\n",
    "            raise ValueError(\"psf_array must be provided for psf_type='array'\")\n",
    "        psf = np.array(psf_array, dtype=float)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown psf_type: {psf_type}\")\n",
    "\n",
    "    # Normalize to unit integral (flux-conserving)\n",
    "    psf /= np.sum(psf)\n",
    "\n",
    "    return psf\n",
    "\n",
    "\n",
    "from astropy.io import fits\n",
    "import os\n",
    "\n",
    "\n",
    "def convolve_ifu_cube(\n",
    "    ifu_fits_path,\n",
    "    psf,\n",
    "    output_path,\n",
    "    ext='SCI'\n",
    "):\n",
    "    \"\"\"\n",
    "    Convolve an IFU cube spatially with a PSF and save the result.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ifu_fits_path : str\n",
    "        Path to input IFU FITS file\n",
    "    psf : 2D ndarray\n",
    "        PSF kernel (must be normalized)\n",
    "    output_path : str\n",
    "        Path to save convolved FITS file\n",
    "    ext : int\n",
    "        FITS extension containing the data cube\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    with fits.open(ifu_fits_path) as hdul:\n",
    "        data = hdul[ext].data\n",
    "        header = hdul[ext].header\n",
    "\n",
    "    # Expected shape: (n_lambda, ny, nx)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected IFU cube with shape (n_lambda, ny, nx)\")\n",
    "\n",
    "    n_lambda, ny, nx = data.shape\n",
    "    convolved = np.full_like(data, np.nan)\n",
    "\n",
    "    for k in range(n_lambda):\n",
    "        slice_k = data[k]\n",
    "\n",
    "        if np.all(~np.isfinite(slice_k)):\n",
    "            continue\n",
    "\n",
    "        # Replace NaNs with zero for convolution\n",
    "        mask = np.isfinite(slice_k)\n",
    "        slice_filled = np.zeros_like(slice_k)\n",
    "        slice_filled[mask] = slice_k[mask]\n",
    "\n",
    "        conv = fftconvolve(slice_filled, psf, mode=\"same\")\n",
    "\n",
    "        # Renormalize to avoid edge bias\n",
    "        weight = fftconvolve(mask.astype(float), psf, mode=\"same\")\n",
    "        good = weight > 0\n",
    "        conv[good] /= weight[good]\n",
    "        conv[~good] = np.nan\n",
    "\n",
    "        convolved[k] = conv\n",
    "\n",
    "    # Save output\n",
    "    hdu = fits.PrimaryHDU(convolved, header=header)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    hdu.writeto(output_path, overwrite=True)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/d/ret1/Taylor/jupyter_notebooks/Research') #TJ change working directory to be the parent directory\n",
    "from Py_files.Functions import *\n",
    "image_files, filter_files = generate_list_of_files(filter_directory, image_directory)\n",
    "\n",
    "full_raw_ifu_files_loc0 = ['Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch4-shortmediumlong_s3d_trimmed.fits']\n",
    "\n",
    "psf = generate_psf(\n",
    "    psf_type=\"gaussian\",\n",
    "    fwhm_pix=2.3,\n",
    "    size=61\n",
    ")\n",
    "\n",
    "# Apply convolution\n",
    "test_convolution = []\n",
    "for i, file in enumerate(full_raw_ifu_files_loc0):\n",
    "    new_file = convolve_ifu_cube(ifu_fits_path=file, psf=psf, output_path=f\"Data_files/misc_data/test_data/testing_convolutions_{i}.fits\")\n",
    "    test_convolution.append(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_synth(filter, ifu_fileset, loc, radius, image_files=image_files, color_min_max = [1, 99.5]):\n",
    "    \"\"\"\n",
    "    Show real image and synthetic IFU-derived image side by side.\n",
    "    Works with either one or two IFU cubes needed for the filter.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Location handling\n",
    "    # ------------------------------------------------------------\n",
    "    if not isinstance(loc, SkyCoord):\n",
    "        loc_sky = SkyCoord(ra=loc[0] * u.deg, dec=loc[1] * u.deg, frame=\"icrs\")\n",
    "    else:\n",
    "        loc_sky = loc\n",
    "\n",
    "    def nearest_spaxel_map(cube_src, cube_target):\n",
    "        ny, nx = cube_target.shape[1:]\n",
    "        y_t, x_t = np.mgrid[:ny, :nx]\n",
    "\n",
    "        world = cube_target.wcs.celestial.pixel_to_world(x_t, y_t)\n",
    "        x_s, y_s = cube_src.wcs.celestial.world_to_pixel(world)\n",
    "\n",
    "        x_s = np.clip(np.round(x_s).astype(int), 0, cube_src.shape[2] - 1)\n",
    "        y_s = np.clip(np.round(y_s).astype(int), 0, cube_src.shape[1] - 1)\n",
    "\n",
    "        return y_s, x_s\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Locate real image\n",
    "    # ------------------------------------------------------------\n",
    "    real_image_file = [x for x in image_files if extract_filter_name(x) == filter][0]\n",
    "    \n",
    "    short_wl, long_wl = [x.value for x in get_filter_wl_range(filter)]\n",
    "\n",
    "    needed_ifus = []\n",
    "    for file in ifu_fileset:\n",
    "        wl = SpectralCube.read(file, hdu=\"SCI\").spectral_axis.to(u.m).value\n",
    "        if (wl[0] < short_wl) and (wl[-1] > long_wl):\n",
    "            needed_ifus = [file]\n",
    "            break\n",
    "        if (long_wl > wl[0]) and (short_wl < wl[-1]):\n",
    "            needed_ifus.append(file)\n",
    "        if len(needed_ifus) > 1:\n",
    "            break\n",
    "\n",
    "    cube1 = SpectralCube.read(needed_ifus[0], hdu=\"SCI\")\n",
    "    cube2 = SpectralCube.read(needed_ifus[1], hdu=\"SCI\") if len(needed_ifus) > 1 else None\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Base cube quantities\n",
    "    # ------------------------------------------------------------\n",
    "    wl1 = cube1.spectral_axis.to(u.m).value\n",
    "    d1 = cube1.unmasked_data[:].value\n",
    "\n",
    "    ny, nx = cube1.shape[1:]\n",
    "    n_pix = ny * nx\n",
    "    d1 = d1.reshape(len(wl1), n_pix).T\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Stitch spectra if needed\n",
    "    # ------------------------------------------------------------\n",
    "    if cube2 is not None:\n",
    "        wl2 = cube2.spectral_axis.to(u.m).value\n",
    "        d2 = cube2.unmasked_data[:].value\n",
    "\n",
    "        y2, x2 = nearest_spaxel_map(cube2, cube1)\n",
    "        d2 = d2[:, y2, x2].reshape(len(wl2), n_pix).T\n",
    "\n",
    "        wl_all = np.concatenate([wl1, wl2])\n",
    "        sort_idx = np.argsort(wl_all)\n",
    "        wl_all = wl_all[sort_idx]\n",
    "\n",
    "        spec_all = np.concatenate([d1, d2], axis=1)[:, sort_idx]\n",
    "\n",
    "        wl_min = max(wl1.min(), wl2.min())\n",
    "        wl_max = min(wl1.max(), wl2.max())\n",
    "        overlap = (wl_all >= wl_min) & (wl_all <= wl_max)\n",
    "        both = np.isin(wl_all, wl1) & np.isin(wl_all, wl2) & overlap\n",
    "        spec_all[:, both] *= 0.5\n",
    "    else:\n",
    "        wl_all = wl1\n",
    "        spec_all = d1\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Synthetic photometry\n",
    "    # ------------------------------------------------------------\n",
    "    filter_wl, filter_trans = get_filter_data(filter)\n",
    "\n",
    "    image = np.empty(n_pix)\n",
    "    for i in range(n_pix):\n",
    "        image[i] = get_Fnu_transmission(\n",
    "            spec_all[i], wl_all, filter_trans, filter_wl, warnings=True\n",
    "        )\n",
    "\n",
    "    synth_image = image.reshape(ny, nx)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Attach WCS to synthetic image\n",
    "    # ------------------------------------------------------------\n",
    "    synth_hdu = fits.PrimaryHDU(\n",
    "        synth_image,\n",
    "        header=cube1.wcs.celestial.to_header()\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Plot real vs synthetic\n",
    "    # ------------------------------------------------------------\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # -------------------------\n",
    "    # REAL IMAGE\n",
    "    # -------------------------\n",
    "    hdu = fits.open(real_image_file)[\"SCI\"]\n",
    "    real_pix_size = hdu.header['PIXAR_A2']**0.5\n",
    "    aperture_radius = radius.to(u.arcsec).value / real_pix_size\n",
    "    cutout_real = Cutout2D(\n",
    "        hdu.data,\n",
    "        position=loc_sky,\n",
    "        size=(radius * 3, radius * 3),\n",
    "        wcs=WCS(hdu.header)\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # SYNTHETIC IMAGE (WCS CUTOUT)\n",
    "    # -------------------------\n",
    "    cutout_synth = Cutout2D(\n",
    "        synth_hdu.data,\n",
    "        position=loc_sky,\n",
    "        size=(radius * 3, radius * 3),\n",
    "        wcs=WCS(synth_hdu.header)\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Shared normalization (1–99%)\n",
    "    # ------------------------------------------------------------\n",
    "    combined = np.concatenate([\n",
    "        cutout_real.data[np.isfinite(cutout_real.data)],\n",
    "        cutout_synth.data[np.isfinite(cutout_synth.data)]\n",
    "    ])\n",
    "\n",
    "    vmin = np.percentile(combined, color_min_max[0])\n",
    "    vmax = np.percentile(combined, color_min_max[1])\n",
    "\n",
    "    cmap = plt.get_cmap(\"viridis\").copy()\n",
    "    cmap.set_under(\"black\")\n",
    "    cmap.set_over(\"white\")\n",
    "    norm = colors.Normalize(vmin=vmin, vmax=vmax, clip=False)\n",
    "\n",
    "    pix_scale = np.abs(cutout_synth.wcs.wcs.cdelt[0]) * 3600\n",
    "    r_ap_pix = radius.to(u.arcsec).value / pix_scale\n",
    "\n",
    "    # -------------------------\n",
    "    # PLOT REAL\n",
    "    # -------------------------\n",
    "    im0 = axes[0].imshow(\n",
    "        cutout_real.data,\n",
    "        origin=\"lower\",\n",
    "        cmap=cmap,\n",
    "        norm=norm\n",
    "    )\n",
    "    axes[0].set_title(f\"{filter} – Real\")\n",
    "\n",
    "    cbar0 = plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    cbar0.set_label(\"Flux\", fontsize=14)\n",
    "    cbar0.ax.tick_params(labelsize=12)\n",
    "\n",
    "    x_r, y_r = cutout_real.wcs.world_to_pixel(loc_sky)\n",
    "    axes[0].add_patch(\n",
    "        Circle((x_r, y_r), aperture_radius, edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # PLOT SYNTHETIC\n",
    "    # -------------------------\n",
    "    im1 = axes[1].imshow(\n",
    "        cutout_synth.data,\n",
    "        origin=\"lower\",\n",
    "        cmap=cmap,\n",
    "        norm=norm\n",
    "    )\n",
    "    axes[1].set_title(f\"{filter} – Synthetic (IFU)\")\n",
    "\n",
    "    cbar1 = plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    cbar1.set_label(\"Flux\", fontsize=14)\n",
    "    cbar1.ax.tick_params(labelsize=12)\n",
    "\n",
    "    x_s, y_s = cutout_synth.wcs.world_to_pixel(loc_sky)\n",
    "    axes[1].add_patch(\n",
    "        Circle((x_s, y_s), r_ap_pix, edgecolor=\"red\", facecolor=\"none\", linewidth=2)\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Cleanup\n",
    "    # -------------------------\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = ['Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f115w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f140m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f150w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f164n.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f182m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f187n.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f200w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f210m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f212n.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f250m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f300m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f335m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f360m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f405n.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f430m.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_nircam_lv3_f444w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f560w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f770w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f1000w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f1130w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f1280w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f1500w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f1800w.fits',\n",
    "       'Data_files/Image_files/v0p3/ngc5194_miri_lv3_f2100w.fits']\n",
    "\n",
    "locations = [[202.5062429, 47.2143358], [202.4335225, 47.1729608], [202.4340450, 47.1732517], [202.4823742, 47.1958589]]\n",
    "radius = 1.25*u.arcsec\n",
    "full_raw_ifu_files_loc0 = ['Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch4-shortmediumlong_s3d_trimmed.fits']\n",
    "show_image_and_synth('F1500W', full_raw_ifu_files_loc0, locations[0], radius, image_files=image_files, color_min_max = [1, 99.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.linspace(0, 10, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "11 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
