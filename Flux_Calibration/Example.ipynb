{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/cluster/medbow/project/phangs/tjuchau/Cluster_backup') #TJ change working directory to be the parent directory\n",
    "from Py_files.Functions import *\n",
    "image_files, filter_files = collect_M51_image_and_filter_files(filter_directory, image_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "full_raw_ifu_files_loc0 = ['/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g395m-f290lp_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_0/Arm1_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_0/Arm1_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_0/Arm1_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_0/Arm1_Level3_ch4-shortmediumlong_s3d_trimmed.fits']\n",
    "full_raw_ifu_files_loc1 = ['/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g395m-f290lp_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_1/Arm2_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_1/Arm2_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_1/Arm2_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_1/Arm2_Level3_ch4-shortmediumlong_s3d_trimmed.fits']\n",
    "#TJ location 2 also within loc1 files\n",
    "full_raw_ifu_files_loc3 = ['/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g395m-f290lp_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_3/Arm3_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_3/Arm3_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_3/Arm3_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              '/project/phangs/tjuchau/Data_files/M51/Spectroscopy/raw_IFUs/location_3/Arm3_Level3_ch4-shortmediumlong_s3d_trimmed.fits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ip25 = create_data(full_raw_ifu_files_loc0, image_files, filter_files, locations[0], 1.25*u.arcsec)\n",
    "#plot_results(Ip25, correction = 'mult')\n",
    "#plot_results(Ip25, correction = 'add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_M51_image_and_filter_files(filter_directory, image_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_sizes = np.linspace(1, 5, 11)\n",
    "mean_add_correction_factors = []\n",
    "mean_mult_correction_factors = []\n",
    "for psf_size in psf_sizes:\n",
    "    if psf_size == 0:\n",
    "        psf_size = 1e-6\n",
    "    print('***********************************************************************************************************************')\n",
    "    print(f'now using psf size {psf_size}')\n",
    "    print('***********************************************************************************************************************')\n",
    "    psf = generate_psf(psf_type=\"gaussian\", fwhm_pix=psf_size, size=61)\n",
    "    new_ifus = []\n",
    "    for i,file in enumerate(full_raw_ifu_files_loc0):\n",
    "        new_file = convolve_ifu_cube(ifu_fits_path=file, psf=psf, output_path=f\"/project/phangs/tjuchau/Data_files/misc_data/temp_outputs/testing_convolutions{i}.fits\")\n",
    "        new_ifus.append(new_file)\n",
    "    data = compare_photometry(new_ifus, image_files, filter_files, locations[0], 1.25*u.arcsec)\n",
    "    mean_add_correction_factors.append(data['add_correction_values'])\n",
    "    mean_mult_correction_factors.append(data['mult_correction_values'])\n",
    "    plot_results(data, correction = 'mult', show_images=['F560W', 'F2100W'])\n",
    "    plot_results(data, correction = 'add', show_images=['F560W', 'F2100W'])\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_sizes = np.linspace(1, 5, 11)\n",
    "mean_add_correction_factors = []\n",
    "mean_mult_correction_factors = []\n",
    "for psf_size in psf_sizes:\n",
    "    if psf_size == 0:\n",
    "        psf_size = 1e-6\n",
    "    print('***********************************************************************************************************************')\n",
    "    print(f'now using psf size {psf_size}')\n",
    "    print('***********************************************************************************************************************')\n",
    "    psf = generate_psf(psf_type=\"gaussian\", fwhm_pix=psf_size, size=61)\n",
    "    new_ifus = []\n",
    "    for i,file in enumerate(full_raw_ifu_files_loc0):\n",
    "        new_file = convolve_ifu_cube(ifu_fits_path=file, psf=psf, output_path=f\"Data_files/misc_data/temp_outputs/testing_convolutions{i}.fits\")\n",
    "        new_ifus.append(new_file)\n",
    "    data = create_data(new_ifus, image_files, filter_files, locations[0], 1*u.arcsec)\n",
    "    mean_add_correction_factors.append(data['add_correction_values'])\n",
    "    mean_mult_correction_factors.append(data['mult_correction_values'])\n",
    "    plot_results(data, correction = 'mult', show_images=['F560W', 'F1500W'])\n",
    "    plot_results(data, correction = 'add', show_images=['F560W', 'F1500W'])\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_sizes = np.linspace(1, 5, 11)\n",
    "mean_add_correction_factors = []\n",
    "mean_mult_correction_factors = []\n",
    "for psf_size in psf_sizes:\n",
    "    if psf_size == 0:\n",
    "        psf_size = 1e-6\n",
    "    print('***********************************************************************************************************************')\n",
    "    print(f'now using psf size {psf_size}')\n",
    "    print('***********************************************************************************************************************')\n",
    "    psf = generate_psf(psf_type=\"gaussian\", fwhm_pix=psf_size, size=61)\n",
    "    new_ifus = []\n",
    "    for i,file in enumerate(full_raw_ifu_files_loc0):\n",
    "        new_file = convolve_ifu_cube(ifu_fits_path=file, psf=psf, output_path=f\"Data_files/misc_data/test_data/testing_convolutions{i}.fits\")\n",
    "        new_ifus.append(new_file)\n",
    "    data = create_data(new_ifus, image_files, filter_files, locations[0], 0.75*u.arcsec)\n",
    "    mean_add_correction_factors.append(data['add_correction_values'])\n",
    "    mean_mult_correction_factors.append(data['mult_correction_values'])\n",
    "    plot_results(data, correction = 'mult', show_images=['F560W', 'F1500W'])\n",
    "    plot_results(data, correction = 'add', show_images=['F560W', 'F1500W'])\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_sizes = np.linspace(1, 5, 11)\n",
    "mean_add_correction_factors = []\n",
    "mean_mult_correction_factors = []\n",
    "for psf_size in psf_sizes:\n",
    "    if psf_size == 0:\n",
    "        psf_size = 1e-6\n",
    "    print('***********************************************************************************************************************')\n",
    "    print(f'now using psf size {psf_size}')\n",
    "    print('***********************************************************************************************************************')\n",
    "    psf = generate_psf(psf_type=\"gaussian\", fwhm_pix=psf_size, size=61)\n",
    "    new_ifus = []\n",
    "    for i,file in enumerate(full_raw_ifu_files_loc0):\n",
    "        new_file = convolve_ifu_cube(ifu_fits_path=file, psf=psf, output_path=f\"Data_files/misc_data/test_data/testing_convolutions{i}.fits\")\n",
    "        new_ifus.append(new_file)\n",
    "    data = create_data(new_ifus, image_files, filter_files, locations[0], 0.5*u.arcsec)\n",
    "    mean_add_correction_factors.append(data['add_correction_values'])\n",
    "    mean_mult_correction_factors.append(data['mult_correction_values'])\n",
    "    plot_results(data, correction = 'mult', show_images=['F560W', 'F1500W'])\n",
    "    plot_results(data, correction = 'add', show_images=['F560W', 'F1500W'])\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "#####################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_sizes = np.linspace(1, 5, 11)\n",
    "mean_add_correction_factors = []\n",
    "mean_mult_correction_factors = []\n",
    "for psf_size in psf_sizes:\n",
    "    if psf_size == 0:\n",
    "        psf_size = 1e-6\n",
    "    print('***********************************************************************************************************************')\n",
    "    print(f'now using psf size {psf_size}')\n",
    "    print('***********************************************************************************************************************')\n",
    "    psf = generate_psf(psf_type=\"gaussian\", fwhm_pix=psf_size, size=61)\n",
    "    new_ifus = []\n",
    "    for i,file in enumerate(full_raw_ifu_files_loc1):\n",
    "        new_file = convolve_ifu_cube(ifu_fits_path=file, psf=psf, output_path=f\"Data_files/misc_data/test_data/testing_convolutions{i}.fits\")\n",
    "        new_ifus.append(new_file)\n",
    "    data = create_data(new_ifus, image_files, filter_files, locations[1], 1*u.arcsec)\n",
    "    mean_add_correction_factors.append(data['add_correction_values'])\n",
    "    mean_mult_correction_factors.append(data['mult_correction_values'])\n",
    "    plot_results(data, correction = 'mult', show_images=['F560W', 'F1500W'])\n",
    "    plot_results(data, correction = 'add', show_images=['F560W', 'F1500W'])\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convolve_fits_with_gaussian_psf(\n",
    "    input_fits_path,\n",
    "    fwhm_arcsec,\n",
    "    output_fits_path,\n",
    "    location=None,\n",
    "    radius_arcsec=None,\n",
    "    psf_size_factor=6,\n",
    "    boundary=\"fill\",\n",
    "    overwrite=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Convolve a FITS image or cube with a Gaussian PSF.\n",
    "    For 2D images, optionally crop a region around `location` to reduce memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_fits_path : str\n",
    "        Path to input FITS (2D image or 3D cube).\n",
    "    fwhm_arcsec : float\n",
    "        PSF FWHM in arcseconds.\n",
    "    output_fits_path : str\n",
    "        Output FITS path.\n",
    "    location : SkyCoord or list/tuple [RA_deg, Dec_deg], optional\n",
    "        Location to crop around for 2D images. If None, use full image.\n",
    "    radius_arcsec : float, optional\n",
    "        Minimum radius around location to include (in arcseconds). PSF must fit inside.\n",
    "    psf_size_factor : float\n",
    "        Kernel size = psf_size_factor * FWHM (in pixels)\n",
    "    boundary : str\n",
    "        FFT boundary handling.\n",
    "    overwrite : bool\n",
    "        Overwrite output file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_fits_path : str\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Load FITS\n",
    "    # ----------------------------\n",
    "    with fits.open(input_fits_path) as hdul:\n",
    "        data = hdul['SCI'].data.astype(float)\n",
    "        header = hdul['SCI'].header\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"FITS contains no data.\")\n",
    "\n",
    "    wcs = WCS(header)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert location to pixel coordinates\n",
    "    # ----------------------------\n",
    "    if type(location) == list:\n",
    "        spatial_coords = SkyCoord(ra=location[0]*u.deg, dec=location[1]*u.deg)\n",
    "    elif type(location) == SkyCoord:\n",
    "        spatial_coords = location\n",
    "    else:\n",
    "        print('loc is not a list of ra, dec and it is not a SkyCoord object.')\n",
    "        return None\n",
    "    \n",
    "    # Convert spatial coordinates to pixels\n",
    "    x0, y0 = wcs.celestial.all_world2pix(spatial_coords.ra.deg, \n",
    "                                      spatial_coords.dec.deg, 0)\n",
    "    x0 = float(x0)\n",
    "    y0 = float(y0)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pixel scale (arcsec/pix)\n",
    "    # ----------------------------\n",
    "    pixscale_arcsec = abs(header['CDELT1']) * 3600.0\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert FWHM to sigma (pixels)\n",
    "    # ----------------------------\n",
    "    fwhm_pix = fwhm_arcsec / pixscale_arcsec\n",
    "    sigma_pix = fwhm_pix / (2.0 * np.sqrt(2.0 * np.log(2.0)))\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build PSF kernel\n",
    "    # ----------------------------\n",
    "    psf_size_pix = int(np.ceil(psf_size_factor * fwhm_pix))\n",
    "    if psf_size_pix % 2 == 0:\n",
    "        psf_size_pix += 1\n",
    "\n",
    "    ax = np.arange(psf_size_pix) - psf_size_pix // 2\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    psf = np.exp(-(xx**2 + yy**2) / (2 * sigma_pix**2))\n",
    "    psf /= psf.sum()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Handle NaNs\n",
    "    # ----------------------------\n",
    "    nanmask = np.isnan(data)\n",
    "    data0 = data.copy()\n",
    "    data0[nanmask] = 0.0\n",
    "    weight = np.ones_like(data0)\n",
    "    weight[nanmask] = 0.0\n",
    "\n",
    "    # ----------------------------\n",
    "    # Crop region for 2D images if requested\n",
    "    # ----------------------------\n",
    "    if data.ndim == 2 and location is not None and radius_arcsec is not None:\n",
    "        radius_pix = max(radius_arcsec / pixscale_arcsec, 1.0)  # minimum 1 px\n",
    "        cut_radius = int(np.ceil(radius_pix + 6 * sigma_pix))\n",
    "\n",
    "        x_min = max(0, int(x0 - cut_radius))\n",
    "        x_max = min(data.shape[1], int(x0 + cut_radius + 1))\n",
    "        y_min = max(0, int(y0 - cut_radius))\n",
    "        y_max = min(data.shape[0], int(y0 + cut_radius + 1))\n",
    "\n",
    "        data0_cut = data0[y_min:y_max, x_min:x_max]\n",
    "        weight_cut = weight[y_min:y_max, x_min:x_max]\n",
    "        nanmask_cut = nanmask[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        def convolve_img(img, wgt):\n",
    "            conv_img = convolve_fft(\n",
    "                img,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=False\n",
    "            )\n",
    "            conv_wgt = convolve_fft(\n",
    "                wgt,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=False\n",
    "            )\n",
    "            out = np.zeros_like(conv_img)\n",
    "            good = conv_wgt > 1e-8\n",
    "            out[good] = conv_img[good] / conv_wgt[good]\n",
    "            out[~good] = np.nan\n",
    "            return out\n",
    "\n",
    "        conv_cut = convolve_img(data0_cut, weight_cut)\n",
    "        conv_cut[nanmask_cut] = np.nan\n",
    "        conv = data.copy()\n",
    "        conv[y_min:y_max, x_min:x_max] = conv_cut\n",
    "\n",
    "    # ----------------------------\n",
    "    # Otherwise, use full data (2D) or 3D cubes\n",
    "    # ----------------------------\n",
    "    elif data.ndim == 2:\n",
    "        def convolve_img(img, wgt):\n",
    "            conv_img = convolve_fft(\n",
    "                img,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            conv_wgt = convolve_fft(\n",
    "                wgt,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            out = np.zeros_like(conv_img)\n",
    "            good = conv_wgt > 1e-8\n",
    "            out[good] = conv_img[good] / conv_wgt[good]\n",
    "            out[~good] = np.nan\n",
    "            return out\n",
    "\n",
    "        conv = convolve_img(data0, weight)\n",
    "        conv[nanmask] = np.nan\n",
    "\n",
    "    elif data.ndim == 3:\n",
    "        conv = np.zeros_like(data0)\n",
    "        for i in range(data.shape[0]):\n",
    "            conv_i = convolve_fft(\n",
    "                data0[i],\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            conv_wgt = convolve_fft(\n",
    "                weight[i],\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            good = conv_wgt > 1e-8\n",
    "            out_i = np.zeros_like(conv_i)\n",
    "            out_i[good] = conv_i[good] / conv_wgt[good]\n",
    "            out_i[~good] = np.nan\n",
    "            out_i[nanmask[i]] = np.nan\n",
    "            conv[i] = out_i\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data dimension: {data.ndim}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Write output\n",
    "    # ----------------------------\n",
    "    hdu = fits.PrimaryHDU(conv, header=header)\n",
    "    hdu.writeto(output_fits_path, overwrite=overwrite)\n",
    "\n",
    "    return output_fits_path\n",
    "\n",
    "\n",
    "    \n",
    "psf_sizes = np.linspace(0.1, 1, 4)\n",
    "mean_add_correction_factors = []\n",
    "mean_mult_correction_factors = []\n",
    "for psf_size in psf_sizes:\n",
    "    if psf_size == 0:\n",
    "        psf_size = 1e-6\n",
    "    print('***********************************************************************************************************************')\n",
    "    print(f'now using psf size {psf_size}')\n",
    "    print('***********************************************************************************************************************')\n",
    "    new_ifus = []\n",
    "\n",
    "    for i,file in enumerate(full_raw_ifu_files_loc0):\n",
    "        new_file = convolve_fits_with_gaussian_psf(file, psf_size,\n",
    "                                                   f\"Data_files/misc_data/test_data/testing_convolutions{i}.fits\", \n",
    "                                                   location = locations[0],\n",
    "                                                  radius_arcsec = 1.25)\n",
    "        new_ifus.append(new_file)\n",
    "    data = create_data(new_ifus, image_files, filter_files, locations[0], 1.25*u.arcsec)\n",
    "    mean_add_correction_factors.append(data['add_correction_values'])\n",
    "    mean_mult_correction_factors.append(data['mult_correction_values'])\n",
    "    plot_results(data, correction = 'mult', show_images=['F560W', 'F2100W'])\n",
    "    plot_results(data, correction = 'add', show_images=['F560W', 'F2100W'])\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve_fft\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "def convolve_fits_with_gaussian_psf(\n",
    "    input_fits_path,\n",
    "    fwhm_arcsec,\n",
    "    output_fits_path,\n",
    "    location=None,\n",
    "    radius_arcsec=None,\n",
    "    psf_size_factor=6,\n",
    "    boundary=\"fill\",\n",
    "    overwrite=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Convolve a FITS image or cube with a Gaussian PSF.\n",
    "    For 2D images, optionally crop a region around `location` to reduce memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_fits_path : str\n",
    "        Path to input FITS (2D image or 3D cube).\n",
    "    fwhm_arcsec : float\n",
    "        PSF FWHM in arcseconds.\n",
    "    output_fits_path : str\n",
    "        Output FITS path.\n",
    "    location : SkyCoord or list/tuple [RA_deg, Dec_deg], optional\n",
    "        Location to crop around for 2D images. If None, use full image.\n",
    "    radius_arcsec : float, optional\n",
    "        Minimum radius around location to include (in arcseconds). PSF must fit inside.\n",
    "    psf_size_factor : float\n",
    "        Kernel size = psf_size_factor * FWHM (in pixels)\n",
    "    boundary : str\n",
    "        FFT boundary handling.\n",
    "    overwrite : bool\n",
    "        Overwrite output file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_fits_path : str\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Load FITS\n",
    "    # ----------------------------\n",
    "    with fits.open(input_fits_path) as hdul:\n",
    "        data = hdul['SCI'].data.astype(float)\n",
    "        header = hdul['SCI'].header\n",
    "\n",
    "    if data is None:\n",
    "        raise ValueError(\"FITS contains no data.\")\n",
    "\n",
    "    wcs = WCS(header)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert location to pixel coordinates\n",
    "    # ----------------------------\n",
    "    if type(location) == list:\n",
    "        spatial_coords = SkyCoord(ra=location[0]*u.deg, dec=location[1]*u.deg)\n",
    "    elif type(location) == SkyCoord:\n",
    "        spatial_coords = location\n",
    "    else:\n",
    "        print('loc is not a list of ra, dec and it is not a SkyCoord object.')\n",
    "        return None\n",
    "    \n",
    "    # Convert spatial coordinates to pixels\n",
    "    x0, y0 = wcs.celestial.all_world2pix(spatial_coords.ra.deg, \n",
    "                                      spatial_coords.dec.deg, 0)\n",
    "    x0 = float(x0)\n",
    "    y0 = float(y0)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Pixel scale (arcsec/pix)\n",
    "    # ----------------------------\n",
    "    pixscale_arcsec = abs(header['CDELT1']) * 3600.0\n",
    "\n",
    "    # ----------------------------\n",
    "    # Convert FWHM to sigma (pixels)\n",
    "    # ----------------------------\n",
    "    fwhm_pix = fwhm_arcsec / pixscale_arcsec\n",
    "    sigma_pix = fwhm_pix / (2.0 * np.sqrt(2.0 * np.log(2.0)))\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build PSF kernel\n",
    "    # ----------------------------\n",
    "    psf_size_pix = int(np.ceil(psf_size_factor * fwhm_pix))\n",
    "    if psf_size_pix % 2 == 0:\n",
    "        psf_size_pix += 1\n",
    "\n",
    "    ax = np.arange(psf_size_pix) - psf_size_pix // 2\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    psf = np.exp(-(xx**2 + yy**2) / (2 * sigma_pix**2))\n",
    "    psf /= psf.sum()\n",
    "\n",
    "    # ----------------------------\n",
    "    # Handle NaNs\n",
    "    # ----------------------------\n",
    "    nanmask = np.isnan(data)\n",
    "    data0 = data.copy()\n",
    "    data0[nanmask] = 0.0\n",
    "    weight = np.ones_like(data0)\n",
    "    weight[nanmask] = 0.0\n",
    "\n",
    "    # ----------------------------\n",
    "    # Crop region for 2D images if requested\n",
    "    # ----------------------------\n",
    "    if data.ndim == 2 and location is not None and radius_arcsec is not None:\n",
    "        radius_pix = max(radius_arcsec / pixscale_arcsec, 1.0)  # minimum 1 px\n",
    "        cut_radius = int(np.ceil(radius_pix + 6 * sigma_pix))\n",
    "\n",
    "        x_min = max(0, int(x0 - cut_radius))\n",
    "        x_max = min(data.shape[1], int(x0 + cut_radius + 1))\n",
    "        y_min = max(0, int(y0 - cut_radius))\n",
    "        y_max = min(data.shape[0], int(y0 + cut_radius + 1))\n",
    "\n",
    "        data0_cut = data0[y_min:y_max, x_min:x_max]\n",
    "        weight_cut = weight[y_min:y_max, x_min:x_max]\n",
    "        nanmask_cut = nanmask[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        def convolve_img(img, wgt):\n",
    "            conv_img = convolve_fft(\n",
    "                img,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=False\n",
    "            )\n",
    "            conv_wgt = convolve_fft(\n",
    "                wgt,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=False\n",
    "            )\n",
    "            out = np.zeros_like(conv_img)\n",
    "            good = conv_wgt > 1e-8\n",
    "            out[good] = conv_img[good] / conv_wgt[good]\n",
    "            out[~good] = np.nan\n",
    "            return out\n",
    "\n",
    "        conv_cut = convolve_img(data0_cut, weight_cut)\n",
    "        conv_cut[nanmask_cut] = np.nan\n",
    "        conv = data.copy()\n",
    "        conv[y_min:y_max, x_min:x_max] = conv_cut\n",
    "\n",
    "    # ----------------------------\n",
    "    # Otherwise, use full data (2D) or 3D cubes\n",
    "    # ----------------------------\n",
    "    elif data.ndim == 2:\n",
    "        def convolve_img(img, wgt):\n",
    "            conv_img = convolve_fft(\n",
    "                img,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            conv_wgt = convolve_fft(\n",
    "                wgt,\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            out = np.zeros_like(conv_img)\n",
    "            good = conv_wgt > 1e-8\n",
    "            out[good] = conv_img[good] / conv_wgt[good]\n",
    "            out[~good] = np.nan\n",
    "            return out\n",
    "\n",
    "        conv = convolve_img(data0, weight)\n",
    "        conv[nanmask] = np.nan\n",
    "\n",
    "    elif data.ndim == 3:\n",
    "        conv = np.zeros_like(data0)\n",
    "        for i in range(data.shape[0]):\n",
    "            conv_i = convolve_fft(\n",
    "                data0[i],\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            conv_wgt = convolve_fft(\n",
    "                weight[i],\n",
    "                psf,\n",
    "                boundary=boundary,\n",
    "                nan_treatment=\"fill\",\n",
    "                fill_value=0.0,\n",
    "                normalize_kernel=False,\n",
    "                preserve_nan=False,\n",
    "                allow_huge=True\n",
    "            )\n",
    "            good = conv_wgt > 1e-8\n",
    "            out_i = np.zeros_like(conv_i)\n",
    "            out_i[good] = conv_i[good] / conv_wgt[good]\n",
    "            out_i[~good] = np.nan\n",
    "            out_i[nanmask[i]] = np.nan\n",
    "            conv[i] = out_i\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data dimension: {data.ndim}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Write output\n",
    "    # ----------------------------\n",
    "    hdu = fits.PrimaryHDU(conv, header=header)\n",
    "    hdu.writeto(output_fits_path, overwrite=overwrite)\n",
    "\n",
    "    return output_fits_path\n",
    "\n",
    "\n",
    "    \n",
    "psf_sizes = np.linspace(0.1, 1.75, 20)\n",
    "mean_add_correction_factors = []\n",
    "mean_mult_correction_factors = []\n",
    "for psf_size in psf_sizes:\n",
    "    if psf_size == 0:\n",
    "        psf_size = 1e-6\n",
    "    print('***********************************************************************************************************************')\n",
    "    print(f'now using psf size {psf_size}')\n",
    "    print('***********************************************************************************************************************')\n",
    "    new_images = []\n",
    "    for i,file in enumerate(image_files):\n",
    "        new_file = convolve_fits_with_gaussian_psf(file, psf_size, f\"Data_files/misc_data/test_data/testing_convolutions{i}_{extract_filter_name(file)}.fits\",\n",
    "                                                   location = locations[0], radius_arcsec = 1.25)\n",
    "        new_images.append(new_file)\n",
    "    data = create_data(full_raw_ifu_files_loc0, new_images, filter_files, locations[0], 1.25*u.arcsec)\n",
    "    mean_add_correction_factors.append(data['add_correction_values'])\n",
    "    mean_mult_correction_factors.append(data['mult_correction_values'])\n",
    "    plot_results(data, correction = 'mult', show_images=['F560W', 'F2100W'])\n",
    "    plot_results(data, correction = 'add', show_images=['F560W', 'F2100W'])\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/d/ret1/Taylor/jupyter_notebooks/Research') #TJ change working directory to be the parent directory\n",
    "from Py_files.Functions import *\n",
    "image_files, filter_files = generate_list_of_files(filter_directory, image_directory)\n",
    "\n",
    "karin_SDuval_IFU_files = ['Data_files/IFU_files/raw_IFUs/jw03435-o012_t014_nirspec_g140m-f100lp_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/jw03435-o012_t014_nirspec_g235m-f170lp_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/jw03435-o012_t014_nirspec_g395m-f290lp_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/SW_IFU_ch1-shortmediumlong_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/SW_IFU_ch2-shortmediumlong_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/SW_IFU_ch3-shortmediumlong_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/SW_IFU_ch4-shortmediumlong_s3d.fits'\n",
    "            ]\n",
    "\n",
    "karin_IFU_files = [ 'Data_files/IFU_files/raw_IFUs/jw03435-o012_t014_nirspec_g140m-f100lp_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/jw03435-o012_t014_nirspec_g235m-f170lp_s3d.fits',\n",
    "             'Data_files/IFU_files/raw_IFUs/jw03435-o012_t014_nirspec_g395m-f290lp_s3d.fits',\n",
    "             'Data_files/IFU_files/Arm2_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "             'Data_files/IFU_files/Arm2_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "             'Data_files/IFU_files/Arm2_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "             'Data_files/IFU_files/Arm2_Level3_ch4-shortmediumlong_s3d.fits',\n",
    "            ]\n",
    "Thomas_IFU_file = 'Data_files/IFU_files/M51_SW_f290lp_g395m-f290lp_s3d.fits'\n",
    "\n",
    "SDuval_IFU_files = ['Data_files/IFU_files/raw_IFUs/SW_IFU_ch1-shortmediumlong_s3d.fits',\n",
    "                  'Data_files/IFU_files/raw_IFUs/SW_IFU_ch2-shortmediumlong_s3d.fits',\n",
    "                  'Data_files/IFU_files/raw_IFUs/SW_IFU_ch3-shortmediumlong_s3d.fits',\n",
    "                  'Data_files/IFU_files/raw_IFUs/SW_IFU_ch4-shortmediumlong_s3d.fits']\n",
    "\n",
    "Grant_conv_IFU_files = ['Data_files/IFU_files/jw03435-o012_t014_nirspec_g140m-f100lp_s3d_conv17p1.fits',\n",
    "                        'Data_files/IFU_files/jw03435-o012_t014_nirspec_g235m-f170lp_s3d_conv17p1.fits',\n",
    "                        'Data_files/IFU_files/jw03435-o012_t014_nirspec_g395m-f290lp_s3d_conv17p1.fits',\n",
    "                        'Data_files/IFU_files/SW_IFU_ch1-shortmediumlong_s3d_conv17p1um.fits',\n",
    "                        'Data_files/IFU_files/SW_IFU_ch2-shortmediumlong_s3d_conv17p1um.fits',\n",
    "                        'Data_files/IFU_files/SW_IFU_ch3-shortmediumlong_s3d_conv17p1um.fits',\n",
    "                        'Data_files/IFU_files/SW_IFU_ch4-shortmediumlong_s3d_conv17p1um.fits']\n",
    "\n",
    "full_raw_ifu_files_loc0 = ['Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/jw03435-o004_t005_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_0/Arm1_Level3_ch4-shortmediumlong_s3d_trimmed.fits']\n",
    "full_raw_ifu_files_loc1 = ['Data_files/IFU_files/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/jw03435-o012_t014_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_1/Arm2_Level3_ch4-shortmediumlong_s3d_trimmed.fits']\n",
    "#TJ location 2 also within loc1 files\n",
    "full_raw_ifu_files_loc3 = ['Data_files/IFU_files/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g140m-f100lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g235m-f170lp_s3d_trimmed.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/jw03435-o006_t010_nirspec_g395m-f290lp_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch1-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch2-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch3-shortmediumlong_s3d.fits',\n",
    "              'Data_files/IFU_files/raw_IFUs/location_3/Arm3_Level3_ch4-shortmediumlong_s3d_trimmed.fits']\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve_fft\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve_fft\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "def native_psf_FWHM(wl):\n",
    "    return (1.22*wl/6.5)*360*3600/(2*np.pi)\n",
    "\n",
    "def psf_homogenize_jwst_fits(\n",
    "    input_fits_path,\n",
    "    fwhm_target_arcsec,\n",
    "    output_fits_path,\n",
    "    native_wavelength=None,    # only for 2D images\n",
    "    location=None,               # SkyCoord or [ra,dec] deg, only for 2D\n",
    "    radius_arcsec=1.0,           # only for 2D\n",
    "    psf_size_factor=6,\n",
    "    boundary=\"fill\",\n",
    "    overwrite=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    PSF-homogenize JWST image or IFU cube to a common Gaussian PSF using\n",
    "    diffraction-limited wavelength-dependent PSF estimation.\n",
    "\n",
    "    For 3D cube:\n",
    "        - Wavelength is read from WCS\n",
    "        - Native PSF computed per slice\n",
    "\n",
    "    For 2D image:\n",
    "        - You must supply native_wavelength_m\n",
    "        - You must supply location + radius_arcsec (for cutout convolution)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # Load FITS\n",
    "    # ----------------------------\n",
    "    with fits.open(input_fits_path) as hdul:\n",
    "        data = hdul['SCI'].data.astype(float)\n",
    "        header = hdul['SCI'].header\n",
    "\n",
    "    wcs = WCS(header)\n",
    "    pixscale_arcsec = abs(header['CDELT1']) * 3600.0\n",
    "\n",
    "    # ----------------------------\n",
    "    # Masked convolution helper\n",
    "    # ----------------------------\n",
    "    def masked_convolve(img, psf):\n",
    "        nanmask = np.isnan(img)\n",
    "        img0 = img.copy()\n",
    "        img0[nanmask] = 0.0\n",
    "\n",
    "        wgt = np.ones_like(img0)\n",
    "        wgt[nanmask] = 0.0\n",
    "\n",
    "        conv_img = convolve_fft(\n",
    "            img0, psf,\n",
    "            boundary=boundary,\n",
    "            nan_treatment=\"fill\",\n",
    "            fill_value=0.0,\n",
    "            normalize_kernel=False,\n",
    "            preserve_nan=False,\n",
    "            allow_huge=False,\n",
    "        )\n",
    "\n",
    "        conv_wgt = convolve_fft(\n",
    "            wgt, psf,\n",
    "            boundary=boundary,\n",
    "            nan_treatment=\"fill\",\n",
    "            fill_value=0.0,\n",
    "            normalize_kernel=False,\n",
    "            preserve_nan=False,\n",
    "            allow_huge=False,\n",
    "        )\n",
    "\n",
    "        out = np.full_like(conv_img, np.nan)\n",
    "        good = conv_wgt > 1e-8\n",
    "        out[good] = conv_img[good] / conv_wgt[good]\n",
    "        return out\n",
    "\n",
    "    # ----------------------------\n",
    "    # Build Gaussian kernel\n",
    "    # ----------------------------\n",
    "    def make_kernel_from_fwhm(fwhm_arcsec):\n",
    "        if fwhm_arcsec <= 0:\n",
    "            return None\n",
    "\n",
    "        fwhm_pix = fwhm_arcsec / pixscale_arcsec\n",
    "        sigma_pix = fwhm_pix / (2.0 * np.sqrt(2.0 * np.log(2.0)))\n",
    "\n",
    "        size = int(np.ceil(psf_size_factor * fwhm_pix))\n",
    "        if size % 2 == 0:\n",
    "            size += 1\n",
    "\n",
    "        ax = np.arange(size) - size // 2\n",
    "        xx, yy = np.meshgrid(ax, ax)\n",
    "        psf = np.exp(-(xx**2 + yy**2) / (2 * sigma_pix**2))\n",
    "        psf /= psf.sum()\n",
    "        return psf, sigma_pix\n",
    "\n",
    "    # ============================\n",
    "    # ========== 2D IMAGE =======\n",
    "    # ============================\n",
    "    if data.ndim == 2:\n",
    "        if native_wavelength is None:\n",
    "            raise ValueError(\"For 2D images you must supply native_wavelength_m.\")\n",
    "        if native_wavelength.unit != u.m:\n",
    "            native_wavelength.to(u.m)\n",
    "        if location is None:\n",
    "            raise ValueError(\"For 2D images you must supply location.\")\n",
    "        if radius_arcsec is None:\n",
    "            raise ValueError(\"For 2D images you must supply radius_arcsec.\")\n",
    "        native_wavelength_m = native_wavelength.value\n",
    "        # Convert location to pixel\n",
    "        if isinstance(location, SkyCoord):\n",
    "            x0, y0 = wcs.world_to_pixel(location)\n",
    "        else:\n",
    "            ra, dec = location\n",
    "            x0, y0 = wcs.world_to_pixel(SkyCoord(ra=ra*u.deg, dec=dec*u.deg))\n",
    "        x0 = float(x0)\n",
    "        y0 = float(y0)\n",
    "\n",
    "        # Native PSF\n",
    "        fwhm_native = native_psf_FWHM(native_wavelength_m)\n",
    "\n",
    "        if fwhm_native >= fwhm_target_arcsec:\n",
    "            print(\"Native PSF is already worse than target PSF.\")\n",
    "            return input_fits_path\n",
    "\n",
    "        fwhm_kernel = np.sqrt(fwhm_target_arcsec**2 - fwhm_native**2)\n",
    "        psf, sigma_pix = make_kernel_from_fwhm(fwhm_kernel)\n",
    "\n",
    "        # Compute cutout size\n",
    "        radius_pix = radius_arcsec / pixscale_arcsec\n",
    "        cut_radius = int(np.ceil(radius_pix + 6 * sigma_pix))\n",
    "\n",
    "        x_min = max(0, int(x0 - cut_radius))\n",
    "        x_max = min(data.shape[1], int(x0 + cut_radius + 1))\n",
    "        y_min = max(0, int(y0 - cut_radius))\n",
    "        y_max = min(data.shape[0], int(y0 + cut_radius + 1))\n",
    "\n",
    "        sub = data[y_min:y_max, x_min:x_max]\n",
    "        conv_sub = masked_convolve(sub, psf)\n",
    "\n",
    "        out = data.copy()\n",
    "        out[y_min:y_max, x_min:x_max] = conv_sub\n",
    "\n",
    "    # ============================\n",
    "    # ========== 3D CUBE ========\n",
    "    # ============================\n",
    "    elif data.ndim == 3:\n",
    "        # Get wavelength axis from WCS\n",
    "        # Assume axis 0 is spectral\n",
    "        spec_pix = np.arange(data.shape[0])\n",
    "        world = wcs.pixel_to_world_values(spec_pix, 0, 0)\n",
    "        wavelengths_m = SpectralCube.read(input_fits_path, hdu = 'SCI').spectral_axis.to(u.m).value\n",
    "\n",
    "        out = np.zeros_like(data)\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "            lam = wavelengths_m[i]\n",
    "            fwhm_native = native_psf_FWHM(lam)\n",
    "            if fwhm_native >= fwhm_target_arcsec:\n",
    "                out[i] = data[i]\n",
    "                continue\n",
    "\n",
    "            fwhm_kernel = np.sqrt(np.abs(fwhm_target_arcsec**2 - fwhm_native**2))\n",
    "            psf, _ = make_kernel_from_fwhm(fwhm_kernel)\n",
    "\n",
    "            out[i] = masked_convolve(data[i], psf)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Only 2D images or 3D cubes supported.\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # Write output\n",
    "    # ----------------------------\n",
    "    fits.PrimaryHDU(out, header=header).writeto(output_fits_path, overwrite=overwrite)\n",
    "    return output_fits_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "psf_sizes = [native_psf_FWHM(13e-6), native_psf_FWHM(15e-6), native_psf_FWHM(17.04e-6), native_psf_FWHM(19e-6), native_psf_FWHM(21e-6)]\n",
    "'''\n",
    "add_correction_factors_cube1 = np.array([])\n",
    "add_correction_factors_cube2 = np.array([])\n",
    "add_correction_factors_cube3 = np.array([])\n",
    "add_correction_factors_cube4 = np.array([])\n",
    "add_correction_factors_cube5 = np.array([])\n",
    "add_correction_factors_cube6 = np.array([])\n",
    "add_correction_factors_cube7 = np.array([])\n",
    "\n",
    "mult_correction_factors_cube1 = np.array([])\n",
    "mult_correction_factors_cube2 = np.array([])\n",
    "mult_correction_factors_cube3 = np.array([])\n",
    "mult_correction_factors_cube4 = np.array([])\n",
    "mult_correction_factors_cube5 = np.array([])\n",
    "mult_correction_factors_cube6 = np.array([])\n",
    "mult_correction_factors_cube7 = np.array([])\n",
    "'''\n",
    "loc_idx = 0\n",
    "\n",
    "add_correction_factors_cube1 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube1_factors.npy')\n",
    "add_correction_factors_cube2 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube2_factors.npy')\n",
    "add_correction_factors_cube3 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube3_factors.npy')\n",
    "add_correction_factors_cube4 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube4_factors.npy')\n",
    "add_correction_factors_cube5 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube5_factors.npy')\n",
    "add_correction_factors_cube6 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube6_factors.npy')\n",
    "add_correction_factors_cube7 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube7_factors.npy')\n",
    "\n",
    "mult_correction_factors_cube1 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube1_factors.npy')\n",
    "mult_correction_factors_cube2 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube2_factors.npy')\n",
    "mult_correction_factors_cube3 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube3_factors.npy')\n",
    "mult_correction_factors_cube4 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube4_factors.npy')\n",
    "mult_correction_factors_cube5 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube5_factors.npy')\n",
    "mult_correction_factors_cube6 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube6_factors.npy')\n",
    "mult_correction_factors_cube7 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube7_factors.npy')\n",
    "\n",
    "radius = 1*u.arcsec\n",
    "location = locations[0]\n",
    "psf_size = psf_sizes[0]\n",
    "#for psf_size in psf_sizes:\n",
    "if psf_size == 0:\n",
    "    psf_size = 1e-6\n",
    "print('***********************************************************************************************************************')\n",
    "print(f'now using psf size {psf_size}')\n",
    "print('***********************************************************************************************************************')\n",
    "new_images = []\n",
    "for i,file in enumerate(image_files):\n",
    "    print('convolving file: ', file)\n",
    "    new_file = psf_homogenize_jwst_fits(file, psf_size, f\"Data_files/misc_data/test_data/testing_convolutions{i}_{extract_filter_name(file)}_image.fits\",\n",
    "                                        native_wavelength = jwst_means[extract_filter_name(file)], \n",
    "                                        location = location, radius_arcsec = radius.value)\n",
    "    new_images.append(new_file)\n",
    "    \n",
    "new_ifus = []\n",
    "for i,file in enumerate(full_raw_ifu_files_loc0):\n",
    "    print('convolving file: ', file)\n",
    "\n",
    "    new_file = psf_homogenize_jwst_fits(file, psf_size, f\"Data_files/misc_data/test_data/testing_convolutions{i}_ifu.fits\",\n",
    "                                        radius_arcsec = radius.value)\n",
    "    new_ifus.append(new_file)\n",
    "    \n",
    "\n",
    "data = create_data(new_ifus, new_images, filter_files, locations[0], radius)\n",
    "\n",
    "\n",
    "add_correction_factors_cube1 = np.append(add_correction_factors_cube1, data['add_correction_values'][0])\n",
    "add_correction_factors_cube2 = np.append(add_correction_factors_cube2, data['add_correction_values'][1])\n",
    "add_correction_factors_cube3 = np.append(add_correction_factors_cube3, data['add_correction_values'][2])\n",
    "add_correction_factors_cube4 = np.append(add_correction_factors_cube4, data['add_correction_values'][3])\n",
    "add_correction_factors_cube5 = np.append(add_correction_factors_cube5, data['add_correction_values'][4])\n",
    "add_correction_factors_cube6 = np.append(add_correction_factors_cube6, data['add_correction_values'][5])\n",
    "add_correction_factors_cube7 = np.append(add_correction_factors_cube7, data['add_correction_values'][6])\n",
    "mult_correction_factors_cube1 = np.append(mult_correction_factors_cube1, data['mult_correction_values'][0])\n",
    "mult_correction_factors_cube2 = np.append(mult_correction_factors_cube2, data['mult_correction_values'][1])\n",
    "mult_correction_factors_cube3 = np.append(mult_correction_factors_cube3, data['mult_correction_values'][2])\n",
    "mult_correction_factors_cube4 = np.append(mult_correction_factors_cube4, data['mult_correction_values'][3])\n",
    "mult_correction_factors_cube5 = np.append(mult_correction_factors_cube5, data['mult_correction_values'][4])\n",
    "mult_correction_factors_cube6 = np.append(mult_correction_factors_cube6, data['mult_correction_values'][5])\n",
    "mult_correction_factors_cube7 = np.append(mult_correction_factors_cube7, data['mult_correction_values'][6])\n",
    "\n",
    "\n",
    "plot_results(data, correction = 'mult', show_images=['F560W', 'F2100W'])\n",
    "plot_results(data, correction = 'add', show_images=['F560W', 'F2100W'])\n",
    "\n",
    "\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube1_factors.npy', add_correction_factors_cube1)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube2_factors.npy', add_correction_factors_cube2)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube3_factors.npy', add_correction_factors_cube3)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube4_factors.npy', add_correction_factors_cube4)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube5_factors.npy', add_correction_factors_cube5)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube6_factors.npy', add_correction_factors_cube6)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube7_factors.npy', add_correction_factors_cube7)\n",
    "\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube1_factors.npy', mult_correction_factors_cube1)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube2_factors.npy', mult_correction_factors_cube2)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube3_factors.npy', mult_correction_factors_cube3)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube4_factors.npy', mult_correction_factors_cube4)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube5_factors.npy', mult_correction_factors_cube5)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube6_factors.npy', mult_correction_factors_cube6)\n",
    "np.save(f'Data_files/misc_data/test_data/loc{loc_idx}mult_cube7_factors.npy', mult_correction_factors_cube7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = ['red', 'orange', 'green', 'blue', 'purple', 'black', 'cyan']\n",
    "for i, factors in enumerate(mean_add_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean additive correction value')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, factors in enumerate(mean_mult_correction_factors):\n",
    "    print(factors)\n",
    "    for r, f in enumerate(factors):\n",
    "        plt.scatter(psf_sizes[i], f, color = colors[r], label = '')\n",
    "i=1\n",
    "\n",
    "for color in colors:\n",
    "    plt.scatter([],[], color = color, label = f'cube#{i}')\n",
    "    i+=1\n",
    "plt.xlabel('psf_FWHM')\n",
    "plt.ylabel('mean multiplicative correction value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_correction_factors_cube1 = np.load(f'Data_files/misc_data/test_data/loc{loc_idx}add_cube1_factors.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(add_correction_factors_cube1, data['add_correction_values'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_correction_factors_cube1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'loc{1+1}' = 2\n",
    "loc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
